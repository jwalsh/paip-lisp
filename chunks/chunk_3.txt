Return bindings that unify two expressions (section 11.2). 
Unify a variable against an expression. 
See if a particular variable occurs inside an expression. 
Substitute bindings into an expression. 
Get the (var . val) binding for a variable. 
Get the value for a variable. 
Add a new variable/value pair to a binding list. 
Is the argument a variable? 
Like cons, except will reuse an old value if possible. 

Figure 11.1: Glossary for the Prolog Interpreter 


<a id='page-362'></a>

(defun clear-db () 
"Remove all clauses (for all predicates) from the data base." 
(mapc #'clear-predicate *db-predicates*)) 

(defun clear-predicate (predicate) 
"Remove the clauses for a single predicate." 
(setf (get predicate 'clauses) nil)) 

A data base is useless without a way of getting data out, as well as putting it in. The 
function prove will be used to prove that a given goal either matches a fact that is in 
the data base directly or can be derived from the rules. To prove a goal, first find all 
the candidate clauses for that goal. For each candidate, check if the goal unifies with 
the head of the clause. If it does, try to prove all the goals in the body of the clause. 
For facts, there will be no goals in the body, so success will be immediate. For rules, 
the goals in the body need to be proved one at a time, making sure that bindings from 
the previous step are maintained. The implementation is straightforward: 

(defun prove (goal bindings) 
"Return a list of possible solutions to goal." 
(mapcan #'(lambda (clause) 

(let ((new-clause (rename-variables clause))) 
(prove-all (clause-body new-clause) 
(unify goal (clause-head new-clause) bindings)))) 
(get-clauses (predicate goal)))) 

(defun prove-all (goals bindings) 
"Return a list of solutions to the conjunction of goals.'" 
(cond ((eq bindings fail) fail) 

((null goals) (list bindings)) 
(t (mapcan #*(lambda (goall-solution) 
(prove-all (rest goals) goall-solution)) 
(prove (first goals) bindings))))) 

The tricky part is that we need some way of distinguishing a variable ?x in one 
clause from another variable ?x in another clause. Otherwise, a variable used in two 
different clauses in the course of a proof would have to take on the same value in 
each clause, which would be a mistake. Just as arguments to a function can have 
different values in different recursive calls to the function, so the variables in a clause 
are allowed to take on different values in different recursive uses. The easiest way to 
keep variables distinct is just to rename all variables in each clause before it is used. 
The function rename-vari abl es does this:^ 

^See exercise 11.12 for an alternative approach. 


<a id='page-363'></a>

(defun rename-variables (x) 
"Replace all variables in . with new ones." 
(sublis (mapcar #'(lambda (var) (cons var (gensym (string var)))) 

(variables-in x)) 

X)) 

Rename - variables makes use of gensym, a function that generates a new symbol each 
time it is called. The symbol is not interned in any package, which means that there 
is no danger of a programmer typing a symbol of the same name. The predicate 
vari abl es -i. and its auxiliary function are defined here: 

(defun variables-in (exp) 
"Return a list of all the variables in EXP." 
(unique-find-anywhere-if #*variable-p exp)) 

(defun unique-find-anywhere-if (predicate tree 

&optional found-so-far) 
"Return a list of leaves of tree satisfying predicate, 
with duplicates removed." 
(if (atom tree) 

(if (funcall predicate tree) 
(adjoin tree found-so-far) 
found-so-far) 

(unique-find-anywhere-if 
predicate 
(first tree) 
(unique-find-anywhere-if predicate (rest tree) 

found-so-far)))) 

Finally, we need a nice interface to the proving functions. We will use ?- as a macro 
to introduce a query. The query might as well allow a conjunction of goals, so ?- will 
call prove-all. Together,<- and?- def ine the complete syntax of our Prolog-In-Lisp 
language. 

(defmacro ? - (&rest goals) '(prove-all '.goals no-bindings)) 

Now we can enter all the clauses given in the prior example: 

(<- (likes Kim Robin)) 

(<- (likes Sandy Lee)) 

(<- (likes Sandy Kim)) 

(<- (likes Robin cats)) 

(<- (likes Sandy ?x) (likes ?x cats)) 

(<- (likes Kim ?x) (likes ?x Lee) (likes ?x Kim)) 

(<- (likes ?x ?x)) 


<a id='page-364'></a>

To ask whom Sandy Hkes, we would use: 

> (?- (likes Sandy ?who)) 

(((?WHO . LEE)) 
((?WHO . KIM)) 
((7X2856 . ROBIN) (?WHO . 7X2856)) 
((7X2860 . CATS) (7X2857 . CATS) (7X2856 . SANDY) (7WH0 . 7X2856)) 
((7X2865 . CATS) (7X2856 . 7X2865) (7WH0 . 7X2856)) 
((7WH0 . SANDY) (7X2867 . SANDY))) 

Perhaps surprisingly, there are six answers. The first two answers are Lee and Kim, 
because of the facts. The next three stem from the clause that Sandy likes everyone 
who likes cats. First, Robin is an answer because of the fact that Robin likes cats. 
To see that Robin is the answer, we have to unravel the bindings: ?who is bound to 
?x2856, which is in turn bound to Robin. 

Now we're in for some surprises: Sandy is listed, because of the following reasoning: 
(1) Sandy likes anyone/thing who likes cats, (2) cats like cats because everyone 
likes themself, (3) therefore Sandy likes cats, and (4) therefore Sandy likes Sandy. 
Cats is an answer because of step (2), and finally, Sandy is an answer again, because 
of the clause about liking oneself. Notice that the result of the query is a list of 
solutions, where each solution corresponds to a different way of proving the query 
true. Sandy appears twice because there are two different ways of showing that 
Sandy likes Sandy. The order in which solutions appear is determined by the order 
of the search. Prolog searches for solutions in a top-down, left-to-right fashion. The 
clauses are searched from the top down, so the first clauses entered are the first ones 
tried. Within a clause, the body is searched left to right. In using the (1 i kes Ki m ?x) 
clause, Prolog would first try to find an . who likes Lee, and then see if . likes Kim. 

The output from prove-al 1 is not very pretty. We can fix that by defining a new 
function, top-level -prove, which calls prove-all as before, but then passes the 
list of solutions to show-prolog-solutions, which prints them in a more readable 
format Note thatshow-prolog-solutions returns no values: (values). This means 
the read-eval-print loop will not print anything when (values) is the result of a 
top-level call. 

(defmacro 7- (&rest goals) 
*(top-level-prove *,goals)) 

(defun top-level-prove (goals) 
"Prove the goals, and print variables readably." 
(show-prolog-solutions 

(variables-in goals) 
(prove-all goals no-bindings))) 


<a id='page-365'></a>

(defun show-prolog-solutions (vars solutions) 
"Print the variables in each of the solutions." 
(if (null solutions) 

(format t "-&No.") 
(mapc #'(lambda (solution) (show-prolog-varssolutions)) 
(values)) 

(defun show-prolog-vars (vars bindings) 
"Print each variable with its binding." 

 vars solution)) 

(if (null vars) 
(format t "~&Yes") 
(dolist (var vars) 

(format t ""&^a = ~a" var 
(subst-bindings bindings

(princ ";")) 

Now let's try some queries: 

> (?- (likes Sandy ?who)) 

?WHO = LEE; 

?WHO = KIM; 

?WHO = ROBIN; 

?WHO = SANDY; 

?WHO = CATS; 

?WHO = SANDY; 

> (?- (likes ?who Sandy)) 
?WHO = SANDY; 
?WHO = KIM; 
?WHO = SANDY; 

> (?- (likes Robin Lee)) 
No. 

 var)))) 

The first query asks again whom Sandy likes, and the second asks who likes Sandy. 
The third asks for confirmation of a fact. The answer is "no," because there are no 
clauses or facts that say Robin likes Lee. Here's another example, a list of pairs of 
people who are in a mutual liking relation. The last answer has an uninstantiated 
variable, indicating that everyone likes themselves. 


<a id='page-366'></a>

> (?- (likes ?x ?y) (likes ?y ?x)) 

?Y = KIM 

?X = SANDY; 

?Y = SANDY 

?X = SANDY; 

?Y = SANDY 

?X = SANDY; 

?Y = SANDY 

?X = KIM; 

?Y = SANDY 

?X = SANDY; 

?Y = 7X3251 

?X = 7X3251; 

It makes sense in Prolog to ask open-ended queries like "what lists is 2 a member of?" 
or even "what items are elements of what lists?" 

(7- (member 2 71ist)) 
(7- (member 7item 71ist)) 

These queries are valid Prolog and will return solutions, but there will be an infinite 
number of them. Since our interpreter collects all the solutions into a single list 
before showing any of them, we will never get to see the solutions. The next section 
shows how to write a new interpreter that fixes this problem. 

&#9635; Exercise 11.1 [m] The representation of relations has been a list whose first element 
is a symbol. However, for relations with no arguments, some people prefer to write 
(<- . q r) rather than (<- (p) (q) (r)). Make changes so that either form is 
acceptable. 

&#9635; Exercise 11.2 [m] Some people find the < - notation difficult to read. Define macros 
rul e and fact so that we can write: 

(fact (likes Robin cats)) 
(rule (likes Sandy 7x) if (likes 7x cats)) 


<a id='page-367'></a>

11.3 Idea 3: Automatic Backtracking 
The Prolog interpreter implemented in the last section solves problems by returning a 
list of all possible solutions. We'll call this a batch approach, because the answers are 
retrieved in one uninterrupted batch of processing. Sometimes that is just what you 
want, but other times a single solution will do. In real Prolog, solutions are presented 
one at a time, as they are found. After each solution is printed, the user has the 
option of asking for more solutions, or stopping. This is an incremental approach. 
The incremental approach will be faster when the desired solution is one of the first 
out of many alternatives. The incremental approach will even work when there is an 
infinite number of solutions. And if that is not enough, the incremental approach can 
be implemented so that it searches depth-first. This means that at any point it will 
require less storage space than the batch approach, which must keep all solutions in 
memory at once. 

In this section we implement an incremental Prolog interpreter. One approach 
would be to modify the interpreter of the last section to use pipes rather than lists. 
With pipes, unnecessary computation is delayed, and even infinite lists can be 
expressed in a finite amount of time and space. We could change to pipes simply by 
changing the mapcan in prove and prove-a11 to mappend-pi pe ([page 286](chapter9.md#page-286)). The books 
by Winston and Horn (1988) and by Abelson and Sussman (1985) take this approach. 
We take a different one. 

The first step is a version of prove and prove-al 1 that return a single solution 
rather than a list of all possible solutions. This should be reminiscent of achi eve and 
achieve-a11 from gps (chapter 4). Unlike gps, recursive subgoals and clobbered 
siblinggoals are not checked for. However, prove is required to search systematically 
through all solutions, so it is passed an additional parameter: a list of other goals to 
achieve after achieving the first goal. This is equivalent to passing a continuation to 
prove. The result is that if prove ever succeeds, it means the entire top-level goal has 
succeeded. If it fails, it just means the program is backtracking and trying another 
sequence of choices. Note that prove relies on the fact that f ai 1 is ni 1, because of 
the way it uses some. 

(defun prove-all (goals bindings) 
"Find a solution to the conjunction of goals." 
(cond ((eq bindings fail) fail) 

((null goals) bindings) 
(t (prove (first goals) bindings (rest goals))))) 

(defun prove (goal bindings other-goals) 
"Return a list of possible solutions to goal." 
(some #*(lambda (clause) 

(let ((new-clause (rename-variables clause))) 
(prove-all 
(append (clause-body new-clause) other-goals) 


<a id='page-368'></a>

(unify goal (clause-head new-clause) bindings)))) 
(get-clauses (predicate goal)))) 

If . rove does succeed, it means a solution has been found. If we want more solutions, 
we need some way of making the process fail, so that it will backtrack and try again. 
One way to do that is to extend every query with a goal that will print out the variables, 
and ask the user if the computation should be continued. If the user says yes, then 
the goalfails, and backtracking starts. If the user says no, the goal succeeds, and since 
it is the final goal, the computation ends. This requires a brand new type of goal: one 
that is not matched against the data base, but rather causes some procedure to take 
action. In Prolog, such procedures are called primitives, because they are built-in to 
the language, and new ones may not be defined by the user. The user may, of course, 
define nonprimitive procedures that call upon the primitives. 

In our implementation, primitives will be represented as Lisp functions. A 
predicate can be represented either as a list of clauses (as it has been so far) or as a 
single primitive. Here is a version of prove that calls primitives when appropriate: 

(defun prove (goal bindings other-goals) 
"Return a list of possible solutions to goal." 
(let ((clauses (get-clauses (predicate goal)))) 

(if (listp clauses) 
(some 
#'(lambda (clause) 
(let ((new-clause (rename-variables clause))) 

(prove-al1 
(append (clause-body new-clause) other-goals) 
(unify goal (clause-head new-clause) bindings)))) 

clauses) 

The predicate's "clauses" can be an atom; 
;; a primitive function to call 
(funcall clauses (rest goal) bindings 

other-goals)))) 

Here is theversionof top-level -provethatadds the primitivegoalshow-prolog-vars 
totheendofthelistofgoals. Note that this versionneednot call show-prolog-sol utions 
itself, since the printing will be handled by the primitive for show-prol og-vars. 

(defun top-level-prove (goals) 
(prove-all '(.goals (show-prolog-vars ,@(variables-in goals))) 

no-bindings) 
(format t "~&No.") 
(values)) 

Here we define the primitive show-prol og- vars. All primitives must be functions of 


<a id='page-369'></a>

three arguments: a Hst of arguments to the primitive relation (here a list of variables 
to show), a binding list for these arguments, and a list of pending goals. A primitive 
should either return f ai 1 or call prove-al 1 to continue. 

(defun show-prolog-vars (vars bindings other-goals) 
"Print each variable with its binding. 
Then ask the user if more solutions are desired." 
(if (null vars) 

(format t "~&Yes") 
(dolist (var vars) 
(format t ""&^a = ~a" var 
(subst-bindings bindings var)))) 

(if (continue-.) 
fail 
(prove-all other-goals bindings))) 

Since primitives are represented as entries on the clauses property of predicate 
symbols, we have to register show- prol og - va rs as a primitive like this: 

(setf (get 'show-prolog-vars 'clauses) 'show-prolog-vars) 

Finally, the Lisp predicate conti nue-p asks the user if he or she wants to see more 
solutions: 

(defun continue-p () 
"Ask user if we should continue looking for solutions." 
(case (read-char) 

(#\; t) 
(#\. nil) 
(#\newline (continue-p)) 
(otherwise 

(format t " Type ; to see more or . to stop") 
(continue-p)))) 

This version works just as well as the previous version on finite problems. The only 
difference is that the user, not the system, types the semicolons. The advantage is 
that we can now use the system on infinite problems as well. First, we'll ask what 
Hsts 2 is a member of: 


<a id='page-370'></a>

> (?- (member 2 ?list)) 
?LIST = (2 . 7REST3302); 
?LIST = (7X3303 2 . 7REST3307); 
7LIST = (7X3303 7X3308 2 . 7REST3312); 
7LIST = (7X3303 7X3308 7X3313 2 . 7REST3317). 
No. 

The answers mean that 2 is a member of any Ust that starts with 2, or whose second 
element is 2, or whose third element is 2, and so on. The infinite computation was 
halted when the user typed a period rather than a semicolon. The "no" now means 
that there are no more answers to be printed; it will appear if there are no answers at 
all, if the user types a period, or if all the answers have been printed. 

We can ask even more abstract queries. The answer to the next query says that 
an item is an element of a list when it is the the first element, or the second, or the 
third, or the fourth, and so on, 

> (7- (member 7item 71ist)) 
7 ITEM = 7ITEM3318 
7LIST = (7ITEM3318 . 7REST3319); 
7ITEM = 7ITEM3323 
7LIST = (7X3320 7ITEM3323 . 7REST3324): 
7 ITEM = 7ITEM3328 
7LIST = (7X3320 7X3325 7ITEM3328 . 7REST3329); 
7 ITEM = 7ITEM3333 
7LIST = (7X3320 7X3325 7X3330 7ITEM3333 . 7REST3334). 
No. 

Now let's add the definition of the relation length: 

(<- (length () 0)) 
(<- (length (?x . ?y) (1+ ?n)) (length ?y ?n)) 

Here are some queries showing that length can be used to find the second argument, 
the first, or both: 

> (7- (length (abed) 7n)) 
7N = (1+ (1+ (1+ (1+ 0)))); 
No. 

> (7- (length 71ist (1+ (1+ 0)))) 
7LIST = (7X3869 7X3872); 
No. 


<a id='page-371'></a>

> (?- (length ?list ?n)) 

?LIST = NIL 

?N = 0; 

?LIST = (?X3918) 

?N = (1+ 0); 

?LIST = (7X3918 7X3921) 

7N = (1+ (1+ 0)). 

No. 

The next two queries show the two lists of length two with a as a member. Both 

queries give the correct answer, a two-element list that either starts or ends with a. 

However, the behavior after generating these two solutions is quite different. 

> (7- (length 71 (1+ (1+ 0))) (member a 71)) 
7L = (A 7X4057); 
7L = (7Y4061 A); 
No. 

> (7- (member a 71) (length 71 (1+ (1+ 0)))) 
7L = (A 7X4081); 
7L = (7Y4085 A);[Abort] 

In the first query, length only generates one possible solution, the list with two 
unbound elements, member takes this solution and instantiates either the first or the 
second element to a. 

In the second query, member keeps generating potential solutions. The first two 
partial solutions, where a is the first or second member of a list of unknown length, 
are extended by length to yield the solutions where the list has length two. After 
that, member keeps generating longer and longer lists, which length keeps rejecting. 
It is implicit in the definition of member that subsequent solutions will be longer, but 
because that is not explicitly known, they are all generated anyway and then explicitly 
tested and rejected by length. 

This example reveals the limitations of Prolog as a pure logic-programming language. 
It turns out the user must be concerned not only about the logic of the problem 
but also with the flow of control. Prolog is smart enough to backtrack and find all 
solutions when the search space is small enough, but when it is infinite (or even 
very large), the programmer still has a responsibility to guide the flow of control. 
It is possible to devise languages that do much more in terms of automatic flow of 
control."* Prolog is a convenient and efficient middle ground between imperative 
languages and pure logic. 

^See the MU-Prolog and NU-Prolog languages (Naish 1986). 


<a id='page-372'></a>

Approaches to Backtracking 

Suppose you are asked to make a "small" change to an existing program. The 
problem is that some function, f, which was thought to be single-valued, is now 
known to return two or more vaUd answers in certain circumstances. In other words, 
f is nondeterministic. (Perhaps f is sqrt, and we now want to deal with negative 
numbers). What are your alternatives as a programmer? Five possibiUties can be 
identified: 

* Guess. Choose one possibility and discard the others. This requires a means 
of making the right guesses, or recovering from wrong guesses. 
* Know. Sometimes you can provide additional information that is enough to 
decide what the right choice is. This means changing the calling function(s) to 
provide the additional information. 
* Return a list. This means that the calling function(s) must be changed to expect 
a list of replies. 
* Return a pipe, as defined in section 9.3. Again, the calling function(s) must be 
changed to expect a pipe. 
* Guess and save. Choose one possibility and return it, but record enough 
information to allow computing the other possibilities later. This requires 
saving the current state of the computation as well as some information on the 
remaining possibilities. 
The last alternative is the most desirable. It is efficient, because it doesn't require 
computing answers that are never used. It is unobtrusive, because it doesn't require 
changing the calling function (and the calling function's calling function) to expect a 
list or pipe of answers. Unfortunately, it does have one major difficulty: there has 
to be a way of packaging up the current state of the computation and saving it away 
so that it can be returned to when the first choice does not work. For our Prolog 
interpreter, the current state is succinctly represented as a list of goals. In other 
problems, it is not so easy to summarize the entire state. 

We will see in section 22.4 that the Scheme dialect of Lisp provides a function, 
ca 11 - wi th - cu r rent - conti nua t i on, that does exactly what we want: it packages the 
current state of the computation into a function, which can be stored away and 
invoked later. Unfortunately, there is no corresponding function in Common Lisp. 

Anonymous Variables 

Before moving on, it is useful to introduce the notion of an anonymous variable. 
This is a variable that is distinct from all others in a clause or query, but which the 


<a id='page-373'></a>

programmer does not want to bother to name. In real Prolog, the underscore is used 
for anonymous variables, but we will use a single question mark. The definition of 
member that follows uses anonymous variables for positions within terms that are not 
needed within a clause: 

(<- (member ?item (?item . ?))) 
(<- (member ?item (? . ?rest)) (member ?item ?rest)) 

However, we also want to allow several anonymous variables in a clause but still be 
able to keep each anonymous variable distinct from all other variables. One way to 
do that is to replace each anonymous variable with a unique variable. The function 
repl ace - ? - va rs uses gensym to do just that. It is installed in the top-level macros <and 
? - so that all clauses and queries get the proper treatment. 

(defmacro <- (&rest clause) 
"Add a clause to the data base." 
*(add-clause '.(replace-?-vars clause))) 

(defmacro ? - (&rest goals) 
"Make a query and print answers." 
'(top-level-prove '.(replace-?-vars goals))) 

(defun replace-?-vars (exp) 
"Replace any ? within exp with a var of the form ?123." 
(cond ((eq exp '?) (gensym "?")) 

((atom exp) exp) 

(t (reuse-cons (replace-?-vars (first exp)) 
(replace-?-vars (rest exp)) 
exp)))) 

A named variable that is used only once in a clause can also be considered an 
anonymous variable. This is addressed in a different way in section 12.3. 

11.4 The Zebra Puzzle 
Here is an example of something Prolog is very good at: a logic puzzle. There are 
fifteen facts, or constraints, in the puzzle: 

1. There are five houses in a line, each with an owner, a pet, a cigarette, a drink, 
and a color. 
2. The Englishman lives in the red house. 
3. The Spaniard owns the dog. 

<a id='page-374'></a>

4. Coffee is drunk in the green house. 
5. The Ukrainian drinks tea. 
6. The green house is immediately to the right of the ivory house. 
7. The Winston smoker owns snails. 
8. Kools are smoked in the yellow house. 
9. Milk is drunk in the middle house. 
10. The Norwegian lives in the first house on the left. 
11. The man who smokes Chesterfields lives next to the man with the fox. 
12. Kools are smoked in the house next to the house with the horse. 
13. The Lucky Strike smoker drinks orange juice. 
14. The Japanese smokes Parliaments. 
15. The Norwegian lives next to the blue house. 
The questions to be answered are: who drinks water and who owns the zebra? To 
solve this puzzle, we first define the relations nextto (for "next to") and i ri ght (for 
"immediately to the right of"). They are closely related to member, which is repeated 
here. 

(<- (member ?item (?item . ?rest))) 
(<- (member ?item (?x . ?rest)) (member ?item ?rest)) 

(<- (nextto ?x ?y ?list) (iright ?x ?y ?list)) 
(<- (nextto ?x ?y ?list) (iright ?y ?x ?list)) 

(<- (iright ?left ?right (?left ?right . ?rest))) 
(<- (iright Tieft ?right (?x . ?rest)) 
(iright ?left ?right ?rest)) 

(<- (= ?x ?x)) 

We also defined the identity relation, =. It has a single clause that says that any . is 
equal to itself. One might think that this implements eq or equal. Actually, since 
Prolog uses unification to see if the two arguments of a goal each unify with ?x, this 
means that = is unification. 

Now we are ready to define the zebra puzzle with a single (long) clause. The 
variable ?h represents the list of five houses, and each house is represented by a term 
of the form (house nationality pet cigarette drink color). The variable ?w is the water 
drinker, and ?z is the zebra owner. Each of the 15 constraints in the puzzle is listed 


<a id='page-375'></a>

in the body of zebra, ahhough constraints 9 and 10 have been combined into the 
first one. Consider constraint 2, "The EngUshman lives in the red house." This is 
interpreted as "there is a house whose nationality is Englishman and whose color is 
red, and which is a member of the list of houses": in other words, (member (house 
englishman ? ? ? red) ?h). The other constraints are similarly straightforward. 

(<- (zebra ?h ?w ?z) 
Each house is of the form: 
(house nationality pet cigarette drink house-color) 

(= ?h ((house norwegian ? ? ? ?) ;1,10 
? 

(house ? ? ? milk ?) ? ?)) ; 9 
(member (house englishman ? ? ? red) ?h) ; 2 
(member (house Spaniard dog ? ? ?) ?h) ; 3 
(member (house 111 coffee green) ?h) ; 4 
(member (house Ukrainian ? ? tea ?) ?h) ; 5 
(iright (house 1111 ivory) ; 6 

(house 1111 green) ?h) 
(member (house ? snails winston ? ?) ?h) ; 7 
(member (house ? ? kools ? yellow) ?h) ; 8 
(nextto (house ? ? chesterfield ? ?) ;11 

(house ? fox ? ? ?) ?h) 
(nextto (house ? ? kools ? ?) ;12 

(house ? horse ? ? ?) ?h) 
(member (house ? ? luckystrike orange-juice ?) ?h);13 
(member (house Japanese ? parliaments ? ?) ?h) ;14 
(nextto (house norwegian 1111) ;15 

(house 1111 blue) ?h) 

Now for the questions: 
(member (house ?w ? ? water ?) ?h) ;Q1 
(member (house ?z zebra 111) ?h)) ;Q2 

Here's the query and solution to the puzzle: 

> (?- (zebra ?houses ?water-drinker ?zebra-owner)) 

7H0USES = ((HOUSE NORWEGIAN FOX KOOLS WATER YELLOW) 
(HOUSE UKRAINIAN HORSE CHESTERFIELD TEA BLUE) 
(HOUSE ENGLISHMAN SNAILS WINSTON MILK RED) 
(HOUSE SPANIARD DOG LUCKYSTRIKE ORANGE-JUICE IVORY) 
(HOUSE JAPANESE ZEBRA PARLIAMENTS COFFEE GREEN)) 

7WATER-DRINKER = NORWEGIAN 

7ZEBRA-0WNER = JAPANESE. 

No. 

This took 278 seconds, and profiHng (see [page 288](chapter9.md#page-288)) reveals that the function prove was 
called 12,825 times. A call to prove has been termed a logical inference, so our system 


<a id='page-376'></a>

is performing 12825/278 = 46 logical inferences per second, or LIPS. Good Prolog 
systems perform at 10,000 to 100,000 LIPS or more, so this is barely Hmping along. 

Small changes to the problem can greatly affect the search time. For example, 
the relation nextto holds when the first house is immediately right of the second, or 
when the second is immediately right of the first. It is arbitrary in which order these 
clauses are listed, and one might think it would make no difference in which order 
they were listed. In fact, if we reverse the order of these two clauses, the execution 
time is roughly cut in half. 

11.5 The Synergy of Backtracking and 
Unification 
Prolog's backward chaining with backtracking is a powerful technique for generating 
the possible solutions to a problem. It makes it easy to implement a generate-and-test 
strategy, where possible solutions are considered one at a time, and when a candidate 
solution is rejected, the next is suggested. But generate-and-test is only feasible when 
the space of possible solutions is small. 

In the zebra puzzle, there are five attributes for each of the five houses. Thus 
there are 5! ^, or over 24 billion candidate solutions, far too many to test one at a time. 
It is the concept of unification (with the corresponding notion of a logic variable) that 
makes generate-and-test feasible on this puzzle. Instead of enumerating complete 
candidate solutions, unification allows us to specify partial candidates. We start out 
knowing that there are five houses, with the Norwegian living on the far left and 
the milk drinker in the middle. Rather than generating all complete candidates that 
satisfy these two constraints, we leave the remaining information vague, by unifying 
the remaining houses and attributes with anonymous logic variables. The next 
constraint (number 2) places the Englishman in the red house. Because of the way 
member is written, this first tries to place the Englishman in the leftmost house. This 
is rejected, because Englishman and Norwegian fail to unify, so the next possibiUty is 
considered, and the Englishman is placed in the second house. But no other features 
of the second house are specified—we didn't have to make separate guesses for the 
Englishman's house being green, yellow, and so forth. The search continues, filling 
in only as much as is necessary and backing up whenever a unification fails. 

For this problem, unification serves the same purpose as the delay macro 
([page 281](chapter9.md#page-281)). It allows us to delay deciding the value of some attribute as long as 
possible, but to immediately reject a solution that tries to give two different values 
to the same attribute. That way, we save time if we end up backtracking before the 
computation is made, but we are still able to fill in the value later on. 

It is possible to extend unification so that it is doing more work, and backtracking 
is doing less work. Consider the following computation: 


<a id='page-377'></a>

(?- (length ?1 4) 
(member d ?1) (member a ?1) (member c ?1) (member b ?1) 
(= ?1 (a b c d))) 

The first two Hnes generate permutations of the Hst (d a c b), and the third line 
tests for a permutation equal to (a b c d). Most of the work is done by backtracking. 
An alternative is to extend unification to deal with lists, as well as constants and 
variables. Predicates like length and member would be primitives that would have to 
know about the representation of lists. Then the first two lines of the above program 
would set ?1 to something like #s( list :length 4 :members (d a c d)). The 
third line would be a call to the extended unification procedure, which would further 
specify ?1 to be something like: 

#s(11st rlength 4 imembers (d a c d) :order (abc d)) 

By making the unification procedure more complex, we eliminate the need for backtracking 
entirely. 

&#9635; Exercise 11.3 [s] Would a unification algorithm that delayed member tests be a good 
idea or a bad idea for the zebra puzzle? 

11.6 Destructive Unification 
As we saw in section 11.2, keeping track of a binding list of variables is a little tricky. 
It is also prone to inefficiency if the binding list grows large, because the list must 
be searched linearly, and because space must be allocated to hold the binding list. 
An alternative implementation is to change unify to a destructive operation. In 
this approach, there are no binding lists. Instead, each variable is represented as 
a structure that includes a field for its binding. When the variable is unified with 
another expression, the variable's binding field is modified to point to the expression. 
Such variables will be called vars to distinguish them from the implementation of 
variables as symbols starting with a question mark, vars are defined with the 
following code: 

(defconstant unbound "Unbound") 

(defstruct var name (binding unbound)) 

(defun bound-p (var) (not (eq (var-binding var) unbound))) 

The macro de ref gets at the binding of a variable, returning its argument when it is an 


<a id='page-378'></a>

unbound variable or a nonvariable expression. It includes a loop because a variable 
can be bound to another variable, which in turn is bound to the ultimate value. 

Normally, it would be considered bad practice to implement de ref as a macro, 
since it could be implemented as an inline function, provided the caller was willing 
to write (setf . (deref x)) instead of (de ref x). However, de ref will appear 
in code generated by some versions of the Prolog compiler that will be presented in 
the next section. Therefore, to make the generated code look neater, I have allowed 
myself the luxury of the deref macro. 

(defmacro deref (exp) 
"Follow pointers for bound variables." 
'(progn (loop while (and (var-p ,exp) (bound-p ,exp)) 

do (setf ,exp (var-binding ,exp))) 
,exp)) 

The function unify! below is the destructive version of unify. It is a predicate 
that returns true for success and false for failure, and has the side effect of altering 
variable bindings. 

(defun unify! (x y) 
"Destructively unify two expressions" 
(cond ((eql (deref x) (deref y)) t) 

((var-p x) (set-binding! . y)) 
((var-p y) (set-binding! y .)) 
((and (consp .) (consp y)) 

(and (unify! (first x) (first y)) 
(unify! (rest x) (rest y)))) 
(t nil))) 

(defun set-binding! (var value) 
"Set var's binding to value. Always succeeds (returns t)." 
(setf (var-binding var) value) 
t) 

To make vars easier to read, we can install a : pri nt-function: 

(defstruct (var (iprint-function print-var)) 
name (binding unbound)) 
(defun print-var (var stream depth) 
(if (or (and (numberp *print-level*) 
(>= depth *print-level*)) 

(var-p (deref var))) 
(format stream "?~a" (var-name var)) 
(write var :stream stream))) 


<a id='page-379'></a>

Thisis the first example of a carefully crafted : pri nt-function. There are three things 
to notice about it. First, it explicitly writes to the stream passed as the argument. 
It does not write to a default stream. Second, it checks the variable depth against 
*pr i nt-1 evel *, and prints just the variable name when the depth is exceeded. Third, 
it uses wr i te to print the bindings. This is because wr i te pays attention to the current 
values of *pr int-escape*, *print-pretty*, and soon. Other printing functions such 
as pri nl or pri nt do not pay attention to these variables. 

Now, for backtracking purposes, we want to make set-bi nding! keep track of 
the bindings that were made, so they can be undone later: 

(defvar *trail* (make-array 200 ifill-pointer 0 ladjustable t)) 

(defun set-binding! (var value) 
"Set var's binding to value, after saving the variable 
in the trail. Always returns t." 
(unless (eq var value) 

(vector-push-extend var *trail*) 
(setf (var-binding var) value)) 
t) 

(defun undo-bindings! (old-trail) 
"Undo all bindings back to a given point in the trail. " 
(loop until (= (fill-pointer nrail*) old-trail) 

do (setf (var-binding (vector-pop *trail*)) unbound))) 

Now we need a way of making new variables, where each one is distinct. That could 
be done by gensym-ing a new name for each variable, but a quicker solution is just to 
increment a counter. The constructor function ? is defined to generate a new variable 
with a name that is a new integer. This is not strictly necessary; we could have just 
used the automatically provided constructor make-var. However, I thought that the 
operation of providing new anonymous variable was different enough from providing 
a named variable that it deserved its own function. Besides, make-var may be less 
efficient, because it has to process the keyword arguments. The function ? has no 
arguments; it just assigns the default values specified in the slots of the va r structure. 

(defvar *var-counter* 0) 

(defstruct (var (iconstructor ? ()) 

(:print-function print-var)) 
(name (incf *var-counter*)) 
(binding unbound)) 

A reasonable next step would be to use destructive unification to make a more 
efficient interpreter. This is left as an exercise, however, and instead we put the 
interpreter aside, and in the next chapter develop a compiler. 


<a id='page-380'></a>

11.7 Prolog in Prolog 
As stated at the start of this chapter, Prolog has many of the same features that 
make Lisp attractive for program development. Just as it is easy to write a Lisp 
interpreter in Lisp, it is easy to write a Prolog interpreter in Prolog. The following 
Prolog metainterpreter has three main relations. The relation c1 a use is used to store 
clauses that make up the rules and facts that are to be interpreted. The relation 
prove is used to prove a goal. It calls prove-al 1, which attempts to prove a list of 
goals, prove-al 1 succeeds in two ways: (1) if the list is empty, or (2) if there is some 
clause whose head matches the first goal, and if we can prove the body of that clause, 
followed by the remaining goals: 

(<- (prove ?goal) (prove-all (?goal))) 

(<- (prove-all nil)) 

(<- (prove-all (?goal . ?goals)) 
(clause (<- ?goal . ?body)) 
(concat ?body ?goals ?new-goals) 
(prove-all ?new-goals)) 

Now we add two clauses to the data base to define the member relation: 

(<- (clause (<- (mem ?x (?x . ?y))))) 
(<- (clause (<- (mem ?x (? . ?z)) (mem ?x ?z)))) 

Finally, we can prove a goal using our interpreter: 

(?- (prove (mem ?x (1 2 3)))) 
?X = 1; 
?X = 2; 
?X = 3; 
No. 

11.8 Prolog Compared to Lisp 
Many of the features that make Prolog a succesful language for AI (and for program 
development in general) are the same as Lisp's features. Let's reconsider the list of 
features that make Lisp different from conventional languages (see [page 25](chapter1.md#page-25)) and see 
what Prolog has to offer: 


<a id='page-381'></a>

* Built-in Support for Lists (and other data types). New data types can be created 
easily using lists or structures (structures are preferred). Support for reading, 
printing, and accessing components is provided automatically. Numbers, 
symbols, and characters are also supported. However, because logic variables 
cannot be altered, certain data structures and operations are not provided. For 
example, there is no way to update an element of a vector in Prolog. 
* Automatic Storage Management. The programmer can allocate new objects without 
worrying about reclaiming them. Reclaiming is usually faster in Prolog than 
in Lisp, because most data can be stack-allocated instead of heap-allocated. 
* Dynamic Typing. Declarations are not required. Indeed, there is no standard 
way to make type declarations, although some implementations allow for them. 
Some Prolog systems provide only fixnums, so that eliminates the need for a 
large class of declarations. 
* First-Class Functions. Prolog has no equivalent of 1 ambda, but the built-in predicate 
cal 1 allows a term—a piece of data—to be called as a goal. Although 
backtracking choice points are not first-class objects, they can be used in a way 
very similar to continuations in Lisp. 
* Uniform Syntax. Like Lisp, Prolog has a uniform syntax for both programs and 
data. This makes it easy to write interpreters and compilers in Prolog. While 
Lisp's prefix-operator list notation is more uniform, Prolog allows infix and 
postfix operators, which may be more natural for some applications. 
* Interactive Environment. Expressions can be immediately evaluated. High-
quality Prolog systems offer both a compiler and interpreter, along with a host 
of debugging tools. 
* Extensibility. Prolog syntax is extensible. Because programs and data share 
the same format, it is possible to write the equivalent of macros in Prolog and 
to define embedded languages. However, it can be harder to ensure that the 
resulting code will be compiled efficiently. The details of Prolog compilation 
are implementation-dependent. 
To put things in perspective, consider that Lisp is at once one of the highest-level 
languages available and a universal assembly language. It is a high-level language 
because it can easily capture data, functional, and control abstractions. It is a good 
assembly language because it is possible to write Lisp in a style that directly reflects 
the operations available on modern computers. 

Prolog is generally not as efficient as an assembly language, but it can be more 

concise as a specification language, at least for some problems. The user writes 

specifications: lists of axioms that describe the relationships that can hold in the 

problem domain. If these specifications are in the right form, Prolog's automatic 


<a id='page-382'></a>

backtracking can find a solution, even though the programmer does not provide an 
explicit algorithm. For other problems, the search space will be too large or infinite, 
or Prolog's simple depth-first search with backup will be too inflexible. In this case, 
Prolog must be used as a programming language rather than a specification language. 
The programmer must be aware of Prolog's search strategy, using it to implement an 
appropriate algorithm for the problem at hand. 

Prolog, like Lisp, has suffered unfairly from some common myths. It has been 
thought to be an inefficient language because early implementations were interpreted, 
and because it has been used to write interpreters. But modern compiled 
Prolog can be quite efficient (see Warren et al. 1977 and Van Roy 1990). There is a 
temptation to see Prolog as a solution in itself rather than as a programming language. 
Those who take that view object that Prolog's depth-first search strategy and basis in 
predicate calculus is too inflexible. This objection is countered by Prolog programmers 
who use the facilities provided by the language to build more powerful search 
strategies and representations, just as one would do in Lisp or any other language. 

11.9 History and References 
Cordell Green (1968) was the first to articulate the view that mathematical results 
on theorem proving could be used to make deductions and thereby answer queries. 
However, the major technique in use at the time, resolution theorem proving (see 
Robinson 1965), did not adequately constrain search, and thus was not practical. 
The idea of goal-directed computing was developed in Carl Hewitt's work (1971) on 
the PLANNER language for robot problem solving. He suggested that the user provide 
explicit hints on how to control deduction. 

At about the same time and independently, Alain Colmerauer was developing 
a system to perform natural language analysis. His approach was to weaken the 
logical language so that computationally complex statements (such as logical disjunctions) 
could not be made. Colmerauer and his group implemented the first 
Prolog interpreter using Algol-W in the summer of 1972 (see Roussel 1975). It was 
Roussel's wife, Jacqueline, who came up with the name Prolog as an abbreviation 
for "programmation en logique." The first large Prolog program was their natural 
language system, also completed that year (Colmerauer et al. 1973). For those who 
read English better than French, Colmerauer (1985) presents an overview of Prolog. 
Robert Kowalski is generally considered the coinventer of Prolog. His 1974 article 
outlines his approach, and his 1988 article is a historical review on the early logic 
programming work. 

There are now dozens of text books on Prolog. In my mind, six of these stand 
out. Clocksin and Mellish's Programming in Prolog (1987) was the first and remains 
one of the best. Sterling and Shapiro's The Art of Prolog (1986) has more substantial 
examples but is not as complete as a reference. An excellent overview from a slightly 


<a id='page-383'></a>

more mathematical perspective is Pereira and Shieber's Prolog and Natural-Language 
Analysis (1987). The book is worthwhile for its coverage of Prolog alone, and it also 
provides a good introduction to the use of logic programming for language understanding 
(see part V for more on this subject). O'Keefe's The Craft of Prolog (1990) 
shows a number of advanced techinques. O'Keefe is certainly one of the most influential 
voices in the Prolog community. He has definite views on what makes for good 
and bad coding style and is not shy about sharing his opinions. The reader is warned 
that this book evolved from a set of notes on the Clocksin and Mellish book, and the 
lack of organization shows in places. However, it contains advanced material that 
can be found nowhere else. Another collection of notes that has been organized into 
a book is Coelho and Cotta's Prolog by Example. Published in 1988, this is an update 
of their 1980 book. How to Solve it in Prolog. The earlier book was an underground 
classic in the field, serving to educate a generation of Prolog programmers. Both 
versions include a wealth of examples, unfortunately with little documentation and 
many typos. Finally, Ivan Bratko's Prolog Programming for Artificial Intelligence (1990) 
covers some introductory AI material from the Prolog perspective. 

Maier and Warren's Computing with Logic (1988) is the best reference for those 
interested in implementing Prolog. It starts with a simple interpreter for a variable-
free version of Prolog, and then moves up to the full language, adding improvements 
to the interpreter along the way. (Note that the second author, David S. Warren of 
Stonybrook, is different from David H. D. Warren, formerly at Edinburgh and now 
at Bristol. Both are experts on Prolog.) 

Lloyd's Foundations of Logic Programming (1987) provides a theoretical explanation 
of the formal semantics of Prolog and related languages. Lassez et al. (1988) and 
Knight (1989) provide overviews of unification. 

There have been many attempts to extend Prolog to be closer to the ideal of Logic 
Programming. The language MU-Prolog and NU-Prolog (Naish 1986) and Prolog III 
(Colmerauer 1990) are particularly interesting. The latter includes a systematic 
treatment of the ^ relation and an interpretation of infinite trees. 

11.10 Exercises 
&#9635; Exercise 11.4 [m] It is somewhat confusing to see "no" printed after one or more 
valid answers have appeared. Modify the program to print "no" only when there are 
no answers at all, and "no more" in other cases. 

&#9635; Exercise 11.5 [h] At least six books (Abelson and Sussman 1985, Charniak and 
McDermottl985, Charniaketal. 1986, Hennessey 1989, Wilensky 1986, and Winston 
and Horn 1988) present unification algorithms with a common error. They all have 
problems unifying (?x ?y a) with (?y ?x ?x). Some of these texts assume that uni fy 


<a id='page-384'></a>

will be called in a context where no variables are shared between the two arguments. 
However, they are still suspect to the bug, as the following example points out: 

> (unify '(f (?x ?y a) (?y ?x ?x)) '(f ?z ?z)) 
((?Y . A) (?X . ?Y) (?Z ?X ?Y A)) 

Despite this subtle bug, I highly recommend each of the books to the reader. It is 
interesting to compare different implementations of the same algorithm. It turns out 
there are more similarities than differences. This indicates two things: (1) there is a 
generally agreed-upon style for writing these functions, and (2) good programmers 
sometimes take advantage of opportunities to look at other's code. 

The question is: Can you give an informal proof of the correctness of the algorithm 
presented in this chapter? Start by making a clear statement of the specification. 
Apply that to the other algorithms, and show where they go wrong. Then see if you 
can prove that the unify function in this chapter is correct. Failing a complete proof, 
can you at least prove that the algorithm will always terminate? See Norvig 1991 for 
more on this problem. 

&#9635; Exercise 11.6 [h] Since logic variables are so basic to Prolog, we would like them 
to be efficient. In most implementations, structures are not the best choice for small 
objects. Note that variables only have two slots: the name and the binding. The 
binding is crucial, but the name is only needed for printing and is arbitrary for most 
variables. This suggests an alternative implementation. Each variable will be a 
cons cell of the variable's binding and an arbitrary marker to indicate the type. This 
marker would be checked by vari abl e-p. Variable names can be stored in a hash 
table that is cleared before each query. Implement this representation for variables 
and compare it to the structure representation. 

&#9635; Exercise 11.7 [m] Consider the following alternative implementation for anonymous 
variables: Leave the macros <- and ?- alone, so that anonymous variables 
are allowed in assertions and queries. Instead, change uni fy so that it lets anything 
match against an anonymous variable: 

(defun unify (x y &optional (bindings no-bindings)) 
"See if . and y match with given bindings." 
(cond ((eq bindings fail) fail) 

((eql . y) bindings) 
((or (eq . *?) (eq y '?)) bindings) 
((variable-p x) (unify-variable . y bindings)) 
((variable-p y) (unify-variable y . bindings)) 
((and (consp x) (consp y)) 

(unify (rest x) (rest y) 


<a id='page-385'></a>
(unify (first x) (first y) bindings))) 
(t fail))) 

Is this alternative correct? If so, give an informal proof. If not, give a counterexample. 

&#9635; Exercise 11.8 Pi] Write a version of the Prolog interpreter that uses destructive 
unification instead of binding lists. 

&#9635; Exercise 11.9 [m] Write Prolog rules to express the terms father, mother, son, 
daughter, and grand- versions of each of them. Also define parent, child, wife, 
husband, brother, sister, uncle, and aunt. You will need to decide which relations 
are primitive (stored in the Prolog data base) and which are derived by rules. 

For example, here's a definition of grandfather that says that G is the grandfather 
of C if G is the father of some P, who is the parent of C: 

(<- (grandfather ?g ?c) 
(father ?g ?p) 
(parent ?p ?c)) 

&#9635; Exercise 11.10 [m] The following problem is presented in Wirth 1976: 

I married a widow (let's call her W) who has a grown-up daughter (call her 
D). My father (F), who visited us often, fell in love with my step-daughter and 
married her. Hence my father became my son-in-law and my step-daughter 
became my mother. Some months later, my wife gave birth to a son (Si), who 
became the brother-in-law of my father, as well as my uncle. The wife of my 
father, that is, my step-daughter, also had a son (S2). 

Represent this situation using the predicates defined in the previous exercise, 
verify its conclusions, and prove that the narrator of this tale is his own grandfather. 

&#9635; Exercise 11.11 [d] Recall the example: 

> (?- (length (abed) ?n)) 
?N = (1+ (1+ (1+ (1+ 0)))); 

It is possible to produce 4 instead of (1+ (1+ (1+ (1+ 0)))) byextendingthenotion 
of unification. Ait-Kaci et al. 1987 might give you some ideas how to do this. 


<a id='page-386'></a>

&#9635; Exercise 11.12 [h] The function rename-vari abl es was necessary to avoid confusion 
between the variables in the first argument to unify and those in the second 
argument. An alternative is to change the uni f y so that it takes two binding lists, one 
for each argument, and keeps them separate. Implement this alternative. 

11.11 Answers 
Answer 11.9 We will choose as primitives the unary predicates mal e and f emal e 
and the binary predicates chi 1 d and married. The former takes the child first; the 
latter takes the husband first. Given these primitives, we can make the following 
definitions: 

(<- (father ?f ?c) (male ?f) (parent ?f ?c)) 
(<- (mother ?m ?c) (female ?m) (parent ?m ?c)) 
(<- (son ?s ?p) (male ?s) (parent ?p ?s)) 
(<- (daughter ?s ?p) (male ?s) (parent ?p ?s)) 

(<- (grandfather ?g ?c) (father ?g ?p) (parent ?p ?c)) 
(<- (grandmother ?g ?c) (mother ?g ?p) (parent ?p ?c)) 
(<- (grandson ?gs ?gp) (son ?gs ?p) (parent ?gp ?p)) 
(<- (granddaughter ?gd ?gp) (daughter ?gd ?p) (parent ?gp ?p)) 

(<- (parent ?p ?c) (child ?c ?p)) 
(<- (wife ?w ?h) (married ?h ?w)) 
(<- (husband ?h ?w) (married ?h ?w)) 

(<- (sibling ?x ?y) (parent ?p ?x) (parent ?p ?y)) 
(<- (brother ?b ?x) (male ?b) (sibling ?b ?x)) 
(<- (sister ?s ?x) (female ?s) (sibling ?s ?x)) 
(<- (uncle ?u ?n) (brother ?u ?p) (parent ?p ?n)) 
(<- (aunt ?a ?n) (sister ?a ?p) (parent ?p ?n)) 

Note that there is no way in Prolog to express a true definition. We would like to say 
that "P is the parent of C if and only if C is the child of P," but Prolog makes us express 
the biconditional in one direction only. 


<a id='page-387'></a>

Answer 11.10 Because we haven't considered step-relations in the prior definitions, 
we have to extend the notion of parent to include step-parents. The definitions 
have to be written very carefully to avoid infinite loops. The strategy is to structure 
the defined terms into a strict hierarchy: the four primitives are at the bottom, then 
pa rent is defined in terms of the primitives, then the other terms are defined in terms 
of parent and the primitives. 

We also provide a definition for son-in-law: 

(<- (parent ?p ?c) (married ?p ?w) (child ?c ?w)) 
(<- (parent ?p ?c) (married ?h ?p) (child ?c ?w)) 
(<- (son-in-law ?s ?p) (parent ?p ?w) (married ?s ?w)) 

Now we add the information from the story. Note that we only use the four primitives 
male, female, married, and child: 

(<- (male I)) (<- (male F)) (<- (male SD) (<- (male S2)) 
(<- (female W)) (<- (female D)) 
(<- (married I W)) 
(<- (married F D)) 
(<- (child D W)) 
(<- (child I F)) 
(<- (child SI I)) 
(<- (child S2 F)) 

Now we are ready to make the queries: 

> (?- (son-in-law F I)) 
Yes. 

> (?- (mother D I)) 
Yes. 

> (?- (uncle SI I)) 
Yes. 

> (?- (grandfather I I)) 
Yes. 


## Chapter 12
<a id='page-388'></a>

Compiling Logic 
Programs 

V I 1 he end of chapter 11 introduced a new, more efficient representation for logic variables. 

I It would be reasonable to build a new version of the Prolog interpreter incorporating 

JL this representation. However, chapter 9 has taught us that compilers run faster than 
interpreters and are not that much harder to build. Thus, this chapter will present a Prolog 
compiler that translates from Prolog to Lisp. 

Each Prolog predicate will be translated into a Lisp function, and we will adopt the convention 
that a predicate called with a different number of arguments is a different predicate. If the symbol 
. can be called with either one or two arguments, we will need two Lisp functions to implement 
the two predicates. Following Prolog tradition, these will be called p/1 and p/2. 

The next step is to decide what the generated Lisp code should look like. It must unify 
the head of each clause against the arguments, and if the unification succeeds, it must call the 
predicates in the body. The difficult part is that the choice points have to be remembered. If 
a call to a predicate in the first clause fails, we must be able to return to the second clause and 
try again. 


<a id='page-389'></a>

This can be done by passing in a success continuation as an extra argument to 
every predicate. This continuation represents the goals that remain unsolved, the 
other-goal s argument of prove. For each clause in the predicate, if all the goals iri a 
clause succeed, then we should call the success continuation. If a goal fails, we don't 
do anything special; we just go on to the next clause. There is one complication: after 
failing we have to undo any bindings made by uni fy I. Consider an example. The 
clauses 

(<- (likes Robin cats)) 

(<- (likes Sandy ?x) (likes ?x cats)) 

(<- (likes Kim ?x) (likes ?x Lee) (likes ?x Kim)) 

could be compiled into this: 

(defun likes/2 (?argl ?arg2 cont) 
First clause: 
(if (and (unify! ?argl 'Robin) (unify! ?arg2 'cats)) 
(funcall cont)) 
(undo-bindings) 
Second clause: 
(if (unify! ?argl 'Sandy) 
(likes/2 ?arg2 'cats cont)) 
(undo-bindings) 
Third clause: 
(if (unify! ?argl 'Kim) 
(likes/2 ?arg2 'Lee 
#'(lambda () (likes/2 ?arg2 'Kim cont)))))) 

In the first clause, we just check the two arguments and, if the unifications succeed, 

call the continuation directly, because the first clause has no body. In the second 

clause, 1 i kes/2 is called recursively, to see if ?arg2 likes cats. If this succeeds, then 

the original goal succeeds, and the continuation cont is called. In the third clause, 

we have to call1 i kes/2 recursively again, this time requesting that it check if ?arg2 

likes Lee. If this check succeeds, then the continuation will be called. In this case, 

the continuation involves another call to 1 i kes/2, to check if ?arg2 likes Kim. If this 

succeeds, then the original continuation, cont, will finally be called. 

Recall that in the Prolog interpreter, we had to append the list of pending goals, 
other-goal s, to the goals in the body of the clause. In the compiler, there is no need 
to do an append. Instead, the continuation cont represents the other-goals, and the 
body of the clause is represented by explicit calls to functions. 


<a id='page-390'></a>

Note that the code for 1 i kes/2 given before has eUminated some unnecessary 
calls to uni fy!. The most obvious implementation would have one call to uni fy 1 for 
each argument. Thus, for the second clause, we would have the code: 

(if (and (unify! ?argl 'Sandy) (unifyl ?arg2 ?x)) 
(likes/2 ?x 'cats cont)) 

where we would need a suitable let binding for the variable ?x. 

12.1 A Prolog Compiler 
This section presents the compiler summarized in figure 12.1. At the top level is 
the function prol og-compi 1 e, which takes a symbol, looks at the clauses defined for 
that symbol, and groups the clauses by arity. Each symbol/arity is compiled into a 
separate Lisp function by compi 1 e-predi cate. 

(defun prolog-compile (symbol &optional 

(clauses (get-clauses symbol))) 
"Compile a symbol; make a separate function for each arity." 
(unless (null clauses) 

(let ((arity (relation-arity (clause-head (first clauses))))) 
;; Compile the clauses with this arity 
(compile-predicate 

symbol arity (clauses-with-arity clauses #'= arity)) 
;; Compile all the clauses with any other arity 
(prolog-compile 

symbol (clauses-with-arity clauses #'/= arity))))) 

Three utility functions are included here: 

(defun clauses-with-arity (clauses test arity) 
"Return all clauses whose head has given arity." 
(find-all arity clauses 

:key #'(lambda (clause) 
(relation-arity (clause-head clause))) 
rtest test)) 

(defun relation-arity (relation) 
"The number of arguments to a relation. 
Example: (relation-arity '(p a b c)) => 3" 
(length (args relation))) 

(defun args (x) "The arguments of a relation" (rest x)) 

The next step is to compile the clauses for a given predicate with a fixed arity into a 


<a id='page-391'></a>
?-

nrail* 

var 

top-level-prove 
run-prolog 

prOlog-compi1e-symbols 
prolog-compile 
compile-predicate 
compile-clause 
compile-body 
compile-call 
compile-arg 
compile-unify 

clauses-with-arity 
relation-arity 
args 
make-parameters 
make-predicate 
make-= 
def-prolog-compi1er-macro 
prolog-compi1er-macro 
has-variable-p 
proper-listp 
maybe-add-undo-bindings 
bind-unbound-vars 
make-anonymous 
anonymous-variables-in 
compile-if 
compile-unify-vari able 
bind-variables-in 
follow-binding 
bind-new-variables 
ignore 

unify! 
undo-bindings! 
binding-val 
symbol 
new-symbol 
find-anywhere 

Top-Level Functions 

Make a query, but compile everything first. 

Special Variables 

A list of all bindings made so far. 

Data Types 

A box for a variable; can be destructively modified. 

Major Functions 

New version compiles everything first. . 
Compile everything and call a Prolog function. 
Compile a list of Prolog symbols. 
Compile a symbol; make a separate function for each arity. 
Compile all the clauses for a given symbol/arity. 
Transform away the head and compile the resulting body. 
Compile the body of a clause. 
Compile a call to a Prolog predicate. 
Generate code for an argument to a goal in the body. 
Return code that tests if var and term unify. 

Auxiliary Functions 

Return all clauses whose head has given arity. 
The number of arguments to a relation. 
The arguments of a relation. 
Build a list of parameters. 
Build a symbol of the form name/ari ty. 
Build a unification relation. 
Define a compiler macro for Prolog. 
Fetch the compiler macro for a Prolog predicate. 
Is there a variable anywhere in the expression x? 
IsX a proper (non-dotted) list? 
Undo any bindings that need undoing. 
Add a let if needed. 
Replace variables that are only used once with ?. 
A list of anonymous variables. 
Compile an IF form. No else-part allowed. 
Compile the unification of a var. 
Bind all variables in exp to themselves. 
Get the ultimate binding of var according to bindings. 

Extend bindings to include any unbound variables. 
Do nothing—ignore the arguments. 

Previously Defined Fimctions 

Destructive unification (see section 11.6). 

Use the trail to backtrack, undoing bindings. 
Pick out the value part of a var/val binding. 
Create or find an interned symbol. 
Create a new uninterned symbol. 
Does item occur anywhere in tree? 

Figure 12.1: Glossary for the Prolog Compiler 


<a id='page-392'></a>

Lisp function. For now, that will be done by compiling each clause indepently and 
wrapping them in a 1 ambda with the right parameter list. 

(defun compile-predicate (symbol arity clauses) 
"Compile all the clauses for a given symbol/arity 
into a single LISP function." 
(let ((predicate (make-predicate symbol arity)) 

(parameters (make-parameters arity))) 
(compile 
(eval 
'(defun .predicate (,parameters cont) 
..(mapcar #*(lambda (clause) 
(compile-clause parameters clause 'cont)) 
clauses)))))) 

(defun make-parameters (arity) 
"Return the list (?argl ?arg2 ... ?arg-arity)" 
(loop for i from 1 to arity 

collect (new-symbol '?arg i))) 

(defun make-predicate (symbol arity) 
"Return the symbol: symbol/arity" 
(symbol symbol V arity)) 

Now for the hard part: we must actually generate the code for a clause. Here again 
is an example of the code desired for one clause. We'll start by setting as a target the 
simple code: 

(<- (likes Kim ?x) (likes ?x Lee) (likes ?x Kim)) 

(defun likes/2 (?argl ?arg2 cont) 

(if (and (unify! ?argl *Kim) (unify! ?arg2 ?x) 
(likes/2 ?arg2 'Lee 
#'(lambda () (likes/2 ?x 'Kim)))) 

...) 

but we'll also consider the possibility of upgrading to the improved code: 

(defun likes/2 (?argl ?arg2 cont) 

(if (unify! ?argl 'Kim) 
(likes/2 ?arg2 'Lee 
#'(lambda () (likes/2 ?arg2 'Kim)))) 

...) 

One approach would be to write two functions, compi 1 e-head and compi 1 e-body. 


<a id='page-393'></a>
and then combine them into the code ( i f head body). This approach could easily 
generate the prior code. However, let's allow ourselves to think ahead a little. If we 
eventually want to generate the improved code, we will need some communication 
between the head and the body. We will have to know that the head decided not 
to compile the unification of ?arg2 and ?x, but because of this, the body will have 
to substitute ?arg2 for ?x. That means that the compi 1 e - head function conceptually 
returns two values: the code for the head, and an indication of substitutions to 
perform in the body. This could be handled by explicitly manipulating multiple 
values, but it seems complicated. 

An alternate approach is to eliminate compi 1 e - head and just write compi 1 e - body. 
This is possible if we in effect do a source-code transformation on the clause. Instead 
of treating the clause as: 

(<- (likes Kim ?x) 
(likes ?x Lee) (likes ?x Kim)) 

we transform it to the equivalent: 

(<- (likes ?argl ?arg2) 
(= ?argl Kim) (= ?arg2 ?x) (likes ?x Lee) (likes ?x Kim)) 

Now the arguments in the head of the clause match the arguments in the function 
1 i kes/2, so there is no need to generate any code for the head. This makes things 
simpler by eliminating compi 1 e-head, and it is a better decomposition for another 
reason: instead of adding optimizations to compi 1 e-head, we will add them to the 
code in compi 1 e-body that handles =. That way, we can optimize calls that the user 
makes to =, in addition to the calls introduced by the source-code transformation. 

To get an overview, the calling sequence of functions will turn out to be as follows: 

prolog-compile 
compile-predicate 
compile-clause 

compile-body 
compile-call 
compile-arg 
compile-unify 

compile-arg 

where each function calls the ones below it that are indented one level. We have already 
defined the first two functions. Here thenisourfirstversionof compi 1 e-cl ause: 


<a id='page-394'></a>

(defun compile-clause (parms clause cont) 
"Transform away the head, and compile the resulting body." 
(compile-body 

(nconc 
(mapcar #'make-= parms (args (clause-head clause))) 
(clause-body clause)) 

cont)) 

(defun make-= (x y) *(= .x .y)) 

The bulk of the work is in compi 1 e - body, which is a little more complicated. There are 
three cases. If there is no body, we just call the continuation. If the body starts with 
a call to =, we compile a call to uni fy!. Otherwise, we compile a call to a function, 
passing in the appropriate continuation. 

However, it is worthwhile to think ahead at this point. If we want to treat = 
specially now, we will probably want to treat other goals specially later. So instead 
of explicitly checking for =, we will do a data-driven dispatch, looking for any predicate 
that has a prol og-compi 1 er-macro property attached to it. Like Lisp compiler 
macros, the macro can decline to handle the goal. We will adopt the convenhon that 
returning .-pass means the macro decided not to handle it, and thus it should be 
compiled as a normal goal. 

(defun compile-body (body cont) 
"Compile the body of a clause." 
(if (null body) 

Mfuncall .cont) 

(let* ((goal (first body)) 
(macro (prolog-compiler-macro (predicate goal))) 
(macro-val (if macro 

(funcall macro goal (rest body) cont)))) 

(if (and macro (not (eq macro-val :pass))) 
macro-val 
(compile-cal 1 

(make-predicate (predicate goal) 
(relation-arity goal)) 
(mapcar #'(lambda (arg) (compile-arg arg)) 
(args goal)) 

(if (null (rest body)) 
cont 
'#'(lambda () 

.(compile-body (rest body) cont)))))))) 

(defun compile-call (predicate args cont) 
"Compile a call to a prolog predicate." 
'(.predicate .@args .cont)) 


<a id='page-395'></a>
(defun prolog-compiler-macro (name) 
"Fetch the compiler macro for a Prolog predicate." 
Note NAME is the raw name, not the name/arity 
(get name 'prolog-compiler-macro)) 

(defmacro def-prolog-compi1er-macro (name arglist &body body) 
"Define a compiler macro for Prolog." 
'(setf (get ',name 'prolog-compiler-macro) 

#'(lambda .arglist ..body))) 

(def-prolog-compi1er-macro = (goal body cont) 
(let ((args (args goal))) 
(if (/= (length args) 2) 
.-pass 

'(if.(compile-unify (first args) (second args)) 
.(compile-body body cont))))) 
(defun compile-unify (x y) 
"Return code that tests if var and term unify." 
'(unify! .(compile-arg x) .(compile-arg y))) 

All that remains is compi1 e-arg, a function to compile the arguments to goals in the 
body. There are three cases to consider, as shown in the compilation to the argument 
of q below: 

1 (<- (p ?x) (q ?x)) (q/1 ?x cont) 

2 (<- (p ?x) (q (f a b))) (q/1 '(f a b) cont) 

3 (<- (p ?x) (q (f ?x b))) (q/1 (list 'f ?x 'b) cont) 

In case 1, the argument is a variable, and it is compiled as is. In case 2, the argument 
is a constant expression (one without any variables) that compiles into a quoted 
expression. In case 3, the argument contains a variable, so we have to generate code 
that builds up the expression. Case 3 is actually split into two in the list below: one 
compiles into a call to 1 i st, and the other a call to cons. It is important to remember 
that the goal (q (f ?x b)) does not involve a call to the function f. Rather, it involves 
the term (f ?x b), which is just a list of three elements. 

(defun compile-arg (arg) 
"Generate code for an argument to a goal in the body." 
(cond ((variable-p arg) arg) 

((not (has-variable-p arg)) ".arg) 
((proper-listp arg) 
'(list ..(mapcar #'compile-arg arg))) 
(t '(cons .(compile-arg (first arg)) 
.(compile-arg (rest arg)))))) 


<a id='page-396'></a>

(defun has-variable-p (x) 
"Is there a variable anywhere in the expression x?" 
(find-if-anywhere #'variable-p x)) 

(defun proper-listp (x) 
"Is X a proper (non-dotted) list? " 
(or (null x) 

(and (consp x) (proper-listp (rest x))))) 

Let's see how it works. We will consider the following clauses: 

(<- (likes Robin cats)) 
(<- (likes Sandy ?x) (likes ?x cats)) 
(<- (likes Kim ?x) (likes ?x Lee) (likes ?x Kim)) 

(<- (member ?item (?item . ?rest))) 
(<- (member ?item (?x . ?rest)) (member Titem ?rest)) 

Here's what prol og-compi 1 e gives us: 

(DEFUN LIKES/2 (7ARG1 ?ARG2 CONT) 
(IF (UNIFY! ?ARG1 'ROBIN) 
(IF (UNIFY! 7ARG2 'CATS) 
(FUNCALL CONT))) 
(IF (UNIFY! ?ARG1 'SANDY) 
(IF (UNIFY! ?ARG2 ?X) 
(LIKES/2 ?X 'CATS CONT))) 
(IF (UNIFY! 7ARG1 'KIM) 
(IF (UNIFY! ?ARG2 ?X) 
(LIKES/2 ?X 'LEE (LAMBDA () 
(LIKES/2 ?X 'KIM CONT)))))) 

(DEFUN MEMBER/2 (7ARG1 7ARG2 CONT) 
(IF (UNIFY! 7ARG1 7ITEM) 
(IF (UNIFY! 7ARG2 (CONS 7ITEM 7REST)) 
(FUNCALL CONT))) 
(IF (UNIFY! 7ARG1 7ITEM) 
(IF (UNIFY! 7ARG2 (CONS 7X 7REST)) 
(MEMBER/2 7ITEM 7REST CONT)))) 


<a id='page-397'></a>
12.2 Fixing the Errors in the Compiler 
There are some problems in this version of the compiler: 

* We forgot to undo the bindings after each call to uni fy!. 
* The definition of undo-bi ndi ngs! defined previously requires as an argument 
an index into the *trai 1 * array. So we will have to save the current top of the 
trail when we enter each function. 
* Local variables, such as ?x, were used without being introduced. They should 
be bound to new variables. 
Undoing the bindings is simple: we add a single line to compile-predicate, 
a call to the function maybe-add-undo-bindings. This function inserts a call to 
undo-bi ndi ngs! after every failure. If there is only one clause, no undoing is necessary, 
because the predicate higher up in the calling sequence will do it when it fails. 
If there are multiple clauses, the function wraps the whole function body in a . et 
that captures the initial value of the trail's fill pointer, so that the bindings can be 
undone to the right point. Similarly, we can handle the unbound-variable problem 
by wrapping a call to bind - unbound- va rs around each compiled clause: 

(defun compile-predicate (symbol arity clauses) 
"Compile all the clauses for a given symbol/arity 
into a single LISP function." 
(let ((predicate (make-predicate symbol arity)) 

(parameters (make-parameters arity))) 
(compile 
(eval 
'(defun .predicate (,parameters cont) 
(maybe-add-undo-bindings 
(mapcar #*(lambda (clause) 
(compile-clause parameters 
clause 'cont)) 
clauses))))))) 

(defun compile-clause (parms clause cont) 
"Transform away the head, and compile the resulting body." 
(bind-unbound-vars 

parms 
(compi1e-body 

(nconc 
(mapcar #'make-= parms (args (clause-head clause))) 
(clause-body clause)) 

cont))) 


<a id='page-398'></a>

(defun maybe-add-undo-bindings (compiled-exps) 
"Undo any bindings that need undoing. 
If there are any, bind the trail before we start." 
(if (length=1 compiled-exps) 

compiled-exps 

'((let ((old-trail (fill-pointer nrail*))) 
,(first compiled-exps) 
,@(loop for exp in (rest compiled-exps) 
collect '(undo-bindings! old-trail) 
collect exp))))) 

(defun bind-unbound-vars (parameters exp) 
"If there are any variables in exp (besides the parameters) 
then bind them to new vars." 
(let ((exp-vars (set-difference (variables-in exp) 

parameters))) 
(if exp-vars 
'(let .(mapcar #'(lambda (var) *(.var (?))) 
exp-vars) 
,exp) 
exp))) 

With these improvements, here's the code we get for 1 i kes and member: 

(DEFUN LIKES/2 (?ARG1 ?ARG2 CONT) 
(LET ((OLD-TRAIL (FILL-POINTER *TRAIL*))) 
(IF (UNIFY! ?ARG1 'ROBIN) 
(IF (UNIFY! ?ARG2 'CATS) 

(FUNCALL CONT))) 
(UNDO-BINDINGS! OLD-TRAIL) 
(LET ((?X (?))) 

(IF (UNIFY! ?ARG1 'SANDY) 
(IF (UNIFY! ?ARG2 ?X) 

(LIKES/2 ?X 'CATS CONT)))) 
(UNDO-BINDINGS! OLD-TRAIL) 
(LET ((?X (?))) 

(IF (UNIFY! ?ARG1 'KIM) 
(IF (UNIFY! ?ARG2 ?X) 
(LIKES/2 ?X 'LEE (LAMBDA () 
(LIKES/2 ?X 'KIM CONT)))))))) 


<a id='page-399'></a>

(DEFUN MEMBER/2 (?ARG1 ?ARG2 CONT) 
(LET ((OLD-TRAIL (FILL-POINTER *TRAIL*))) 
(LET ((?ITEM (?)) 
(?RE$T (?))) 
(IF (UNIFY! ?ARG1 ?ITEM) 
(IF (UNIFY! ?ARG2 (CONS ?ITEM ?REST)) 

(FUNCALL CONT)))) 
(UNDO-BINDINGS! OLD-TRAIL) 
(LET ((?X (?)) 

(?ITEM (?)) 
(?REST (?))) 
(IF (UNIFY! ?ARG1 ?ITEM) 
(IF (UNIFY! ?ARG2 (CONS ?X ?REST)) 
(MEMBER/2 ?ITEM ?REST CONT)))))) 

12.3 Improving the Compiler 
This is fairly good, although there is still room for improvement. One minor improvement 
is to eliminate unneeded variables. For example, ?rest in the first clause of 
member and ?x in the second clause are bound to new variables—the result of the (?) 
call—and then only used once. The generated code could be made a little tighter by 
just putting (?) inline, rather than binding it to a variable and then referencing that 
variable. There are two parts to this change: updating compi 1 e-arg to compile an 
anonymous variable inline, and changing the < - macro so that it converts all variables 
that only appear once in a clause into anonymous variables: 

(defmacro <- (&rest clause) 
"Add a clause to the data base." 
'(add-clause '.(make-anonymous clause))) 

(defun compile-arg (arg) 
"Generate code for an argument to a goal in the body." 
(cond ((eq arg '?) '(?)) 

((variable-p arg) arg) 
((not (has-variable-p arg)) ",arg) 
((proper-listp arg) 

'(list ..(mapcar #'compile-arg arg))) 
(t '(cons .(compile-arg (first arg)) 
.(compile-arg (rest arg)))))) 
(defun make-anonymous (exp &optional 
(anon-vars (anonymous-variables-in exp))) 
"Replace variables that are only used once with ?. " 
(cond ((consp exp) 

(reuse-cons (make-anonymous (first exp) anon-vars) 


<a id='page-400'></a>

(make-anonymous (rest exp) anon-vars) 

exp)) 
((member exp anon-vars) *?) 
(t exp))) 

Finding anonymous variables is tricky. The following function keeps two lists: the 
variables that have been seen once, and the variables that have been seen twice 
or more. The local function wal k is then used to walk over the tree, recursively 
considering the components of each cons cell and updating the two lists as each 
variable is encountered. This use of local functions should be remembered, as well 
as an alternative discussed in exercise 12.23 on [page 428](chapter12.md#page-428). 

(defun anonymous-variables-in (tree) 
"Return a list of all variables that occur only once in tree." 
(let ((seen-once nil) 

(seen-more nil)) 
(labels ((walk (x) 
(cond 
((variable-p x) 

(cond ((member . seen-once) 
(setf seen-once (delete . seen-once)) 
(push . seen-more)) 

((member . seen-more) nil) 
(t (push . seen-once)))) 

((consp x) 
(walk (first x)) 
(walk (rest x)))))) 

(walk tree) 
seen-once))) 

Now member compiles into this: 

(DEFUN MEMBER/2 (?ARG1 ?ARG2 CONT) 
(LET ((OLD-TRAIL (FILL-POINTER nRAIL*))) 
(LET ((?ITEM (?))) 
(IF (UNIFY! ?ARG1 ?ITEM) 
(IF (UNIFY! ?ARG2 (CONS ?ITEM (?))) 

(FUNCALL CONT)))) 
(UNDO-BINDINGS! OLD-TRAIL) 
(LET ((?ITEM (?)) 

(?REST (?))) 
(IF (UNIFY! ?ARG1 ?ITEM) 
(IF (UNIFY! ?ARG2 (CONS (?) ?REST)) 
(MEMBER/2 ?ITEM ?REST CONT)))))) 


<a id='page-401'></a>
12.4 Improving the Compilation of Unification 
Now we turn to the improvement of compi1 e - un i f y. Recall that we want to eliminate 
certain calls to uni fy ! so that, for example, the first clause of member: 

(<- (member ?item (?item . ?rest))) 

compiles into: 

(LET ((?ITEM (?))) 
(IF (UNIFY! ?ARG1 ?ITEM) 
(IF (UNIFY! ?ARG2 (CONS ?ITEM (?))) 
(FUNCALL CONT)))) 

when it could compile to the more efficient: 

(IF (UNIFY! ?ARG2 (CONS ?ARG1 (?))) 
(FUNCALL CONT)) 

Eliminating the unification in one goal has repercussions in other goals later on, so 
we will need to keep track of expressions that have been unified together. We have 
a design choice. Either compi1 e-unify can modify a global state variable, or it can 
return multiple values. On the grounds that global variables are messy, we make the 
second choice: compi 1 e- uni fy will take a binding list as an extra argument and will 
return two values, the actual code and an updated binding list. We will expect that 
other related functions will have to be modified to deal with these multiple values. 

When compi le -unify is first called in our example clause, it is asked to unify 
?argl and ?item. We want it to return no code (or more precisely, the trivially true 
test, t). For the second value, it should return a new binding list, with ?i tem bound 
to ?argl. That binding will be used to replace ?i tem with ?argl in subsequent code. 

How do we know to bind ?item to ?argl rather than the other way around? 
Because ?argl is already bound to something—the value passed in to member. We 
don't know what this value is, but we can't ignore it. Thus, the initial binding list will 
have to indicate that the parameters are bound to something. A simple convention 
is to bind the parameters to themselves. Thus, the initial binding list will be: 

((?argl . ?argl) (?arg2 . ?arg2)) 

We saw in the previous chapter ([page 354](chapter11.md#page-354)) that binding a variable to itself can lead to 
problems; we will have to be careful. 
Besides eliminating unifications of new variables against parameters, there are 
quite a few other improvements that can be made. For example, unifications involv



<a id='page-402'></a>

ing only constants can be done at compile time. The call (= (f a) (f a)) always 
succeeds, while (=3 4) always fails. In addition, unification of two cons cells can 
be broken into components at compile time: (= (f ?x) (f a)) reduces to (= ?x 

a) and (= f f), where the latter trivially succeeds. We can even do some occm-s 
checking at compile time: (= ?x (f ?x)) should fail. 
The following table lists these improvements, along with a breakdown for the 
cases of unifying a bound (? a rg1) or unbound (?x) variable agains another expression. 
The first column is the unification call, the second is the generated code, and the third 
is the bindings that will be added as a result of the call: 

Unification Code Bindings 
1 (= 3 3) t — 
2 (= 3 4) nil — 
3 (= (f ?x) (?p 3)) t (?x . 3) (?p . f) 
4 (= ?argl ?y) t (?y . ?argl) 
5 (= ?argl ?arg2) (unify! ?argl ?arg2) (?argl . ?arg2) 
6 (= ?argl 3) (unify! ?argl 3) (?argl . 3) 
7 (= ?argl (f ?y)) (unify! ?argl ...) (?y . ?y) 
8 (= ?x ?y) t (?x . ?y) 
9 (= ?x 3) t (?x . 3) 

10 (= ?x (f ?y)) (unify! ?x ...) (?y . ?y) 
11 (= ?x (f ?x)) nil — 
12 (= ?x ?) t 

-

From this table we can craft our new version of compi1 e-uni fy. The first part 
is fairly easy. It takes care of the first three cases in this table and makes stue 
that compi 1 e-uni fy-vari abl e is called with a variable as the first argument for the 
other cases. 

(defun compile-unify (x y bindings) 
"Return 2 values: code to test if . and y unify, 
and a new binding list." 
(cond 

Unify constants and conses: ; Case 
((not (or (has-variable-p x) (has-variable-p y))) ; 1,2 
(values (equal . y) bindings)) 
((and (consp x) (consp y)) : 3 
(multiple-value-bind (codel bindingsl) 
(compile-unify (first x) (first y) bindings) 
(multiple-value-bind (code2 bindings2) 
(compile-unify (rest x) (rest y) bindingsl) 
(values (compile-if codel code2) bindings2)))) 

Here . or y is a variable. Pick the right one: 
((variable-p x) (compi1e-unify-variable . y bindings)) 
(t (compile-unify-variab1e y . bindings)))) 


<a id='page-403'></a>
(defun compile-if (pred then-part) 
"Compile a Lisp IF form. No else-part allowed." 
(case pred 

((t) then-part) 
((nil) nil) 
(otherwise *(if ,pred .then-part)))) 

The function compi 1 e - uni fy - va r i abl e following is one of the most complex we have 

seen. For each argument, we see if it has a binding (the local variables xb and yb), 

and then use the bindings to get the value of each argument (xl and y 1). Note that for 

either an unbound variable or one bound to itself, . will equal xl (and the same for y 

andyl). If either of the pairs of values is not equal, we should use the new ones (xl or 

y 1), and the clause commented deref does that. After that point, we just go through 

the cases, one at a time. It turns out that it was easier to change the order slightly from 

the preceding table, but each clause is commented with the corresponding number: 

(defun compile-unify-vari able (x y bindings) 
"X is a variable, and Y may be." 
(let* ((xb (follow-binding . bindings)) 

(xl (if xb (cdr xb) x)) 
(yb (if (variable-p y) (follow-binding y bindings))) 
(yl (if yb (cdr yb) y))) 

(cond ; Case: 
((or (eq . *?) (eq y *?)) (values t bindings)) ; 12 
((not (and (equal . xl) (equal y yl))) ; deref 

(compile-unify xl yl bindings)) 
((find-anywhere xl yl) (values nil bindings)) ; 11 
((consp yl) ; 7.10 

(values '(unifyl .xl .(compile-arg yl bindings)) 
(bind-variables-in yl bindings))) 

((not (null xb)) 
;.. i.e. X is an ?arg variable 
(if (and (variable-p yl) (null yb)) 

(values 't (extend-bindings yl xl bindings)) ; 4 
(values '(unify! .xl ,(compile-arg yl bindings)) 
(extend-bindings xl yl bindings)))) ; 5.6 
((not (null yb)) 
(compile-unify-variable yl xl bindings)) 
(t (values 't (extend-bindings xl yl bindings)))))) ; 8.9 

Take some time to understand just how this function works. Then go on to the 
following auxiliary functions: 


<a id='page-404'></a>

(defun bind-variables-in (exp bindings) 
"Bind all variables in exp to themselves, and add that to 
bindings (except for variables already bound)." 
(dolist (var (variables-in exp)) 

(unless (get-binding var bindings) 
(setf bindings (extend-bindings var var bindings)))) 
bindings) 

(defun follow-binding (var bindings) 
"Get the ultimate binding of var according to bindings." 
(let ((b (get-binding var bindings))) 

(if (eq (car b) (cdr b)) 
b 
(or (follow-binding (cdr b) bindings) 

b)))) 

Now we need to integrate the new compi 1 e - uni f y into the rest of the compiler. The 
problem is that the new version takes an extra argument and returns an extra value, 
so all the functions that call it need to be changed. Let's look again at the calling 
sequence: 

prolog-compile 
compile-predicate 
compile-clause 

compile-body 
compile-call 
compile-arg 

compile-unify 
compile-arg 

First, going downward, we see that compi 1 e-arg needs to take a binding Ust as an 
argument, so that it can look up and substitute in the appropriate values. But it will 
not alter the binding list, so it still returns one value: 

(defun compile-arg (arg bindings) 
"Generate code for an argument to a goal in the body." 
(cond ((eq arg *?) '(?)) 

((variable-p arg) 
(let ((binding (get-binding arg bindings))) 
(if (and (not (null binding)) 

(not (eq arg (binding-val binding)))) 
(compile-arg (binding-val binding) bindings) 
arg))) 

((not (find-if-anywhere #'variable-p arg)) ".arg) 
((proper-listp arg) 

'(list ..(mapcar #*(lambda (a) (compile-arg a bindings)) 

<a id='page-405'></a>

arg))) 
(t '(cons ,(compile-arg (first arg) bindings) 
.(compile-arg (rest arg) bindings))))) 

Now, going upward, compile-body needs to take a binding list and pass it on to 
various functions: 

(defun compile-body (body cont bindings) 
"Compile the body of a clause." 
(cond 

((null body) 
'(funcall .cont)) 

(t (let* ((goal (first body)) 
(macro (prolog-compiler-macro (predicate goal))) 
(macro-val (if macro 

(funcall macro goal (rest body) 
contbindings)))) 

(if (and macro (not (eq macro-val rpass))) 
macro-val 
(compile-cal1 

(make-predicate (predicate goal) 
(relation-arity goal)) 
(mapcar #*(lambda (arg) 
(compile-arg arg bindings)) 
(args goal)) 

(if (null (rest body)) 
cont 
'#'(lambda () 

.(compile-body 
(rest body) cont 
(bind-new-variables bindings goal)))))))))) 

The function bind -new-variables takes any variables mentioned in the goal that 
have not been bound yet and binds these variables to themselves. This is because 
the goal, whatever it is, may bind its arguments. 

(defun bind-new-variables (bindings goal) 
"Extend bindings to include any unbound variables in goal." 
(let ((variables (remove-if #'(lambda (v) (assoc . bindings)) 

(variables-in goal)))) 
(nconc (mapcar #*self-cons variables) bindings))) 

(defun self-cons (x) (cons . .)) 

One of the functions that needs to be changed to accept a binding list is the compiler 
macro for =: 


<a id='page-406'></a>

(def-prolog-compiler-macro = (goal body cont bindings) 
"Compile a goal which is a call to =. " 
(let ((args (args goal))) 

(if (/= (length args) 2) 
:pass decline to handle this goal 
(multiple-value-bind (codel bindingsl) 

(compile-unify (first args) (second args) bindings) 

(compile-if 
codel 
(compile-body body cont bindingsl)))))) 

The last step upward is to change compi 1 e-cl ause so that it starts everything off by 
passingin to comp i 1 e - body a binding list with all the parameters bound to themselves: 

(defun compile-clause (parms clause cont) 
"Transform away the head, and compile the resulting body." 
(bind-unbound-vars 

parms 

(compile-body 

(nconc 
(mapcar #*make-= parms (args (clause-head clause))) 
(clause-body clause)) 

cont 

(mapcar #'self-cons parms)))) 

Finally, we can see the fruits of our efforts: 

(DEFUN MEMBER/2 (?ARG1 ?ARG2 CONT) 
(LET ((OLD-TRAIL (FILL-POINTER *TRAIL*))) 
(IF (UNIFYl ?ARG2 (CONS ?ARG1 (?))) 

(FUNCALL CONT)) 
(UNDO-BINDINGS! OLD-TRAIL) 
(LET ((?REST (?))) 

(IF (UNIFY! ?ARG2 (CONS (?) ?REST)) 
(MEI^BER/2 ?ARG1 ?REST CONT))))) 

(DEFUN LIKES/2 (?ARG1 ?ARG2 CONT) 
(LET ((OLD-TRAIL (FILL-POINTER *TRAIL*))) 
(IF (UNIFY! ?ARG1 'ROBIN) 
(IF (UNIFY! ?ARG2 'CATS) 

(FUNCALL CONT))) 
(UNDO-BINDINGS! OLD-TRAIL) 
(IF (UNIFY! ?ARG1 'SANDY) 

(LIKES/2 ?ARG2 'CATS CONT)) 
(UNDO-BINDINGS! OLD-TRAIL) 
(IF (UNIFY! ?ARG1 'KIM) 

(LIKES/2 ?ARG2 'LEE (LAMBDA () 
(LIKES/2 ?ARG2 'KIM CONT)))))) 


<a id='page-407'></a>
12.5 Further Improvements to Unification 
Could compile-unify be improved yet again? If we insist that it call unifyl, it 
seems that it can't be made much better. However, we could improve it by in effect 
compiling unify!. This is a key idea in the Warren Abstract Machine, or WAM, 
which is the most commonly used model for Prolog compilers. 

We call uni fy! in four cases (5, 6, 7, and 10), and in each case the first argument 
is a variable, and we know something about the second argument. But the first 
thing uni fy! does is redundantly test if the first argument is a variable. We could 
eliminate unnecessary tests by calling more specialized functions rather than the 
general-purpose function uni fy!. Consider this call: 

(unify! ?arg2 (cons ?argl (?))) 

If ?arg2 is an unbound variable, this code is appropriate. But if ?arg2 is a constant 
atom, we should fail immediately, without allowing cons and ? to generate garbage. 
We could change the test to: 

(and (consp-or-variable-p ?arg2) 
(unify-first! ?arg2 ?argl) 
(unify-rest! ?arg2 (?))) 

with suitable definitions for the functions referenced here. This change should 
speed execution time and limit the amount of garbage generated. Of course, it makes 
the generated code longer, so that could slow things down if the program ends up 
spending too much time bringing the code to the processor. 

&#9635; Exercise 12.1 [h] Write definitions for consp-or-variable-p, unify-firstl, and 
uni fy - rest!, and change the compiler to generate code like that outlined previously. 
You might want to look at the function compile-rule in section 9.6, starting on 
[page 300](chapter9.md#page-300). This function compiled a call to pat-match into individual tests; now we 
want to do the same thing to uni fy!. Run some benchmarks to compare the altered 
compiler to the original version. 

&#9635; Exercise 12.2 [h] We can gain some more efficiency by keeping track of which 
variables have been dereferenced and calling an appropriate unification function: 
either one that dereferences the argument or one that assumes the argument has 
already been dereferenced. Implement this approach. 

&#9635; Exercise 12.3 [m] What code is generated for (= (f (g ?x) ?y) (f ?y (?p a)))? 


<a id='page-408'></a>

What more efficient code represents the same unification? How easy is it to change 

the compiler to get this more efficient result? 

&#9635; Exercise 12.4 [h] In retrospect, it seems that binding variables to themselves, as 
in (?argl . ?argl), was not such a good idea. It complicates the meaning of 
bindings, and prohibits us from using existing tools. For example, I had to use 
find-anywhere instead of occur-check for case 11, because occur-check expects 
a noncircular binding list. But find-anywhere does not do as complete a job as 
occur-check. Write a version of compi 1 e - uni fy that returns three values: the code, 
a noncircular binding list, and a list of variables that are bound to unknown values. 

&#9635; Exercise 12.5 [h] An alternative to the previous exercise is not to use binding lists at 
all. Instead, we could pass in a list of equivalence classes—that is, a list of lists, where 
each sublist contains one or more elements that have been unified. In this approach, 
the initial equivalence class Hst would be ((?argl) (?arg2)). After unifying ?argl 
with ?x,?arg2 with ?y, and ?x with 4, the list would be ((4 ?argl ?x) (?arg2 ?y)). 
This assumes the convention that the canonical member of an equivalence class (the 
one that will be substituted for all others) comes first. Implement this approach. 
What advantages and disadvantages does it have? 

12.6 The User Interface to the Compiler 
The compiler can translate Prolog to Lisp, but that does us no good unless we can 
conveniently arrange to compile the right Prolog relations and call the right Lisp 
functions. In other words, we have to integrate the compiler with the <- and ? 
macros. Surprisingly, we don't need to change these macros at all. Rather, we 
will change the functions these macros call. When a new clause is entered, we will 
enter the clause's predicate in the list *uncompi 1 ed*. This is a one-line addition to 
add-clause: 

(defvar *uncompiled* nil 
"Prolog symbols that have not been compiled.") 

(defun add-clause (clause) 
"Add a clause to the data base, indexed by head's predicate." 
;; The predicate must be a non-variable symbol, 
(let ((pred (predicate (clause-head clause)))) 

(assert (and (symbolp pred) (not (variable-p pred)))) 
(pushnew pred *db-predicates*) 
(pushnew pred *uncompiled*) 
(setf (get pred 'clauses) 


<a id='page-409'></a>

(nconc (get-clauses pred) (list clause))) 
pred)) 

Now when a query is made, the ?- macro expands into a call to top-level - prove. 
The Hst of goals in the query, along with the show-prol og-vars goal, is added as the 
sole clause for the relation top -1 evel - query. Next, that query, along with any others 
that are on the uncompiled list, are compiled. Finally, the newly compiled top-level 
query function is called. 

(defun top-level-prove (goals) 
"Prove the list of goals by compiling and calling it." 

First redefine top-level-query 
(clear-predicate 'top-level-query) 
(let ((vars (delete *? (variables-in goals)))) 

(add-clause *((top-level-query) 
,goals 
(show-prolog-vars ,(mapcar #'symbol-name vars) 

.vars)))) 
;; Now run it 
(run-prolog 'top-level-query/0 #'ignore) 
(format t "~&No.") 
(values)) 

(defun run-prolog (procedure cont) 
"Run a O-ary prolog procedure with a given continuation." 

First compile anything else that needs it 
(prolog-compi1e-symbols) 
;; Reset the trail and the new variable counter 
(setf (fill-pointer nrail*) 0) 
(setf *var-counter* 0) 
;; Finally, call the query 
(catch 'top-level-prove 

(funcall procedure cont))) 

(defun prolog-compi1e-symbols (&optional (symbols *uncompiled*)) 
"Compile a list of Prolog symbols. 
By default, the list is all symbols that need it." 
(mapc #'prolog-compile symbols) 
(setf *uncompiled* (set-difference *uncompiled* symbols))) 

(defun ignore (&rest args) 
(declare (ignore args)) 
nil) 

Note that at the top level, we don't need the continuation to do anything. Arbitrarily, 
we chose to pass in the function ignore, which is defined to ignore its arguments. 


<a id='page-410'></a>

This function is useful in a variety of places; some programmers will proclaim it 
inline and then use a call to i gnore in place of an ignore declaration: 

(defun third-arg (x y .) 
(ignore . y) 
.) 

The compiler's calling convention is different from the interpreter, so the primitives 
need to be redefined. The old definition of the primitive show-prol og- va rs had three 
parameters: the list of arguments to the goal, a binding list, and a list of pending 
goals. The new definition ofshow-prolog-vars/2 also has three parameters, but that 
is just a coincidence. The first two parameters are the two separate arguments to the 
goal: a list of variable names and a list of variable values. The last parameter is a 
continuation function. To continue, we call that function, but to fail, we throw to the 
catch point set up in top-1 evel - prove. 

(defun show-prolog-vars/2 (var-names vars cont) 
"Display the variables, and prompt the user to see 
if we should continue. If not, return to the top level." 
(if (null vars) 

(format t "~&Yes") 

(loop for name in var-names 
for var in vars do 
(format t "~&~a = '^a" name (deref-exp var)))) 

(if (continue-p) 
(funcall cont) 
(throw 'top-level-prove nil))) 

(defun deref-exp (exp) 
"Build something equivalent to EXP with variables dereferenced." 
(if (atom (deref exp)) 

exp 

(reuse-cons 
(deref-exp (first exp)) 
(deref-exp (rest exp)) 
exp))) 

With these definitions in place, we can invoke the compiler automatically just by 
making a query with the ? - macro. 

&#9635; Exercise 12.6 [m] Suppose you define a predicate p, which calls q, and then define 

q.In some implementations of Lisp, when you make a query like (? - (. ?x)), you 
may get a warning message like "function q/1 undef i ned" before getting the correct 

<a id='page-411'></a>

answer. The problem is that each function is compiled separately, so warnings detected 
during the compilation of p/1 will be printed right away, even if the function 
q/1 will be defined later. In ANSI Common Lisp there is a way to delay the printing 
of warnings until a series of compilations are done: wrap the compilation with the 
macro wi th - compi1 at i on - uni t. Even if your implementation does not provide this 
macro, it may provide the same functionality under a different name. Find out if 
with-compilation-unit is already defined in your implementation, or if it can be 
defined. 

12.7 Benchmarking the Compiler 
Our compiled Prolog code runs the zebra puzzle in 17.4 seconds, a 16-fold speed-up 
over the interpreted version, for a rate of 740 LIPS. 
Another popular benchmark is Lisp's reverse function, which we can code as 
the rev relation: 

(<- (rev () ())) 

(<- (rev (?x . ?a) ?b) (rev ?a ?c) (concat ?c (?x) ?b)) 

(<- (concat () ?1 ?1)) 

(<- (concat (?x . ?a) ?b (?x . ?c)) (concat ?a ?b ?c)) 

rev uses the relation concat, which stands for concatenation, (concat ?a ?b ?c)is 
true when ?a concatenated to ?b yields ?c. This relationlike name is preferred over 
more procedural names like append. But rev is very similar to the following Lisp 
definitions: 

(defun rev (1) 

(if (null 1) 
nil 
(app (rev (rest 1)) 

(list (first 1))))) 

(defun app (x y) 

(if (null X) 
y 
(cons (first x) 

(app (rest x) y)))) 

Both versions are inefficient. It is possible to write an iterative version of reverse 
that does no extra consing and is tail-recursive: 


<a id='page-412'></a>

(<- (irev ?1 ?r) (irev3 ?1 () ?r)) 

(<- (irevS (?x . ?1) ?so-far ?r) (irevS ?1 (?x ?so-far) ?r)) 
(<- (irev3 () ?r ?r)) 

The Prolog i rev is equivalent to this Lisp program: 

(defun irev (list) (irev2 list nil)) 

(defun irev2 (list so-far) 

(if (consp list) 
(irev2 (rest list) (cons (first list) so-far)) 
so-far)) 

The following table shows times in seconds to execute these routines on lists of length 
20 and 100, for both Prolog and Lisp, both interpreted and compiled. (Only compiled 
Lisp could execute rev on a 100-element list without running out of stack space.) 
Times for the zebra puzzle are also included, although there is no Lisp version of 
this program. 

Interp. Comp. Interp. Comp. 
Problem Prolog Prolog Speed-up Lisp Lisp 

zebra 278.000 17.241 16 — — 
rev 20 4.24 .208 20 .241 .0023 
rev 100 — — — — .0614 
irev 20 .22 .010 22 .028 .0005 
irev 100 9.81 .054 181 .139 .0014 

This benchmark is too small to be conclusive, but on these examples the Prolog 
compiler is 16 to 181 times faster than the Prolog interpreter, slightly faster than 
interpreted Lisp, but still 17 to 90 times slower than compiled Lisp. This suggests 
that the Prolog interpreter cannot be used as a practical programming tool, but the 
Prolog compiler can. 

Before moving on, it is interesting to note that Prolog provides for optional arguments 
automatically. Although there is no special syntax for optional arguments, an 
often-used convention is to have two versions of a relation, one with . arguments 
and one with . - 1. A single clause for the . — 1 case provides the missing, and 
therefore "optional," argument. In the following example, i rev /2 can be considered 
as a version of i rev/3 where the missing optional argument is (). 

(<- (irev ?1 ?r) (irev ?1 () ?r)) 
(<- (irev (?x . ?1) ?so-far ?r) (irev ?1 (?x ?so-far) ?r)) 
(<- (irev () ?r ?r)) 

This is roughly equivalent to the following Lisp verison: 


<a id='page-413'></a>

(defun irev (list &optional (so-far nil)) 

(if (consp list) 
(irev (rest list) (cons (first list) so-far)) 
so-far)) 

12.8 Adding More Primitives 
Just as a Lisp compiler needs machine instructions to do input/output, arithmetic, 
and the like, so our Prolog system needs to be able to perform certain primitive actions. 
For the Prolog interpreter, primitives were implemented by function symbols. When 
the interpreter went to fetch a list of clauses, if it got a function instead, it called that 
function, passing it the arguments to the current relation, the current bindings, and 
a list of unsatisfied goals. For the Prolog compiler, primitives can be installed simply 
by writing a Lisp function that respects the convention of taking a continuation as 
the final argument and has a name of the form symbol/arity. For example, here's an 
easy way to handle input and output: 

(defun read/1 (exp cont) 
(if (unify! exp (read)) 
(funcall cont))) 

(defun write/1 (exp cont) 
(write (deref-exp exp) :pretty t) 
(funcall cont)) 

Calling (write ?x) will always succeed, so the continuation will always be called. 
Similarly, one could use (read ?x) to read a value and unify it with ?x. If ?x is 
unbound, this is the same as assigning the value. However, it is also possible to make 
a call like (read (?x + ?y)), which succeeds only if the input is a three-element list 
with + in the middle. It is an easy extension to define read/2 and wr i te/2 as relations 
that indicate what stream to use. To make this useful, one would need to define 
open/2 as a relation that takes a pathname as one argument and gives a stream back 
as the other. Other optional arguments could also be supported, if desired. 

The primitive nl outputs a newline: 

(defun nl/0 (cont) (terpri) (funcall cont)) 

We provided special support for the unification predicate, =. However, we could 
have simplified the compiler greatly by having a simple definition for =/2: 


<a id='page-414'></a>

(defun =/2 (?argl ?arg2 cont) 
(if (unify! ?argl ?arg2) 
(funcall cont))) 

In fact, if we give our compiler the single clause: 

(<- (= ?x ?x)) 

it produces just this code for the definition of =/ 2. There are other equaUty predicates 
to worry about. The predicate= =/2 is more like equal in Lisp. It does no unification, 
but instead tests if two structures are equal with regard to their elements. A variable 
is considered equal only to itself. Here's an implementation: 

(defun =/2 (?argl ?arg2 cont) 
"Are the two arguments EQUAL with no unification, 
but with dereferencing? If so, succeed." 
(if (deref-equal ?argl ?arg2) 

(funcall cont))) 

(defun deref-equal (x y) 
"Are the two arguments EQUAL with no unification, 
but with dereferencing?" 
(or (eql (deref x) (deref y)) 

(and (consp x) 
(consp y) 
(deref-equal (first x) (first y)) 
(deref-equal (rest x) (rest y))))) 

One of the most important primitives is cal 1. Like funcall in Lisp, cal 1 allows us 
to build up a goal and then try to prove it. 

(defun cal 1/1 (goal cont) 
"Try to prove goal by calling it." 
(deref goal) 
(apply (make-predicate (first goal) 

(length (args goal))) 
(append (args goal) (list cont)))) 

This version of cal 1 will give a run-time error if the goal is not instantiated to a list 
whose first element is a properly defined predicate; one might want to check for that, 
and fail silently if there is no defined predicate. Here's an example of call where the 
goal is legal: 


<a id='page-415'></a>
> (?- (= ?p member) (call (?p ?x (a b c)))) 
?P = MEMBER 
?X = A; 
?P = MEMBER 
?X = B; 
?P = MEMBER 
?X = C; 
No. 

Now that we have ca 11, a lot of new things can be implemented. Here are the logical 
connectives and and or: 

(<- (or ?a ?b) (call ?a)) 

(<- (or ?a ?b) (call ?b)) 

(<- (and ?a ?b) (call ?a) (call ?b)) 

Note that these are only binary connectives, not the n-ary special forms used in Lisp. 
Also, this definition negates most of the advantage of compilation. The goals inside 
an and or or will be interpreted by cal 1, rather than being compiled. 

We can also define not, or at least the normal Prolog not, which is quite distinct 
from the logical not. In fact, in some dialects, not is written \+, which is supposed to 
be reminiscent of the logical symbol I/, that is, "can not be derived." The interpretation 
is that if goal G can not be proved, then (not G) is true. Logically, there is a difference 
between (not G) being true and being unknown, but ignoring that difference makes 
Prolog a more practical programming language. See Lloyd 1987 for more on the 
formal semantics of negation in Prolog. 

Here's an implementation of not/L Since it has to manipulate the trail, and we 

may have other predicates that will want to do the same, we'll package up what was 

done in maybe-add-undo-bindings into the macro with-undo-bindings: 

(defmacro with-undo-bindings (&body body) 
"Undo bindings after each expression in body except the last." 
(if (length=1 body) 

(first body) 

'(let ((old-trail (fill-pointer nrail*))) 
.(first body) 
.(loop for exp in (rest body) 

collect *(undo-bindings! old-trail) 
collect exp)))) 

(defun not/1 (relation cont) 
"Negation by failure: If you can't prove G. then (not G) true." 
;; Either way. undo the bindings, 
(with-undo-bindings 

(call/1 relation #'(lambda () (return-from not/1 nil))) 

(funcall cont))) 


<a id='page-416'></a>

Here's an example where not works fine: 

> (?- (member ?x (a b c)) (not (= ?x b))) 
?X = A; 
?X = C; 
No. 

Now see what happens when we simply reverse the order of the two goals: 

> (?- (not (= ?x b)) (member ?x (a b c))) 
No. 

The first example succeeds unless ?x is bound to b. In the second example, ?x is 
unbound at the start, so (= ?x b) succeeds, the not fails, and the member goal is never 
reached. So our implementation of not has a consistent procedural interpretation, 
but it is not equivalent to the declarative interpretation usually given to logical negation. 
Normally, one would expect that a and c would be valid solutions to the query, 
regardless of the order of the goals. 

One of the fundamental differences between Prolog and Lisp is that Prolog is 
relational: you can easily express individual relations. Lisp, on the other hand, is 
good at expressing collections of things as lists. So far we don't have any way of 
forming a collection of objects that satisfy a relation in Prolog. We can easily iterate 
over the objects; we just can't gather them together. The primitive bagof is one way 
of doing the collection. In general, (bagof ?x (p ?x) ?bag) unifies ?bag with a list 
of all ?x's that satisfy (. ?x). If there are no such ?x's, then the call to bagof fails. A 
bagis an unordered collection with duplicates allowed. For example, the bag {a, 6, a} 
is the same as the bag {a, a, 6}, but different from {a, 6}. Bags stands in contrast to 
sets, which are unordered collections with no duplicates. The set {a, 6} is the same 
as the set {6, a}. Here is an implementation of bagof: 

(defun bagof/3 (exp goal result cont) 
"Find all solutions to GOAL, and for each solution, 
collect the value of EXP into the list RESULT." 
;; Ex: Assume (p 1) (p 2) (p 3). Then: 
;: (bagof ?x (p ?x) ?1) => ?1= (1 2 3) 
(let ((answers nil)) 

(call/1 goal #'(lambda () 
(push (deref-copy exp) answers))) 
(if (and (not (null answers)) 
(unify! result (nreverse answers))) 
(funcall cont)))) 


<a id='page-417'></a>
(defun deref-copy (exp) 
"Copy the expression, replacing variables with new ones. 
The part without variables can be returned as is. " 
(sublis (mapcar #'(lambda (var) (cons (deref var) (?)) 

(unique-find-anywhere-if #'var-p exp)) 
exp)) 

Below we use bagof to collect a list of everyone Sandy likes. Note that the result is a 
bag, not a set: Sandy appears more than once. 

> (?- (bagof ?who (likes Sandy ?who) ?bag)) 
?WHO = SANDY 

?BAG = (LEE KIM ROBIN SANDY CATS SANDY); 

No. 

In the next example, we form the bag of every list of length three that has A and . as 
members: 

> (?- (bagof ?1 (and (length ?1 (1+ (1+ (1+ 0)))) 
(and (member a ?1) (member b ?1))) 

?bag)) 
?L = (?5 ?8 ?11 ?68 ?66) 
?BAG = ((A . ?17) (A ?21 B) (B A ?31) (?38 A B) (B ?48 A) (?52 . A)) 
No. 

Those who are disappointed with a bag containing multiple versions of the same 
answer may prefer the primitive setof, which does the same computation as bagof 
but then discards the duplicates. 

(defun setof/3 (exp goal result cont) 
"Find all unique solutions to GOAL, and for each solution, 
collect the value of EXP into the list RESULT." 
;; Ex: Assume (p 1) (p 2) (p 3). Then: 
;; (setof ?x (p ?x) ?1) = > ?1 = (1 2 3) 
(let ((answers nil)) 

(call/1 goal #'(lambda () 
(push (deref-copy exp) answers))) 
(if (and (not (null answers)) 

(unify! result (delete-duplicates 
answers 
:test #*deref-equal))) 

(funcall cont)))) 

Prolog supports arithmetic with the operator is. For example, (Is ?x (+ ?y 1)) 
unifies ?x with the value of ?y plus one. This expression fails if ?y is unbound, and it 


<a id='page-418'></a>

gives a run-time error if ?y is not a number. For our version of Prolog, we can support 
not just arithmetic but any Lisp expression: 

(defun is/2 (var exp cont) 
Example: (is ?x (+ 3 (* ?y (+ ?z 4)))) 
Or even: (is (?x ?y ?x) (cons (first ?z) ?1)) 

(if (and (not (find-if-anywhere #*unbound-var-p exp)) 
(unify 1 var (eval (deref-exp exp)))) 
(funcall cont))) 

(defun unbound-var-p (exp) 
"Is EXP an unbound var?" 
(and (var-p exp) (not (bound-p exp)))) 

As an aside, we might as well give the Prolog programmer access to the function 
unbound -var-p. The standard name for this predicate is va r/1: 

(defun var/1 (?argl cont) 
"Succeeds if ?argl is an uninstantiated variable." 
(if (unbound-var-p ?argl) 

(funcall cont))) 

The is primitive fails if any part of the second argument is unbound. However, there 
are expressions with variables that can be solved, although not with a direct call to 
eval. For example, the following goal could be solved by binding ?x to 2: 

(solve (= 12 (* (+ ?x 1) 4))) 

We might want to have more direct access to Lisp from Prolog. The problem with 
is is that it requires a check for unbound variables, and it calls eval to evaluate 
arguments recursively. In some cases, we just want to get at Lisp's apply, without 
going through the safety net provided by i s. The primitive lisp does that. Needless 
to say,1 i sp is not a part of standard Prolog. 

(defun lisp/2 (?result exp cont) 
"Apply (first exp) to (rest exp), and return the result." 
(if (and (consp (deref exp)) 

(unify! ?result (apply (first exp) (rest exp)))) 
(funcall cont))) 

&#9635; Exercise 12.7 [m] Define the primitive solve/1, which works like the function 
sol ve used in student ([page 225](chapter7.md#page-225)). Decide if it should take a single equation as 
argument or a list of equations. 


<a id='page-419'></a>
&#9635; Exercise 12.8 [h] Assumewehadagoalof the form (solve (= 12 (* (+ ?x 1) 
4))). Rather than manipulate the equation when sol ve/1 is called at run time, we 
might prefer to do part of the work at compile time, treating the call as if it were 
(solve (= ?x 2)). Write a Prolog compiler macro for sol ve. Notice that even when 
you have defined a compiler macro, you still need the underlying primitive, because 
the predicate might be invoked through a cal 1 / I. The same thing happens in Lisp: 
even when you supply a compiler macro, you still need the actual function, in case 
of a funcall or apply. 

&#9635; Exercise 12.9 [h] Which of the predicates call, and, or, not, or repeat could 
benefit from compiler macros? Write compiler macros for those predicates that 
could use one. 

&#9635; Exercise 12.10 [m] You might have noticed that ca 11 /1 is inefficient in two important 
ways. First, it calls make-predi cate, which must build a symbol by appending 
strings and then look the string up in the Lisp symbol table. Alter make-predi cate 
to store the predicate symbol the first time it is created, so it can do a faster lookup 
on subsequent calls. The second inefficiency is the call to append. Change the whole 
compiler so that the continuation argument comes first, not last, thus eliminating 
the need for append in cal 1. 

&#9635; Exercise 12.11 [s] The primitive true/0 always succeeds, and f a i 1 /O always fails. 
Define these primitives. Hint: the first corresponds to a Common Lisp function, and 
the second is a function already defined in this chapter. 

&#9635; Exercise 12.12 [s] Would it be possible to write = =/ 2 as a list of clauses rather than 
as a primitive? 

&#9635; Exercise 12.13 [m] Write a version of deref - copy that traverses the argument expression 
only once. 


<a id='page-420'></a>

12.9 The Cut 
In Lisp, it is possible to write programs that backtrack explicitly, although it can 
be awkward when there are more than one or two backtrack points. In Prolog, 
backtracking is automatic and implicit, but we don't yet know of any way to avoid 
backtracking. There are two reasons why a Prolog programmer might want to disable 
backtracking. First, keeping track of the backtrack points takes up time and space. 
A programmer who knows that a certain problem has only one solution should be 
able to speed up the computation by telling the program not to consider the other 
possible branches. Second, sometimes a simple logical specification of a problem 
will yield redundant solutions, or even some unintended solutions. It may be that 
simply pruning the search space to eliminate some backtracking will yield only 
the desired answers, while restructuring the program to give all and only the right 
answers would be more difficult. Here's an example. Suppose we wanted to define 
a predicate, max/3, which holds when the third argument is the maximum of the 
first two arguments, where the first two arguments will always be instantiated to 
numbers. The straightforward definition is: 

(<- (max ?x ?y ?x) (>= ?x ?y)) 
(<- (max ?x ?y ?y) ??x ?y)) 

Declaratively, this is correct, but procedurally it is a waste of time to compute the < 
relation if the >= has succeeded: in that case the < can never succeed. The cut symbol, 
written !, can be used to stop the wasteful computation. We could write: 

(<- (max ?x ?y ?x) (>= ?x ?y) !) 
(<- (max ?x ?y ?y)) 

The cut in the first clause says that if the first clause succeeds, then no other clauses 
will be considered. So now the second clause can not be interpreted on its own. 
Rather, it is interpreted as "if the first clause fails, then the max of two numbers is the 
second one." 

In general, a cut can occur anywhere in the body of a clause, not just at the end. 
There is no good declarative interpretation of a cut, but the procedural interpretation 
is two-fold. First, when a cut is "executed" as a goal, it always succeeds. But in 
addition to succeeding, it sets up a fence that cannot be crossed by subsequent 
backtracking. The cut serves to cut off backtracking both from goals to the right of 
the cut (in the same clause) and from clauses below the cut (in the same predicate). 
Let's look at a more abstract example: 

(<- (p) (q) (r) ! (s) (t)) 
(<- (p) (s)) 


<a id='page-421'></a>

In processing the first clause of p, backtracking can occur freely while attempting 
to solve q and r. Once r is solved, the cut is encountered. From that point on, 
backtracking can occur freely while solving s and t, but Prolog will never backtrack 
past the cut into r, nor will the second clause be considered. On the other hand, if 
q or . failed (before the cut is encountered), then Prolog would go on to the second 
clause. 

Now that the intent of the cut is clear, let's think of how it should be implemented. 
We'll look at a slightly more complex predicate, one with variables and multiple cuts: 

(<- (p ?x a) ! (q ?x)) 

(<- (p ?x b) (r ?x) ! (s ?x)) 

We have to arrange it so that as soon as we backtrack into a cut, no more goals 
are considered. In the first clause, when q/1 fails, we want to return from p/2 
immediately, rather than considering the second clause. Similarly, the first time s /1 
fails, we want to return from p/2, rather than going on to consider other solutions to 
r/1. Thus, we want code that looks something like this: 

(defun p/2 (argl arg2 cont) 
(let ((old-trail (fil 1-pointer nrail*))) 
(if (unify! arg2 'a) 
(progn (q/1 argl cont) 

(return-from p/2 nil))) 
(undo-bindings! old-trail) 
(if (unify! arg2 'b) 

(r/1 argl #'(lambda () 
(progn (s/1 argl cont) 
(return-from p/2 nil))))))) 

We can get this code by making a single change to compi 1 e - body: when the first goal 

in a body (or what remains of the body) is the cut symbol, then we should generate a 

progn that contains the code for the rest of the body, followed by a return-from the 

predicate being compiled. Unfortunately, the name of the predicate is not available 

to compi 1 e-body. We could change compile-clause and compi 1 e-body to take the 

predicate name as an extra argument, or we could bind the predicate as a special 

variable in compi 1 e-predi cate. I choose the latter: 

(defvar ^predicate* nil 
"The Prolog predicate currently being compiled") 


<a id='page-422'></a>

(defun compile-predicate (symbol arity clauses) 
"Compile all the clauses for a given symbol/arity 
into a single LISP function." 
(let ((^predicate* (make-predicate symbol arity)) 

(parameters (make-parameters arity))) 
(compile 
(eval 
'(defun ,*predicate* (.parameters cont) 
(maybe-add-undo-bindings 
(mapcar #*(lambda (clause) 
(compile-clause parameters 
clause *cont)) 
clauses))))))) 

(defun compile-body (body cont bindings) 
"Compile the body of a clause." 
(cond 

((null body) 
'(funcall .cont)) 
((eq (first body) .) 
'(progn .(compile-body (rest body) cont bindings) 
(return-from .*predicate* nil))) 

(t (let* ((goal (first body)) 
(macro (prolog-compiler-macro (predicate goal))) 
(macro-val (if macro 

(funcall macro goal (rest body) 
contbindings)))) 

(if (and macro (not (eq macro-val .-pass))) 
macro-val 
'(.(make-predicate (predicate goal) 

(relation-arity goal)) 
.(mapcar #'(lambda (arg) 
(compile-arg arg bindings)) 
(args goal)) 

.(if (null (rest body)) 
cont 
'#*(lambda () 

.(compile-body 
(rest body) cont 
(bind-new-variables bindings goal)))))))))) 

&#9635; Exercise 12.14 [m] Given the definitions below, figure out what a call to test - cut 
will do, and what it will write: 

(<- (test-cut) (p a) (p b) ! (p c) (p d)) 
(<- (test-cut) (p e)) 


<a id='page-423'></a>
(<- (p ?x) (write (?x 1))) 
(<- (p ?x) (write (?x 2))) 

Another way to use the cut is in a repeat/fail loop. The predicate repeat is defined 
with the following two clauses: 

(<- (repeat)) 
(<- (repeat) (repeat)) 

An alternate definition as a primitive is: 

(defun repeat/0 (cont) 
(loop (funcall cont))) 

Unfortunately, repeat is one of the most abused predicates. Several Prolog books 
present programs like this: 

(<- (main) 
(write "Hello.") 
(repeat) 
(write "Command: ") 
(read ?command) 
(process ?command) 
(= ?command exit) 
(write "Good bye.")) 

The intent is that commands are read one at a time, and then processed. For each 
command except exit, process takes the appropriate action and then fails. This 
causes a backtrack to the repeat goal, and a new command is read and processed. 
When the command is ex i t, the procedure returns. 

There are two reasons why this is a poor program. First, it violates the principle of 
referential transparency. Things that look alike are supposed to be alike, regardless 
of the context in which they are used. But here there is no way to tell that four of the six 
goals in the body comprise a loop, and the other goals are outside the loop. Second, 
it violates the principle of abstraction. A predicate should be understandable as a 
separate unit. But here the predicate process can only be understood by considering 
the context in which it is called: a context that requires it to fail after processing each 
command. As Richard O'Keefe 1990 points out, the correct way to write this clause 
is as follows: 


<a id='page-424'></a>

(<- (main) 
(write "Hello.") 
(repeat) 

(write "Command: ") 
(read ?command) 
(process ?command) 
(or (= ?command exit) (fail)) 

(write "Good bye.")) 

The indentation clearly indicates the limits of the repeat loop. The loop is terminated 
by an explicit test and is followed by a cut, so that a calling program won't accidently 
backtrack into the loop after it has exited. Personally, I prefer a language like Lisp, 
where the parentheses make constructs like loops explicit and indentation can be 
done automatically. But O'Keefe shows that well-structured readable programs can 
be written in Prolog. 

The if-then and if-then-else constructions can easily be written as clauses. Note 
that the if-then-else uses a cut to commit to the then part if the test is satisfied. 

(<- (if ?test ?then) (if ?then ?else (fail))) 

(<- (if ?test ?then ?else) 
(call ?test) 

(call ?then)) 

(<- (if ?test ?then ?else) 
(call ?else)) 

The cut can be used to implement the nonlogical not. The following two clauses are 
often given before as the definition of not. Our compiler succesfuUy turns these two 
clauses into exactly the same code as was given before for the primitive not/1: 

(<- (not ?p) (call ?p) I (fail)) 
(<- (not ?p)) 

12.10 '^ear Prolog 
The Prolog-In-Lisp system developed in this chapter uses Lisp syntax because it is 
intended to be embedded in a Lisp system. Other Prolog implementations using 
Lisp syntax include micro-Prolog, Symbolics Prolog, and LMI Prolog. 


<a id='page-425'></a>
However, the majority of Prolog systems use a syntax closer to traditional mathematical 
notation. The following table compares the syntax of "standard" Prolog to 
the syntax of Prolog-In-Lisp. While there is currently an international committee 
working on standardizing Prolog, the final report has not yet been released, so different 
dialects may have slightly different syntax. However, most implementations 
follow the notation summarized here. They derive from the Prolog developed at the 
University of Edinburgh for the DEC-10 by David H. D. Warren and his colleagues. 
The names for the primitives in the last section are also taken from Edinburgh Prolog. 

Prolog Prolog-In-Lisp 
atom lower const 
variable Upper ?var 
anonymous -? 
goal p(Var,const) (p ?var const) 
rule p(X) q(X). (<- (p ?x) (q ?x)) 
fact p(a). (<- (p a)) 
query ?- p(X). (?- (p ?x)) 
list [a.b.c] (a b c) 
cons [a 1 Rest] (a . ?rest) 
nil [] () 
and p(X), q(X) (and (p ?x) (q ?x)) 
or p(X): q(X) (or (p ?x) (q ?x)) 
not \+ p(X) (not (p ?x)) 

We have adopted Lisp's bias toward lists; terms are built out of atoms, variables, 
and conses of other terms. In real Prolog cons cells are provided, but terms are 
usually built out of structures, not lists. The Prolog term p(a,b) corresponds to the 
Lisp vector #( p/2 a b), not the list (. a b). A minority of Prolog implementations 
use structure sharing. In this approach, every non-atomic term is represented by 
a skeleton that contains place holders for variables and a header that points to the 
skeleton and also contains the variables that will fill the place holders. With structure 
sharing, making a copy is easy: just copy the header, regardless of the size of the 
skeleton. However, manipulating terms is complicated by the need to keep track of 
both skeleton and header. See Boyer and Moore 1972 for more on structure sharing. 

Another major difference is that real Prolog uses the equivalent of failure contin


uations, not success continuations. No actual continuation, in the sense of a closure, 

is built. Instead, when a choice is made, the address of the code for the next choice 

is pushed on a stack. Upon failure, the next choice is popped off the stack. This is 

reminiscent of the backtracking approach using Scheme's cal 1 /cc facility outlined 

on [page 772](chapter22.md#page-772). 


<a id='page-426'></a>

&#9635; Exercise 12.15 [m] Assuming an approach using a stack of failure continuations 
instead of success continuations, show what the code for . and member would look 
like. Note that you need not pass failure continuations around; you can just push 
them onto a stack that top-1 evel - prove will invoke. How would the cut be implemented? 
Did we make the right choice in implementing our compiler with success 
continuations, or would failure continuations have been better? 

12.11 History and References 
As described in chapter 11, the idea of logic programming was fairly well understood 
by the mid-1970s. But because the implementations of that time were slow, logic 
programming did not catch on. It was the Prolog compiler for the DEC-10 that made 
logic programming a serious alternative to Lisp and other general-purpose languages. 
The compiler was developed in 1977 by David H. D. Warren with Fernando Pereira 
and Luis Pereira. See the paper by Warren (1979) and by all three (1977). 

Unfortunately, David H. D. Warren's pioneering work on compiling Prolog has 
never been published in a widely accessible form. His main contribution was the 
description of the Warren Abstract Machine (WAM), an instruction set for compiled 
Prolog. Most existing compilers use this instruction set, or a slight modification 
of it. This can be done either through byte-code interpretation or through macro-
expansion to native machine instructions. Ait-Kaci 1991 provides a good tutorial on 
the WAM, much less terse than the original (Warren 1983). The compiler presented in 
this chapter does not use the WAM. Instead, it is modeled after Mark Stickel's (1988) 
theorem prover. A similar compiler is briefly sketched by Jacques Cohen 1985. 

12.12 Exercises 
&#9635; Exercise 12.16 [m] Change the Prolog compiler to allow implicit calls. That is, if 
a goal is not a cons cell headed by a predicate, compile it as if it were a cal 1. The 
clause: 

(<- (p ?x ?y) (?x c) ?y) 

should be compiled as if it were: 

(<- (p ?x ?y) (call (?x c)) (call ?y)) 


<a id='page-427'></a>
&#9635; Exercise 12.17 [h] Here are some standard Prolog primitives: 

* get/1 Read a single character and unify it with the argument. 
* put/1 Print a single character. 
* nonvar/1, /=, /==Theoppositesof var, = and==, respectively. 
* i nteger/1 True if the argument is an integer. 
* atom/1 True if the argument is a symbol (like Lisp's symbol p). 
* atomi c/1 True if the argument is a number or symbol (like Lisp's atom). 
.<,>,=<,> = Arithmetic comparison; succeeds when the arguments are both 
instantiated to numbers and the comparison is true. 

* listing/0 Print out the clauses for all defined predicates. 
* 1 i sti ng/1 Print out the clauses for the argument predicate. 
Implement these predicates. In each case, decide if the predicate should be 
implemented as a primitive or a list of clauses, and if it should have a compiler 
macro. 

There are some naming conflicts that need to be resolved. Terms like atom have 
one meaning in Prolog and another in Lisp. Also, in Prolog the normal notation is \ = 
and \==, not / = and /==. For Prolog-In-Lisp, you need to decide which notations to 
use: Prolog's or Lisp's. 

&#9635; Exercise 12.18 [s] In Lisp, we are used to writing n-ary calls like (< 1 . 10)or(= 
. y .). Write compiler macros that expand n-ary calls into a series of binary calls. 
Forexample, (< 1 . 10) should expand into (and (< 1 n) (< . 10)). 

&#9635; Exercise 12.19 [m] One feature of Lisp that is absent in Prolog is the quote mechanism. 
Is there a use for quote? If so, implement it; if not, explain why it is not 
needed. 

&#9635; Exercise 12.20 [h] Write a tracing mechanism for Prolog. Add procedures . -1 ra ce 
and p-untrace to trace and untrace Prolog predicates. Add code to the compiler to 
generate calls to a printing procedure for goals that are traced. In Lisp, we have to 
trace procedures when they are called and when they return. In Prolog, there are 
four cases to consider: the call, successful completion, backtrack into subsequent 
clauses, and failure with no more clauses. We will call these four cases cal 1, exi t. 


<a id='page-428'></a>

redo, and f ai 1 , respectively. If we traced member, we would expect tracing output to 
look something like this: 

> (? - (member ?x (a b c d)) (fail)) 
CALL MEMBER: ?1 (A . C D) 
EXIT MEMBER: A (A . C D) 
REDO MEMBER: ?1 (A . C D) 

CALL MEMBER: 11 (B C D) 

EXIT MEMBER: . (B C D) 

REDO MEMBER: ?1 (B C D) 

CALL MEMBER: 11 (C D) 
EXIT MEMBER: C (C D) 
REDO MEMBER: ?1 (C D) 

CALL MEMBER: 11 (D) 
EXIT MEMBER: D (D) 
REDO MEMBER: ?1 (D) 

CALL MEMBER: 11 NIL 
REDO MEMBER: 11 NIL 
FAIL MEMBER: 11 NIL 

FAIL MEMBER: 11 (D) 
FAIL MEMBER: 11 (C D) 
FAIL MEMBER: 11 (B C D) 
FAIL MEMBER: ?1 (A . C D) 
No. 

&#9635; Exercise 12.21 [m] Some Lisp systems are very slow at compiling functions. KCL 
is an example; it compiles by translating to C and then calling the C compiler and 
assembler. In KCL it is best to compile only code that is completely debugged, and 
run interpreted while developing a program. 
Alter the Prolog compiler so that calling the Lisp compiler is optional. In all cases, 
Prolog functions are translated into Lisp, but they are only compiled to machine 
language when a variable is set. 

&#9635; Exercise 12.22 [d] Some Prolog systems provide the predicate freeze to "freeze" a 
goal until its variables are instantiated. For example, the goal (freeze . (> . 0)) 
is interpreted as follows: if x is instantiated, then just evaluate the goal (> . 0), and 
succeed or fail depending on the result. However, if . is unbound, then succeed and 
continue the computation, but remember the goal (> . 0) and evaluate it as soon as 
X becomes instantiated. Implement freeze. 

&#9635; Exercise 12.23 [m] Write a recursive version of anonymous - va r i abl es -1. that does 
not use a local function. 


<a id='page-429'></a>

12.13 Answers 
Answer 12.6 Here's a version that works for Texas Instruments and Lucid implementations: 


(defmacro with-compilation-unit (options &body body) 
"Do the body, but delay compiler warnings until the end." 
This is defined in Common Lisp the Language, 2nd ed. 

*(.(read-time-case 
#+TI * compi1 er:compi1er-warni ngs-context-bi nd 
#+Lucid *with-deferred-warnings 

'progn) 
..body)) 

(defun prolog-compile-symbols (&optional (symbols *uncompiled*)) 
"Compile a list of Prolog symbols. 
By default, the list is all symbols that need it." 
(with-compilation-unit () 

(mapc #*prolog-compile symbols) 

(setf *uncompiled* (set-difference *uncompiled* symbols)))) 

Answer 12.9 Macros for and and or are very important, since these are commonly 
used. The macro for and is trivial: 

(def-prolog-compiler-macro and (goal body cont bindings) 
(compile-body (append (args goal) body) cont bindings)) 

The macro for or is trickier: 

(def-prolog-compiler-macro or (goal body cont bindings) 
(let ((disjuncts (args goal))) 

(case (length disjuncts) 
(0 fail) 
(1 (compile-body (cons (first disjuncts) body) cont bindings)) 
(t (let ((fn (gensym "F"))) 

'(flet ((,fn () ,(compile-body body cont bindings))) 
.,(maybe-add-undo-bindings 
(loop for g in disjuncts collect 
(compile-body (list g) *#',fn 
bindings))))))))) 


<a id='page-430'></a>

Answer 12.11 true /0 is funcall: when a goal succeeds, we call the continuation, 
fai 1 /O is i gnore: when a goal fails, we ignore the continuation. We could also define 
compiler macros for these primitives: 

(def-prolog-compi1er-macro true (goal body cont bindings) 
(compile-body body cont bindings)) 

(def-prolog-compiler-macro fail (goal body cont bindings) 
(declare (ignore goal body cont bindings)) 
nil) 

Answer 12.13 

(defun deref-copy (exp) 
"Build a copy of the expression, which may have variables. 
The part without variables can be returned as is. " 
(let ((var-alist nil)) 

(labels 

((walk (exp) 
(deref exp) 
(cond ((consp exp) 

(reuse-cons (walk (first exp)) 
(walk (rest exp)) 
exp)) 

((var-p exp) 
(let ((entry (assoc exp var-alist))) 

(if (not (null entry)) 
(cdr entry) 
(let ((var-copy (?))) 

(push (cons exp var-copy) var-alist) 
var-copy)))) 
(t exp)))) 
(walk exp)))) 


<a id='page-431'></a>
Answer 12.14 In the first clause of test - cut, all four calls to . will succeed via the 
first clause of p. Then backtracking will occur over the calls to (. c) and (. d). All 
four combinations of 1 and 2 succeed. After that, backtracking would normally go 
back to the call to (p b). But the cut prevents this, and the whole (test-cut) goal 
fails, without ever considering the second clause. Here's the actual output: 

(?- (test-cut)) 

(A 1)(B 1)(C 1)(D 1) 

Yes; 

(D 2) 

Yes; 

(C 2)(D 1) 

Yes; 

(D 2) 

Yes; 

No. 

Answer 12.17 Forexample: 

(defun >/2 (x y cont) 
(if (and (numberp (deref x)) (numberp (deref y)) (> . y)) 
(funcall cont))) 

(defun numberp/1 (x cont) 
(if (numberp (deref x)) 
(funcall cont))) 

Answer 12.19 Lisp uses quote in two ways: to distinguish a symbol from the value 
of the variable represented by that symbol, and to distinguish a literal list from the 
value that would be returned by evaluating a function call. The first distinction Prolog 
makes by a lexical convention: variables begin with a question mark in our Prolog, 
and they are capitalized in real Prolog. The second distinction is not necessary 
because Prolog is relational rather than functional. An expression is a goal if it is a 
member of the body of a clause, and is a literal if it is an argument to a goal. 


<a id='page-432'></a>

Answer 12.20 Hint: Here's how member could be augmented with calls to a procedure, 
pro! og-trace, which will print information about the four kinds of tracing 
events: 

(defun member/2 (?argl ?arg2 cont) 
(let ((old-trail (fill-pointer nrail*)) 

(exit-cont #*(lambda () 
(prolog-trace 'exit 'member ?argl ?arg2 ) 
(funcall cont)))) 

(prolog-trace 'call 'member ?argl ?arg2) 
(if (unify! ?arg2 (cons ?argl (?))) 

(funcall exit-cont)) 
(undo-bindings! old-trail) 
(prolog-trace 'redo 'member ?argl ?arg2) 
(let ((?rest (?))) 

(if (unify! ?arg2 (cons (?) ?rest)) 
(member/2 ?argl ?rest exit-cont))) 
(prolog-trace 'fail 'member ?argl ?arg2))) 

The definition of prol og-trace is: 

(defvar *prolog-trace-indent* 0) 

(defun prolog-trace (kind predicate &rest args) 
(if (member kind '(call redo)) 
(incf *prolog-trace-indent* 3)) 
(format t "~rvra ~a:~{ ~a~}" 
*prolog-trace-indent* kind predicate args) 
(if (member kind '(fail exit)) 
(decf *prolog-trace-indent* 3))) 


<a id='page-433'></a>
Answer 12.23 

(defun anonymous-variables-in (tree) 
"Return a list of all variables that occur only once in tree." 
(values (anon-vars-in tree nil nil))) 

(defun anon-vars-in (tree seen-once seen-more) 
"Walk the data structure TREE, returning a list of variables 
seen once, and a list of variables seen more than once." 
(cond 
((consp tree) 
(multiple-value-bind (new-seen-once new-seen-more) 
(anon-vars-in (first tree) seen-once seen-more) 

(anon-vars-in (rest tree) new-seen-once new-seen-more))) 
((not (variable-p tree)) (values seen-once seen-more)) 
((member tree seen-once) 

(values (delete tree seen-once) (cons tree seen-more))) 
((member tree seen-more) 
(values seen-once seen-more)) 
(t (values (cons tree seen-once) seen-more)))) 


## Chapter 13
<a id='page-434'></a>

Object-Oriented 
Programming 

r I 1 he programs in this book cover a wide range of problems. It is only natural that a 

I wide range of programming styles have been introduced to attack these problems. One 

JL style not yet covered that has gained popularity in recent years is called object-oriented 
programming. To understand what object-oriented programming entails, we need to place it in 
the context of other styles. 

Historically, the first computer programs were written in an imperative programming style. A 
program was construed as a series of instructions, where each instruction performs some action: 
changing the value of a memory location, printing a result, and so forth. Assembly language is 
an example of an imperative language. 

As experience (and ambition) grew, programmers looked for ways of controlling the complexity 
of programs. The invention of subroutines marked the algorithmic or procedural programming 
style, a subclass of the imperative style. Subroutines are helpful for two reasons: breaking 
up the problem into small pieces makes each piece easier to understand, and it also makes it 
possible to reuse pieces. Examples of procedural languages are FORTRAN, C, Pascal, and Lisp 
with setf. 


<a id='page-435'></a>

Subroutines are still dependent on global state, so they are not completely separate 
pieces. The use of a large number of global variables has been criticized as a 
factor that makes it difficult to develop and maintain large programs. To eliminate 
this problem, the functional programming style insists that functions access only the 
parameters that are passed to them, and always return the same result for the same 
inputs. Functional programs have the advantage of being mathematically clean—it 
is easy to prove properties about them. However, some applications are more naturally 
seen as taking action rather than calculating functional values, and are therefore 
unnatural to program in a functional style. Examples of functional languages are FP 
and Lisp without setf. 

In contrast to imperative languages are declarative languages, which attempt to 
express "what to do" rather than "how to do it." One type of declarative programming 
is rule-based programming, where a set of rules states how to transform a problem 
into a solution. Examples of rule-based systems are ELIZA and STUDENT. 

An important kind of declarative programming is logic programming, where axioms 
are used to describe constraints, and computation is done by a constructive proof of 
a goal. An example of logic language is Prolog. 

Object-oriented programming is another way to tame the problem of global state. 
Instead of prohibiting global state (as functional programming does), object-oriented 
programming breaks up the unruly mass of global state and encapsulates it into small, 
manageable pieces, or objects. This chapter covers the object-oriented approach. 

13,1 Object-Oriented Programming 

Object-oriented programming turns the world of computing on its side: instead 
of viewing a program primarily as a set of actions which manipulate objects, it is 
viewed as a set of objects that are manipulated by actions. The state of each object 
and the actions that manipulate that state are defined once and for all when the 
object is created. This can lead to modular, robust systems that are easy to use and 
extend. It also can make systems correspond more closely to the "real world," which 
we humans perceive more easily as being made up of objects rather than actions. 
Examples of object-oriented languages are Simula, C++, and CLOS, the Common 
Lisp Object System. This chapter will first introduce object-oriented programming 
in general, and then concentrate on the Common Lisp Object System. 

Many people are promoting object-oriented programming as the solution to the 
software development problem, but it is hard to get people to agree on just what 
object-orientation means. Peter Wegner 1987 proposes the following formula as a 
definition: 

Object-orientation = Objects + Classes + Inheritance 


<a id='page-436'></a>

Briefly, objects are modules that encapsulate some data and operations on that data. 
The idea of information /z/dm^—insulating the representation of that data from operations 
outside of the object—is an important part of this concept. Classes are groups 
of similar objects with identical behavior. Objects are said to be instances of classes. 
Inheritance is a means of defining new classes as variants of existing classes. The new 
class inherits the behavior of the parent class, and the programmer need only specify 
how the new class is different. 

The object-oriented style brings with it a new vocabulary, which is summarized in 
the following glossary. Each term will be explained in more detail when it comes up. 

class: A group of similar objects with identical behavior. 
class variable: A variable shared by all members of a class. 
delegation: Passing a message from an object to one of its components. 
generic function: A function that accepts different types or classes of 

arguments. 
inheritance: A means of defining new classes as variants of existing 

classes. 
instance: An instance of a class is an object. 
instance variable: A variable encapsulated within an object. 
message: A name for an action. Equivalent to generic function. 
method: A means of handling a message for a particular class. 
multimethod: A method that depends on more than one argument. 
multiple inheritance: Inheritance from more than one parent class. 
object: An encapsulation of local state and behavior. 

13.2 Objects 
Object-oriented programming, by definition, is concerned with objects. Any datum 
that can be stored in computer memory can be thought of as an object. Thus, the 
number 3, the atom x, and the string "hel 1 o" are all objects. Usually, however, the 
term object is used to denote a more complex object, as we shall see. 

Of course, all programming is concerned with objects, and with procedures 
operating on those objects. Writing a program to solve a particular problem will 
necessarily involve writing definitions for both objects and procedures. What distinguishes 
object-oriented programming is that the primary way of decomposing the 
problem into modules is based on the objects rather than on the procedures. The 
difference can best be seen with an example. Here is a simple program to create bank 
accounts and keep track of withdrawals, deposits, and accumulation of interest. 
First, the program is written in traditional procedural style: 

(defstruct account 
(name "") (balance 0.00) (interest-rate .06)) 


<a id='page-437'></a>

(defun account-withdraw (account amt) 
"Make a withdrawal from this account." 
(if (<= amt (account-balance account)) 

(decf (account-balance account) amt) 

'insufficient-funds)) 

(defun account-deposit (account amt) 
"Make a deposit to this account." 
(incf (account-balance account) amt)) 

(defun account-interest (account) 
"Accumulate interest in this account." 
(incf (account-balance account) 

(* (account-interest-rate account) 
(account-balance account)))) 

We can create new bank accounts with make-a ccount and modify them with 
account-wi thdraw, account-deposi t, and account-i nterest. This is a simple problem, 
and this simple solution suffices. Problems appear when we change the specification 
of the problem, or when we envision ways that this implementation could 
be inadvertently used in error. For example, suppose a programmer looks at the 
account structure and decides to use (decf (account-balance account)) directly 
instead of going through the account-wi thdraw function. This could lead to negative 
account balances, which were not intended. Or suppose that we want to create a 
new kind of account, where only a certain maximum amount can be withdrawn at 
one time. There would be no way to ensure that account-withdraw would not be 
applied to this new, limited account. 

The problem is that once we have created an account, we have no control over 

what actions are applied to it. The object-oriented style is designed to provide that 

control. Here is the same program written in object-oriented style (using plain Lisp): 

(defun new-account (name &optional (balance 0.00) 

(interest-rate .06)) 
"Create a new account that knows the following messages:" 
#'(lambda (message) 

(case message 
(withdraw #*(lambda (amt) 
(if (<= amt balance) 
(decf balance amt) 

'insufficient-funds))) 
(deposit #'(lambda (amt) (incf balance amt))) 
(balance #'(lambda () balance)) 
(name #'(lambda () name)) 
(interest #*(lambda () 
(incf balance 
(* interest-rate balance))))))) 


<a id='page-438'></a>

The function new-account creates account objects, which are implemented as closures 
that encapsulate three variables: the name, balance, and interest rate of the 
account. An account object also encapsulates functions to handle the five messages 
to which the object can respond. An account object can do only one thing: receivea 
message and return the appropriate function to execute that message. For example, 
if you pass the message wi thdraw to an account object, it will return a function that, 
when applied to a single argument (the amount to withdraw), will perform the withdrawal 
action. This function is called the method that implements the message. The 
advantage of this approach is that account objects are completely encapsulated; the 
information corresponding to the name, balance, and interest rate is only accessible 
through the five messages. We have a guarantee that no other code can manipulate 
the information in the account in any other way.^ 

The function get - method finds the method that implements a message for a given 
object. The function send gets the method and applies it to a list of arguments. The 
name send comes from the Flavors object-oriented system, which is discussed in the 
history section ([page 456](chapter13.md#page-456)). 

(defun get-method (object message) 
"Return the method that implements message for this object." 
(funcall object message)) 

(defun send (object message &rest args) 
"Get the function to implement the message, 
and apply the function to the args." 
(apply (get-method object message) args)) 

Here is an example of the use of new- account and send: 

> (setf acct (new-account "J. Random Customer" 1000.00)) => 
#<CLOSURE 23652465> 

> (send acct 'withdraw 500.00) 500.0 

> (send acct 'deposit 123.45) => 623.45 

> (send acct 'name) ^ "J. Random Customer" 

> (send acct 'balance) => 623.45 

^More accurately, we have a guarantee that there is no way to get at the inside of a closure 
using portable Common Lisp code. Particular implementations may provide debugging tools 
for getting at this hidden information, such as i .spect. So closures are not perfect at hiding 
information from these tools. Of course, no information-hiding method will be guaranteed 
against such covert channels—even with the most sophisticated software security measures, 
it is always possible to, say, wipe a magnet over the computer's disks and alter sensitive data. 


<a id='page-439'></a>
13.3 Generic Functions 
The send syntax is awkward, as it is different from the normal Lisp function-calling 
syntax, and it doesn't fit in with the other Lisp tools. For example, we might like to 
say (ma pea . ' ba 1 anee accounts), but with messages we would have to write that as: 

(mapcar #*(lambda (acct) (send acct 'balance)) accounts) 

We can fix this problem by deiining generic functions that find the right method to 
execute a message. For example, we could define: 

(defun withdraw (object &rest args) 
"Define withdraw as a generic function on objects." 
(apply (get-method object 'withdraw) args)) 

and then write (withdraw acct .) instead of (send acct 'withdraw x). The 
function wi thdraw is generic because it not only works on account objects but also 
works on any other class of object that handles the wi thdraw message. For example, 
we might have a totally unrelated class, army, which also implements a withdraw 
method. Then we could say (send 5th-army 'withdraw) or (withdraw 5th-army) 
and have the correct method executed. So object-oriented programming eliminates 
many problems with name clashes that arise in conventional programs. 

Many of the built-in Common Lisp functions can be considered generic functions, 
in that they operate on different types of data. For example, sqrt does one thing 
when passed an integer and quite another when passed an imaginary number. The 
sequence functions (like findordelete) operate on lists, vectors, or strings. These 
functions are not implemented like wi thd raw, but they still act like generic functions.^ 

13.4 Classes 
It is possible to write macros to make the object-oriented style easier to read and 
write. The macro def i ne - cl ass defines a class with its associated message-handling 
methods. It also defines a generic function for each message. Finally, it allows the 
programmer to make a distinction between variables that are associated with each 
object and those that are associated with a class and are shared by all members of the 
class. For example, you might want to have all instances of the class account share 
the same interest rate, but you wouldn't want them to share the same balance. 

^There is a technical sense of "generic function" that is used within CLOS. These functions 
are not generic according to this technical sense. 


<a id='page-440'></a>

(defmacro define-class (class inst-vars class-vars &body methods) 
"Define a class for object-oriented programming." 
Define constructor and generic functions for methods 

'(let ,class-vars 
(mapcar #'ensure-generic-fn \(mapcar #'first methods)) 
(defun .class ,inst-vars 

#*(lambda (message) 
(case message 
,(mapcar #'make-clause methods)))))) 

(defun make-clause (clause) 
"Translate a message from define-class into a case clause." 
'(.(first clause) #'(lambda .(second clause) ..(rest2 clause)))) 

(defun ensure-generic-fn (message) 
"Define an object-oriented dispatch function for a message, 
unless it has already been defined as one." 
(unless (generic-fn-p message) 

(let ((fn #'(lambda (object &rest args) 

(apply (get-method object message) args)))) 
(setf (symbol-function message) fn) 
(setf (get message *generic-fn) fn)))) 

(defun generic-fn-p (fn-name) 
"Is this a generic function?" 
(and (fboundp fn-name) 

(eq (get fn-name 'generic-fn) (symbol-function fn-name)))) 

Now we define the class account with this macro. We make i nterest- rate a class 
variable, one that is shared by all accounts: 

(define-class account (name &optional (balance 0.00)) 
((interest-rate .06)) 

(withdraw (amt) (if (<= amt balance) 
(decf balance amt) 
'insufficient-funds)) 

(deposit (amt) (incf balance amt)) 
(balance () balance) 
(name () name) 

(interest () (incf balance (* interest-rate balance)))) 

Here we use the generic functions defined by this macro: 

> (setf acct2 (account "A. User" 2000.00)) #<CL0SURE 24003064> 

> (deposit acct2 42.00) => 2042.0 

> (interest acct2) 2164.52 


<a id='page-441'></a>

> (balance acct2) ^ 2164.52 

> (balance acct) => 623.45 

In this last line, the generic function bal anee is applied to acct, an object that was 
created before we even defined the account class and the function balance. But 
bal anee still works properly on this object, because it obeys the message-passing 
protocol. 

13.5 Delegation 
Suppose we want to create a new kind of account, one that requires a password for 
each action. We can define a new class, password-account, that has two message 
clauses. The first clause allows for changing the password (if you have the original 
password), and the second is an otherwi se clause, which checks the password given 
and, if it is correct, passes the rest of the arguments on to the account that is being 
protected by the password. 

The definition of password-account takes advantage of the internal details of 
define-class in two ways: it makes use of the fact that otherwise can be used 
as a catch-all clause in a case form, and it makes use of the fact that the dispatch 
variable is called message. Usually, it is not a good idea to rely on details about the 
implementation of a macro, and soon we will see cleaner ways of defining classes. 
But for now, this simple approach works: 

(define-class password-account (password acct) () 
(change-password (pass new-pass) 

(if (equal pass password) 
(setf password new-pass) 
'wrong-password)) 

(otherwise (pass &rest args) 

(if (equal pass password) 
(apply message acct args) 
'wrong-password))) 

Now we see how the class password-account can be used to provide protection for 
an existing account: 

(setf acct3 (password-account "secret" acct2)) => #<CLOSURE 33427277> 
> (balance acct3 "secret") => 2164.52 
> (withdraw acct3 "guess" 2000.00) => WRONG-PASSWORD 
> (withdraw acct3 "secret" 2000.00) 164.52 

Now let's try one more example. Suppose we want to have a new class of account 


<a id='page-442'></a>

where only a limited amount of money can be withdrawn at any time. We could 
define the class 1 i mi ted - account: 

(define-class limited-account (limit acct) () 
(withdraw (amt) 

(if (> amt limit) 
'over-limit 
(withdraw acct amt))) 

(otherwise (&rest args) 
(apply message acct args))) 

This definition redefines the wi t hd raw message to check if the limit is exceeded before 
passing on the message, and it uses the otherwi se clause simply to pass on all other 
messages unchanged. In the following example, we set up an account with both a 
password and a limit: 

> (setf acct4 (password-account "pass" 
(limited-account 100.00 
(account "A. Thrifty Spender" 500.00)))) 
#<CLOSURE 34136775> 

> (withdraw acct4 "pass" 200.00) ^ OVER-LIMIT 

> (withdraw acct4 "pass" 20.00) => 480.0 

> (withdraw acct4 "guess" 20.00) => WRONG-PASSWORD 

Note that functions like wi thdraw are still simple generic functions that just find the 
right method and apply it to the arguments. The trick is that each class defines a different 
way to handle the withdraw message. Calling wi thdraw with acct4 as argument 
results in the following flow of control. First, the method in the password-account 
class checks that the password is correct. If it is, it calls the method from the 
1i mi ted-account class. If the limit is not exceeded, we finally call the method from 
the account class, which decrements the balance. Passing control to the method of 
a component is called delegation. 

The advantage of the object-oriented style is that we can introduce a new class 
by writing one definition that is localized and does not require changing any existing 
code. If we had written this in traditional procedural style, we would end up with 
functions like the following: 

(defun withdraw (acct amt &optional pass) 
(cond ((and (typep acct 'password-account) 
(not (equal pass (account-password acct)))) 
'wrong-password) 
((and (typep acct 'limited-account) 


<a id='page-443'></a>
(> amt (account-limit account))) 
'over-limit) 
((> amt balance) 
'insufficient-funds) 
(t (decf balance amt)))) 

There is nothing wrong with this, as an individual function. The problem is that 
when the bank decides to offer a new kind of account, we will have to change this 
function, along with all the other functions that implement actions. The "definition" 
of the new account is scattered rather than localized, and altering a bunch of existing 
functions is usually more error prone than writing a new class definition. 

13.6 Inheritance 
In the following table, data types (classes) are listed across the horizontal axis, and 
functions (messages) are listed up and down the vertical axis. A complete program 
needs to fill in all the boxes, but the question is how to organize the process of filling 
them in. In the traditional procedural style, we write function definitions that fill in 
a row at a time. In the object-oriented style, we write class definitions that fill in a 
column at a time. A third style, the data-dnven or generic style, fills in only one box at 
a time. 

account limited-password-
account account 
name object 
deposit oriented 
withdraw function oriented 
balance 
interest generic 

In this table there is no particular organization to either axis; both messages and 
classes are listed in random order. This ignores the fact that classes are organized hierarchically: 
both Hmited-account and password-account are subclasses of account. 
This was implicit in the definition of the classes, because both 1 i mi ted - account and 
password-account contain accounts as components and delegate messages to those 
components. But it would be cleaner to make this relationship explicit. 

The defstruct mechanism does allow for just this kind of explicit inheritance. If 
we had defined account as a structure, then we could define 1 i mi ted - account with: 


<a id='page-444'></a>

(defstruct (limited-account (:include account)) limit) 

Two things are needed to provide an inheritance facility for classes. First, we should 
modify define-class so that it takes the name of the class to inherit from as the 
second argument. This will signal that the new class will inherit all the instance 
variables, class variables, and methods from the parent class. The new class can, of 
course, define new variables and methods, or it can shadow the parent's variables and 
methods. In the form below, we define 1 i ml ted - account to be a subclass of account 
that adds a new instance variable, 11 mi t, and redefines the wi thdraw method so that 
it checks for amounts that are over the limit. If the amount is acceptable, then it uses 
the function cal 1 -next-method (not yet defined) to get at the withdraw method for 
the parent class, account. 

(define-class limited-account account (limit) () 
(withdraw (amt) 

(if (> amt limit) 
Over-limit 
(call-next-method)))) 

If inheritance is a good thing, then multiple inheritance is an even better thing. For 
example, assuming we have defined the classes 1 i mi ted - account and 
password - account, it is very convenient to define the following class, which inherits 
from both of them: 

(define-class limited-account-with-password 
(password-account 1 i mi ted-account)) 

Notice that this new class adds no new variables or methods. All it does is combine 

the functionality of two parent classes into one. 

&#9635; Exercise 13.1 [d] Define a version of def i ne-cl ass that handles inheritance and 
call-next-method. 

&#9635; Exercise 13.2 [d] Define a version of def i ne-cl ass that handles multiple inheritance. 



<a id='page-445'></a>
13.7 GLOS: The Common Lisp Object System 
So far, we have developed an object-oriented programming system using a macro, 
define-class, and a protocol for implementing objects as closures. There have 
been many proposals for adding object-oriented features to Lisp, some similar to 
our approach, some quite different. Recently, one approach has been approved to 
become an official part of Common Lisp, so we will abandon our ad hoc approach 
and devote the rest of this chapter to CLOS, the Common Lisp Object System. The 
correspondence between our system and CLOS is summarized here: 

our system CLOS 
define-class defclass 
methods defined in class defmethod 
class-name make-instance 
call-next-method call-next-method 
ensure-generic-fn ensure-generic-function 

Like most object-oriented systems, CLOS is primarily concerned with defining 
classes and methods for them, and in creating instances of the classes. In CLOS the 
macro def class defines a class, defmethod defines a method, and make-instance 
creates an instance of a class—an object. The general form of the macro def cl ass is: 

(def cl ass class-name (superclass...) (slot-specifier...) optional-class-option...) 

The class-options are rarely used, def cl ass can be used to define the class account: 

(defclass account () 

((name :initarg -.name :reader name) 

(balance linitarg rbalance linitform 0.00 raccessor balance) 

(interest-rate :allocation :class :initform .06 

:reader interest-rate))) 

In the definition of account, we see that the Ust of superclasses is empty, because 
account does not inherit from any classes. There are three slot specifiers, for the 
name, bal anee, and i nterest - rate slots. Each slot name can be followed by optional 
keyword/value pairs defining how the slot is used. The name slot has an : i ni targ 
option, which says that the name can be specified when a new account is created 
with make-instance. The :reader slot creates a method called name to get at the 
current value of the slot. 

The balance slot has three options: another :initarg, saying that the balance 
can be specified when a new account is made; an rinitform, which says that if 
the balance is not specified, it defaults to 0.00, and an raccessor, which creates a 


<a id='page-446'></a>

method for getting at the slot's value just as : reader does, and also creates a method 
for updating the slot with setf . 

The i nterest- rate slot has an : i ni tf orm option to give it a defauh value and an 
rail ocati on option to say that this slot is part of the class, not of each instance of the 
class. 

Here we see the creation of an object, and the application of the automatically 
defined methods to it. 

> (setf al (make-instance 'account chalanee 5000.00 

:name "Fred")) #<ACCOUNT 26726272> 

> (name al) ^ "Fred" 

> (balance al) 5000.0 

> (interest-rate al) ^ 0.06 

CLOS differs from most object-oriented systems in that methods are defined separately 
from classes. To define a method (besides the ones defined automatically by 
: reader, :writer, or :accessor options) we use the defmethod macro. It is similar 
to defun in form: 

(defmethod method-name (parameter..:) body...) 

Required parameters to a defmethod can be of the form (var class), meaning that 
this is a method that applies only to arguments of that class. Here is the method for 
withdrawing from an account. Note that CLOS does not have a notion of instance 
variable, only instance slot. So we have to use the method (bal ance acct) rather 
than the instance variable bal anee: 

(defmethod withdraw ((acct account) amt) 

(if (< amt (balance acct)) 
(decf (balance acct) amt) 
'i nsuffi ci ent-funds)) 

With CLOS it is easy to define a 1 imited-account as a subclass of account, and to 
define the wi thd raw method for 11 mi ted - accounts: 

(defclass limited-account (account) 
((limit :initarg ilimit -.reader limit))) 

(defmethod withdraw ((acct limited-account) amt) 

(if (> amt (limit acct)) 
Over-limit 
(call-next-method))) 


<a id='page-447'></a>
Note the use of cal1 -next-method to invoke the withdraw method for the account 
class. Also note that all the other methods for accounts automatically work on 
instances of the class limited-account, because it is defined to inherit from account. In 
the following example, we show that the name method is inherited, that the wi thdraw 
method for 1 i mi ted-account is invoked first, and that the withdraw method for 
account is invoked by the cal1 -next-method function: 

> (setf a2 (make-instance 'limited-account 
:name "A. Thrifty Spender" 
:balance 500.00 :limit 100.00)) ^ 

#<LIMITED-ACCOUNT 24155343> 

> (name a2) ^ "A. Thrifty Spender" 

> (withdraw a2 200.00) ^ OVER-LIMIT 

> (withdraw a2 20.00) 480.0 

In general, there may be several methods appropriate to a given message. In that case, 
all the appropriate methods are gathered together and sorted, most specific first. The 
most specific method is then called. That is why the method for 1 i mi ted - account is 
called first rather than the method for account. The function cal 1 -next-method can 
be used within the body of a method to call the next most specific method. 

The complete story is actually even more complicated than this. As one example 
of the complication, consider the class audi ted-a ccount, which prints and keeps 
a trail of all deposits and withdrawals. It could be defined as follows using a new 
feature of CLOS, : before and : after methods: 

(defclass audited-account (account) 
((audit-trail :initform nil :accessor audit-trail))) 

(defmethod withdraw ibefore ((acct audited-account) amt) 
(push (print '(withdrawing .amt)) 
(audit-trail acct))) 

(defmethod withdraw rafter ((acct audited-account) amt) 
(push (print '(withdrawal (.amt) done)) 
(audit-trail acct))) 

Now a call to withdraw with a audited-account as the first argument yields three 

applicable methods: the primary method from account and the : before and rafter 

methods. In general, there might be several of each kind of method. In that case, 

all the : before methods are called in order, most specific first. Then the most 

specific primary method is called. It may choose to invoke cal 1 - next-method to 

get at the other methods. (It is an error for a : before or : after method to use 

cal 1 -next-method.) Finally, all the rafter methods are called, least specific first. 


<a id='page-448'></a>

The values from the : before and : after methods are ignored, and the value from 
the primary method is returned. Here is an example: 

> (setf a3 (make-instance 'audited-account .-balance 1000.00)) 
#<AUDITED-ACCOUNT 33555607> 

> (withdraw a3 100.00) 

(WITHDRAWING 100.0) 

(WITHDRAWAL (100.0) DONE) 

900.0 

> (audit-trail a3) 
((WITHDRAWAL (100.0) DONE) (WITHDRAWING 100.0)) 

> (setf (audit-trail a3) nil) 
NIL 

The last interaction shows the biggest flaw in CLOS: it fails to encapsulate information. 
In order to make the audi t-trai 1 accessible to the wi thdraw methods, we had 
to give it accessor methods. We would like to encapsulate the writer function for 
audit-trail so that it can only be used with deposit and withdraw. But once the 
writer function is defined it can be used anywhere, so an unscrupulous outsider can 
destroy the audit trail, setting it to nil or anything else. 

13.8 A CLOS Example: Searching Tools 
CLOS is most appropriate whenever there are several types that share related behavior. 
A good example of an application that fits this description is the set of searching 
tools defined in section 6.4. There we defined functions for breadth-first, depth-
first, and best-first search, as well as tree- and graph-based search. We also defined 
functions to search in particular domains, such as planning a route between cities. 

If we had written the tools in a straightforward procedural style, we would have 
ended up with dozens of similar functions. Instead, we used higher-order functions 
to control the complexity. In this section, we see how CLOS can be used to break up 
the complexity in a slightly different fashion. 

We begin by defining the class of search problems. Problems will be classified 
according to their domain (route planning, etc.), their topology (tree or graph) and 
their search strategy (breadth-first or depth-first, etc.). Each combination of these 
features results in a new class of problem. This makes it easy for the user to add a new 
class to represent a new domain, or a new search strategy. The basic class, probl em, 
contains a single-instance variable to hold the unexplored states of the problem. 


<a id='page-449'></a>
(defclass problem () 
((states linitarg estates :accessor problem-states))) 

The function searcher is similar to the function tree-search of section 6.4. The 
main difference is that searcher uses generic functions instead of passing around 
functional arguments. 

(defmethod searcher ((prob problem)) 
"Find a state that solves the search problem." 
(cond ((no-states-p prob) fail) 

((goal-p prob) (current-state prob)) 
(t (let ((current (pop-state prob))) 
(setf (problem-states prob) 

(problem-combiner 
prob 
(problem-successors prob current) 
(problem-states prob)))) 

(searcher prob)))) 

searcher does not assume that the problem states are organized in a list; rather, it 
uses the generic function no-states-p to test if there are any states, pop-state to 
remove and return the first state, and current - state to access the first state. For the 
basic probl em class, we will in fact implement the states as a list, but another class of 
problem is free to use another representation. 

(defmethod current-state ((prob problem)) 
"The current state is the first of the possible states." 
(first (problem-states prob))) 

(defmethod pop-state ((prob problem)) 
"Remove and return the current state." 
(pop (problem-states prob))) 

(defmethod no-states-p ((prob problem)) 
"Are there any more unexplored states?" 
(null (problem-states prob))) 

In tree - sea rch, we included a statement to print debugging information. We can do 
that here, too, but we can hide it in a separate method so as not to clutter up the main 
definition of searcher. It is a :before method because we want to see the output 
before carrying out the operation. 


<a id='page-450'></a>

(defmethod searcher .-before ((prob problem)) 
(dbg 'search "~&;; Search: ~a" (problem-states prob))) 

The generic functions that remain to be defined are goal -p, probl em-combi ner, and 
probl em-successors. We will address goal -p first, by recognizing that for many 
problems we will be searching for a state that is eql to a specified goal state. We 
define the class eql -probl em to refer to such problems, and specify goal -p for that 
class. Note that we make it possible to specify the goal when a problem is created, 
but not to change the goal: 

(defclass eql-problem (problem) 
((goal :initarg :goal :reader problem-goal))) 

(defmethod goal-p ((prob eql-problem)) 
(eql (current-state prob) (problem-goal prob))) 

Now we are ready to specify two search strategies: depth-first search and 
breadth-first search. We define problem classes for each strategy and specify the 
probl em- combi ner function: 

(defclass dfs-problem (problem) () 
(:documentation "Depth-first search problem.")) 

(defclass bfs-problem (problem) () 
(:documentation "Breadth-first search problem.")) 

(defmethod problem-combiner ((prob dfs-problem) new old) 
"Depth-first search looks at new states first." 
(append new old)) 

(defmethod problem-combiner ((prob bfs-problem) new old) 
"Depth-first search looks at old states first." 
(append old new)) 

While this code will be sufficient for our purposes, it is less than ideal, because it 
breaks an information-hiding barrier. It treats the set of old states as a list, which is the 
default for the . r obi em class but is not necessarily the implementation that every class 
will use. It would have been cleaner to define generic functions add - sta tes - to - end 
and add-states-to-front and then define them with append in the default class. 
But Lisp provides such nice list-manipulation primitives that it is difficult to avoid 
the temptation of using them directly. 

Of course, the user who defines a new implementation for probl em-states 
could just redefine probl em- combi ner for the offending classes, but this is precisely 
what object-oriented programming is designed to avoid: specializing one abstraction 
(states) should not force us to change anything in another abstraction (search 
strategy). 


<a id='page-451'></a>
The last step is to define a class that represents a particular domain, and define 
problem-successors for that domain. As the first example, consider the simple 
binary tree search from section 6.4. Naturally, this gets represented as a class: 

(defclass binary-tree-problem (problem) ()) 

(defmethod problem-successors ((prob binary-tree-problem) state) 
(let ((n (* 2 state))) 
(list . (+ . 1)))) 

Now suppose we want to solve a binary-tree problem with breadth-first search, 
searching for a particular goal. Simply create a class that mixes in 
binary-tree-problem, eql-problem and bfs-problem, create an instance of that 
class, and call searcher on that instance: 

(defclass binary-tree-eql-bfs-problem 
(binary-tree-problem eql-problem bfs-problem) ()) 

> (setf pi (make-instance 'binary-tree-eql-bfs-problem 
istates '(1) :goal 12)) 
#<BINARY-TREE-EQL-BFS-PROBLEM 26725536> 

> (searcher pi) 
Search: (1) 
Search: (2 3) 
Search: (3 4 5) 
Search: (4 5 6 7) 
Search: (5 6 7 8 9) 
Search: (6 7 8 9 10 11) 
Search: (7 8 9 10 11 12 13) 
Search: (8 9 10 11 12 13 14 15) 
Search: (9 10 11 12 13 14 15 16 17) 
Search: (10 11 12 13 14 15 16 17 18 19) 
Search: (11 12 13 14 15 16 17 18 19 20 21) 
Search: (12 13 14 15 16 17 18 19 20 21 22 23) 

12 

Best-First Search 

It should be clear how to proceed to define best-first search: define a class to represent 
best-first search problems, and then define the necessary methods for that class. 
Since the search strategy only affects the order in which states are explored, the only 
method necessary will be for probl em- combi ner. 


<a id='page-452'></a>

(defclass best-problem (problem) 0 
(.'documentation "A Best-first search problem.")) 

(defmethod problem-combiner ((prob best-problem) new old) 
"Best-first search sorts new and old according to cost-fn." 
(sort (append new old) #'< 

:key #'(lambda (state) (cost-fn prob state)))) 

This introduces the new function cost -f n; naturally it will be a generic function. The 
following is a cos t -f . that is reasonable for any eq 1 - . rob1 em dealing with numbers, 
but it is expected that most domains will specialize this function. 

(defmethod cost-fn ((prob eql-problem) state) 
(abs (- state (problem-goal prob)))) 

Beam search is a modification of best-first search where all but the best b states are 
thrown away on each iteration. A beam search problem is represented by a class 
where the instance variable beam-width holds the parameter b. If this nil, then full 
best-first search is done. Beam search is implemented by an : a round method on 
problem-combiner. It calls the next method to get the list of states produced by 
best-first search, and then extracts the first 6 elements. 

(defclass beam-problem (problem) 
((beam-width :initarg :beam-width :initform nil 
:reader problem-beam-width))) 

(defmethod problem-combiner raround ((prob beam-problem) new old) 
(let ((combined (call-next-method))) 
(subseq combined 0 (min (problem-beam-width prob) 
(length combined))))) 

Now we apply beam search to the binary-tree problem. As usual, we have to make 
up another class to represent this type of problem: 

(defclass binary-tree-eql-best-beam-problem 
(binary-tree-problem eql-problem best-problem beam-problem) 
()) 

> (setf p3 (make-instance 'binary-tree-eql-best-beam-problem 
rstates '(1) :goal 12 :beam-width 3)) 
#<BINARY-TREE-EQL-BEST-BEAM-PROBLEM 27523251> 

> (searcher p3) 
Search: (1) 
Search: (3 2) 
Search: (7 6 2) 
Search: (14 15 6) 
Search: (15 6 28) 


<a id='page-453'></a>
Search: (6 28 30) 
Search: (12 13 28) 
12 

So far the case for CLOS has not been compelling. The code in this section duplicates 
the functionality of code in section 6.4, but the CLOS code tends to be more verbose, 
and it is somewhat disturbing that we had to make up so many long class names. 
However, this verbosity leads to flexibility, and it is easier to extend the CLOS code by 
adding new specialized classes. It is useful to make a distinction between the systems 
programmer and the applications programmer. The systems programmer would 
supply a library of classes like dfs-problem and generic functions like searcher. 
The applications programmer then just picks what is needed from the library. From 
the following we see that it is not too difficult to pick out the right code to define a 
trip-planning searcher. Compare this with the definition of tri . on [page 198](chapter6.md#page-198) to see 
if you prefer CLOS in this case. The main difference is that here we say that the cost 
function is a i r-di stance and the successors are the nei ghbors by defining methods; 
in tri . we did it by passing parameters. The latter is a little more succint, but the 
former may be more clear, especially as the number of parameters grows. 

(defclass trip-problem (binary-tree-eql-best-beam-problem) 
((beam-width :initform 1))) 

(defmethod cost-fn ((prob trip-problem) city) 
(air-distance (problem-goal prob) city)) 

(defmethod problem-successors ((prob trip-problem) city) 
(neighbors city)) 

With the definitions in place, it is easy to use the searching tool: 

> (setf p4 (make-instance 'trip-problem 
estates (list (city 'new-york)) 
:goal (city 'san-francisco))) 

#<TRIP-PROBLEM 31572426> 

> (searcher p4) 
Search: ((NEW-YORK 73.58 40.47)) 
Search: ((PITTSBURG 79.57 40.27)) 
Search: ((CHICAGO 87.37 41.5)) 
Search: ((KANSAS-CITY 94.35 39.06)) 
Search: ((DENVER 105.0 39.45)) 

;; Search: ((FLAGSTAFF 111.41 35.13)) 

Search: ((RENO 119.49 39.3)) 
;: Search: ((SAN-FRANCISCO 122.26 37.47)) 
(SAN-FRANCISCO 122.26 37.47) 


<a id='page-454'></a>

13.9 Is CLOS Object-Oriented? 
There is some argument whether CLOS is really object-oriented at all. The arguments 
are: 

CLOS IS an object-oriented system because it provides all three of the main criteria 
for object-orientation: objects with internal state, classes of objects with specialized 
behavior for each class, and inheritance between classes. 

CLOS is not an object-oriented system because it does not provide modular 
objects with information-hiding. In the audi ted-account example, we would like to 
encapsulate the audit-trail instance variable so that only the withdraw methods 
can change it. But because methods are written separately from class definitions, 
we could not do that. Instead, we had to define an accessor for audi t-trai 1. That 
enabled us to write the withdraw methods, but it also made it possible for anyone 
else to alter the audit trail as well. 

CLOS ismore general than an object-oriented system because it allows for methods 
that specialize on more than one argument. In true object-oriented systems, methods 
are associated with objects of a particular class. This association is lexically obvious 
(and the message-passing metaphor is clear) when we write the methods inside the 
definition of the class, asinourdef i ne-cl ass macro. The message-passing metaphor 
is still apparent when we write generic functions that dispatch on the class of their 
first argument, which is how we've been using CLOS so far. 

But CLOS methods can dispatch on the class of any required argument, or any 
combination of them. Consider the following definition of cone, which is like append 
except that it works for vectors as well as lists. Rather than writing cone using 
conditional statements, we can use the multimethod dispatch capabilities of CLOS 
to define the four cases: (1) the first argument is nil, (2) the second argument is nil, 

(3) both arguments are lists, and (4) both arguments are vectors. Notice that if one of 
the arguments is nil there will be two applicable methods, but the method for nul 1 
will be used because the class nul 1 is more specific than the class list. 
(defmethod cone ((x null) y) y) 

(defmethod cone (x (y null)) x) 

(defmethod cone ((x list) (y list)) 
(cons (first x) (cone (rest x) y))) 

(defmethod cone ((x vector) (y vector)) 

(let ((vect (make-array (+ (length x) (length y))))) 

(replace vect x) 

(replace vect y rstartl (length x)))) 


<a id='page-455'></a>
Here we see that this definition works: 

> (cone nil '(a b c)) => (A . C) 

> (cone '(a b c) nil) => (A . C) 

> (cone '(a b c) '(d e f)) (A . C D . F) 

> (cone '#(a b e) '#(d e f)) => #(A . C D . F) 

It works, but one might well ask: where are the objects? The metaphor of passing a 
message to an object does not apply here, unless we consider the object to be the list 
of arguments, rather than a single privileged argument. 

It is striking that this style of method definition is very similar to the style used 
in Prolog. As another example, compare the following two definitions of 1 en, a 
relation/function to compute the length of a list: 

CLOS %% Prolog 
(defmethod len ((x null)) 0) len(C].0). 

(defmethod len ((x eons)) len([XIL].N1) :(+ 
1 (len (rest x)))) len(L.N). Nl is N+1. 

13.10 Advantages of Object-Oriented 
Programming 
Bertrand Meyer, in his book on the object-oriented language Eiffel (1988), lists five 
qualities that contribute to software quality: 

* Correctness. Clearly, a correct program is of the upmost importance. 
* Robustness. Programs should continue to function in a reasonable manner even 
for input that is beyond the original specifications. 
* Extendability. Programs should be easy to modify when the specifications 
change. 
* Reusability. Program components should be easy to transport to new programs, 
thus amortizing the cost of software development over several projects. 
* Compatibility. Programs should interface well with other programs. For example, 
a spreadsheet program should not only manipulate numbers correctly but 
also be compatible with word processing programs, so that spreadsheets can 
easily be included in documents. 

<a id='page-456'></a>

Here we list how the object-oriented approach in general and CLOS in particular 
can effect these measures of quality: 

* Conectness. Correctness is usually achieved in two stages: correctness of 
individual modules and correctness of the whole system. The object-oriented 
approach makes it easier to prove correctness for modules, since they are 
clearly defined, and it may make it easier to analyze interactions between 
modules, since the interface is strictly limited. CLOS does not provide for 
information-hiding the way other systems do. 
* Robustness. Generic functions make it possible for a function to accept, at run 
time, a class of argument that the programmer did not anticipate at compile 
time. This is particularly true in CLOS, because multiple inheritance makes it 
feasible to write default methods that can be used by a wide range of classes. 
* Extendability. Object-oriented systems with inheritance make it easy to define 
new classes that are slight variants on existing ones. Again, CLOS's multiple 
inheritance makes extensions even easier than in single-inheritance systems. 
* Reusability. This is the area where the object-oriented style makes the biggest 
contribution. Instead of writing each new program from scratch, object-
oriented programmers can look over a library of classes, and either reuse 
existing classes as is, or specialize an existing class through inheritance. Large 
libraries of CLOS classes have not emerged yet. Perhaps they will when the 
language is more established. 
* Compatibility. The more programs use standard components, the more they will 
be able to communicate with each other. Thus, an object-oriented program will 
probably be compatible with other programs developed from the same library 
of classes. 
13.11 History and References 
The first object-oriented language was Simula, which was designed by Ole-Johan 
Dahl and Krysten Nygaard (1966, Nygaard and Dahl 1981) as an extension of Algol 60. 
It is still in use today, mostly in Norway and Sweden. Simula provides the ability to 
define classes with single inheritance. Methods can be inherited from a superclass 
or overridden by a subclass. It also provides coroutines, class instances that execute 
continuously, saving local state in instance variables but periodically pausing to let 
other coroutines run. Although Simula is a general-purpose language, it provides 
special support for simulation, as the name implies. The built-in class simul ation 
allows a programmer to keep track of simulated time while running a set of processes 
as coroutines. 


<a id='page-457'></a>
In 1969 Alan Kay was a graduate student at the University of Utah. He became 
aware of Simula and realized that the object-oriented style was well suited to his 
research in graphics (Kay 1969). A few years later, at Xerox, he joined with Adele 
Goldberg and Daniel Ingalls to develop the Smalltalk language (see Goldberg and 
Robinson 1983). While Simula can be viewed as an attempt to add object-oriented 
features to strongly typed Algol 60, Smalltalk can be seen as an attempt to use the 
dynamic, loosely typed features of Lisp, but with methods and objects replacing 
functions and s-expressions. In Simula, objects existed alongside traditional data 
types like numbers and strings; in Smalltalk, every datum is an object. This gave 
Smalltalk the feel of an integrated Lisp environment, where the user can inspect, copy, 
or edit any part of the environment. In fact, it was not the object-oriented features of 
Smalltalk per se that have made a lasting impression but rather the then-innovative 
idea that every user would have a large graphical display and could interact with the 
system using a mouse and menus rather than by typing commands. 

Guy Steele's LAMBDA: The Ultimate Declarative (1976a and b) was perhaps the 
first paper to demonstrate how object-oriented programming can be done in Lisp. As 
the title suggests, it was all done using 1 ambda, in a similar way to our def i ne-cl ass 
example. Steele summarized the approach with the equation "Actors = Closures 
(mod Syntax)," refering to Carl Hewitt's "Actors" object-oriented formalism. 

In 1979, the MIT Lisp Machine group developed the Flavors system based on this 
approach but offering considerable extensions (Cannon 1980, Weinreb 1980, Moon 
et al. 1983). "Flavor" was a popular jargon word for "type" or "kind" at MIT, so it was 
natural that it became the term for what we call classes. 

The Flavor system was the first to support multiple inheritance. Other languages 
shunned multiple inheritance because it was too dynamic. With single inheritance, 
each instance variable and method could be assigned a unique offset number, and 
looking up a variable or method was therefore trivial. But with multiple inheritance, 
these computations had to be done at run time. The Lisp tradition enabled programmers 
to accept this dynamic computation, when other languages would not. 
Once it was accepted, the MIT group soon came to embrace it. They developed 
complex protocols for combining different flavors into new ones. The concept of 
mix-ins was developed by programmers who frequented Steve's Ice Cream parlor in 
nearby Davis Square. Steve's offered a list of ice cream flavors every day but also 
offered to create new flavors—dynamically—by mixing in various cookies, candies, 
or fruit, at the request of the individual customer. For example, Steve's did not have 
chocolate-chip ice cream on the menu, but you could always order vanilla ice cream 
with chocolate chips mixed in.^ 

This kind of "flavor hacking" appealed to the MIT Lisp Machine group, who 

^Flavor fans will be happy to know that Steve's Ice Cream is now sold nationally in the 
United States. Alas, it is not possible to create flavors dynamically. Also, be warned that 
Steve's was bought out by his Teal Square rival, Joey's. The original Steve retired from the 
business for years, then came back with a new line of stores under his last name, Harrell. 


<a id='page-458'></a>

adopted the metaphor for their object-oriented programming system. All flavors 
inherited from the top-most flavor in the hierarchy: vanilla. In the window system, for 
example, the flavor basi c-wi ndow was defined to support the minimal functionality 
of all windows, and then new flavors of window were defined by combining mix-in 
flavors such as scroll -bar-mixin, label -mixin, and border-mixin. These mix-in 
flavors were used only to define other flavors. Just as you couldn't go into Steve's and 
order "crushed Heath bars, hold the ice cream," there was a mechanism to prohibit 
instantiation of mix-ins. 

A complicated repetoire of method combinations was developed. The default 
method combination on Flavors was similar to CLOS: first do all the : before methods, 
then the most specific primary method, then the : after methods. But it was 
possible to combine methods in other ways as well. For example, consider the 
i ns i de - wi dth method, which returns the width in pixels of the usuable portion of a 
window. A programmer could specify that the combined method for i nsi de-wi dth 
was to be computed by calling all applicable methods and summing them. Then an 
inside-width method for the basic-window flavor would be defined to return the 
width of the full window, and each mix-in would have a simple method to say how 
much of the width it consumed. For example, if borders are 8 pixels wide and scroll 
bars are 12 pixels wide, then the i nsi de-wi dth method for border-mi xi . returns -8 
andscroll -bar-mixinreturns -12. Thenany window, no matter how many mix-ins 
it is composed of, automatically computes the proper inside width. 

In 1981, Symbolics came out with a more efficient implementation of Flavors. 
Objects were no longer just closures. They were still funcallable, but there was 
additional hardware support that distinguished them from other functions. After a 
few years Symbolics abandoned the (send object message) syntax in favor of a new 
syntax based on generic functions. This system was known as New Flavors. It had a 
strong influence on the eventual CLOS design. 

The other strong influence on CLOS was the CommonLoops system developed 
at Xerox PARC. (See Bobrow 1982, Bobrow et al. 1986, Stefik and Bobrow 1986.) 
CommonLoops continued the New Flavors trend away from message passing by 
introducing multimethods: methods that specialize on more than one argument. 

As of summer 1991, CLOS itself is in a state of limbo. It was legitimitized by its 
appearance in Common Lisp the Language, 2d edition, but it is not yet official, and an 
important part, the metaobject protocol, is not yet complete. A tutorial on CLOS is 
Keenel989. 

We have seen how easy it is to build an object-oriented system on top of Lisp, 
using 1 ambda as the primary tool. An interesting alternative is to build Lisp on top of 
an object-oriented system. That is the approach taken in the Oaklisp system of Lang 
and Perlmutter (1988). Instead of defining methods using 1 ambda as the primitive, 
OakHsp has add-method as a primitive and defines 1 ambda as a macro that adds a 
method to an anonymous, empty operation. 

Of course, object-oriented systems are thriving outside the Lisp world. With the 


<a id='page-459'></a>

success of UNIX-based workstations, C has become one of the most widely available 
programming languages. C is a fairly low-level language, so there have been several 
attempts to use it as a kind of portable assembly language. The most succesful of 
these attempts is C++, a language developed by Bjarne Stroustrup of AT&T Bell Labs 
(Stroustrup 1986). C++ provides a number of extensions, including the ability to 
define classes. However, as an add-on to an existing language, it does not provide as 
many features as the other languages discussed here. Crucially, it does not provide 
garbage collection, nor does it support fully generic functions. 

Eiffel (Meyer 1988) is an attempt to define an object-oriented system from the 
ground up rather than tacking it on to an existing language. Eiffel supports multiple 
inheritance and garbage collection and a limited amount of dynamic dispatching. 

So-called modern languages like Ada and Modula support information-hiding 
through generic functions and classes, but they do not provide inheritance, and thus 
can not be classified as true object-oriented languages. 

Despite these other languages, the Lisp-based object-oriented systems are the 
only ones since Smalltalk to introduce important new concepts: multiple inheritance 
and method combination from Flavors, and multimethods from CommonLoops. 

13.12 Exercises 
&#9635; Exercise 13.3 [m] Implement deposit and interest methods for the account class 
using CLOS. 

&#9635; Exercise 13.4 [m] Implement the password-account class using CLOS. Can it be 
done as cleanly with inheritance as it was done with delegation? Or should you use 
delegation within CLOS? 

&#9635; Exercise 13.5 [h] Implement graph searching, search paths, and A* searching as 
classes in CLOS. 

&#9635; Exercise 13.6 [h] Implement a priority queue to hold the states of a problem. Instead 
of a list, the probl em-states will be a vector of lists, each initially null. Each 
new state will have a priority (determined by the generic function priori ty) which 
must be an integer between zero and the length of the vector, where zero indicates the 
highest priority. A new state with priority . is pushed onto element . of the vector, 
and the state to be explored next is the first state in the first nonempty position. As 
stated in the text, some of the previously defined methods made the unwarranted 
assumption that probl em-states would always hold a Hst. Change these methods. 


## Chapter 14
<a id='page-460'></a>

Knowledge Representation 
and Reasoning 

Knowledge itself is power. 

-Francis Bacon (. 561-1626) 

The power resides in the knowledge. 

—Edward Feigenbaum 
Stanford University Heuristic Programming Project 

Knowledge is Knowledge, and vice versa. 

—Tee shirt 
Stanford University Heuristic Programming Project 

I
I
n the 1960s, much of AI concentrated on search techniques. In particular, a lot of w^ork v^as 
concerned with theorem proving: stating a problem as a small set of axioms and searching for 
a proof of the problem. The implicit assumption was that the power resided in the inference 
mechanism-if we could just find the right search technique, then all our problems would be 
solved, and all our theorems would be proved. 


<a id='page-461'></a>

Starting in the 1970s, this began to change. The theorem-proving approach failed 
to live up to its promise. AI workers slowly began to realize that they were not going 
to solve NP-hard problems by conung up with a clever inference algorithm. The 
general inferencing mechanisms that worked on toy examples just did not scale up 
when the problem size went into the thousands (or sometimes even into the dozens). 

The expert-system approach offered an alternative. The key to solving hard problems 
was seen to be the acquisition of special-case rules to break the problem into 
easier problems. According to Feigenbaum, the lesson learned from expert systems 
like MYCIN (which we will see in chapter 16) is that the choice of inferencing mechanism 
is not as important as having the right knowledge. In this view it doesn't 
matter very much if MYCIN uses forward- or backward-chaining, or if it uses certainty 
factors, probabilities, or fuzzy set theory. What matters crucially is that we know 
Pseudomonas is a gram-negative, rod-shaped organism that can infect patients with 
compromised immune systems. In other words, the key problem is acquiring and 
representing knowledge. 

While the expert system approach had some successes, it also had failiu-es, and 
researchers were interested in learning the limits of this new technology and understanding 
exactly how it works. Many found it troublesome that the meaning of the 
knowledge used in some systems was never clearly defined. For example, does the 
assertion (color appl e red) mean that a particular apple is red, that all apples are 
red, or that some/most apples are red? The field of knowledge representation concentrated 
on providing clear semantics for such representations, as well as providing 
algorithms for manipulating the knowledge. Much of the emphasis was on finding a 
good trade-off between expressiveness and efficiency. An efficient language is one for 
which all queries (or at least the average query) can be answered quickly. If we want 
to guarantee that queries will be answered quickly, then we have to limit what can 
be expressed in the language. 

In the late 1980s, a series of results shed doubt on the hopes of finding an efficient 
language with any reasonable degree of expressiveness at all. Using mathematical 
techniques based on worst-case analysis, it was shown that even seemingly trivial 
languages were intractable—in the worst case, it would take an exponential amount of 
time to answer a simple query. 

Thus, in the 1990s the emphasis has shifted to knowledge representation and reasoning, 
a field that encompasses both the expressiveness and efficiency of languages but 
recognizes that the average case is more important than the worst case. No amount 
of knowledge can help solve an intractable problem in the worse case, but in practice 
the worst case rarely occurs. 


<a id='page-462'></a>

14.1 A Taxonomy of Representation Languages 
AI researchers have investigated hundreds of knowledge representation languages, 
trying to find languages that are convenient, expressive, and efficient. The languages 
can be classified into four groups, depending on what the basic unit of representation 
is. Here are the four categories, with some examples: 

* Logical Formulae (Prolog) 
* Networks (semantic nets, conceptual graphs) 
* Objects (scripts, frames) 
* Procedures (Lisp, production systems) 
We have already dealt with logic-based languages like Prolog. 

Network-based languages can be seen as a syntactic variation on logical languages. 
A link L between nodes A and . is just another way of expressing the logical relation 
B), The difference is that network-based languages take their links more 
seriously: they are intended to be implemented directly by pointers in the computer, 
and inference is done by traversing these pointers. So placing a link L between A 
and . not only asserts that L(A, B) is true, but it also says something about how the 
knowledge base is to be searched. 

Object-oriented languages can also be seen as syntactic variants of predicate calculus. 
Here is a statement in a typical slot-filler frame language: 

(a person 

(name = Jan) 

(age = 32)) 

This is equivalent to the logical formula: 

3 p: person(p) . name(p,Jan) . age(p,32) 

The frame notation has the advantage of being easier to read, in some people's 
opinion. However, the frame notation is less expressive. There is no way to say that 
the person's name is either Jan or John, or that the person's age is not 34. In predicate 
calculus, of course, such statements can be easily made. 

Finally, procedural languages are to be contrasted with representation languages: 
procedural languages compute answers without explicit representation of knowledge. 


There are also hybrid representation languages that use different methods to 
encode different kinds of knowledge. The KL-ONE family of languages uses both 
logical formulae and objects arranged into a network, for example. Many frame 


<a id='page-463'></a>
languages allow procedural attachment, a technique that uses arbitrary procedures to 
compute values for expressions that are inconvenient or impossible to express in the 
frame language itself. 

14.2 Predicate Calculus and its Problems 
So far, many of our representations have been based on predicate calculus, a notation 
with a distinguished position in AI: it serves as the universal standard by which other 
representations are defined and evaluated. The previous section gave an example 
expression from a frame language. The frame language may have many merits in 
terms of the ease of use of its syntax or the efficiency of its internal representation of 
data. However, to understand what expressions in the language mean, there must be 
a clear definition. More often than not, that definition is given in terms of predicate 
calculus. 

A predicate calculus representation assumes a universe of individuals, with relations 
and functions on those individuals, and sentences formed by combining 
relations with the logical connectives and, or, and not. Philosophers and psychologists 
will argue the question of how appropriate predicate calculus is as a model of 
human thought, but one point stands clear: predicate calculus is sufficient to represent 
anything that can be represented in a digital computer. This is easy to show: 
assuming the computer's memory has . bits, and the equation hi = 1 means that bit 
i is on, then the entire state of the computer is represented by a conjunction such as: 

(6o = 0) . (6i = 0) . (62 = 1) . ... . {bn = 0) 

Once we can represent a state of the computer, it becomes possible to represent 
any computer program in predicate calculus as a set of axioms that map one state onto 
another. Thus, predicate calculus is shown to be asufficientlangaage for representing 
anything that goes on inside a computer—it can be used as a tool for analyzing any 
program from the outside. 

This does not prove that predicate calculus is an appropriate tool for all applications. 
There are good reasons why we may want to represent knowledge in a form 
that is quite different from predicate calculus, and manipulate the knowledge with 
procedures that are quite different from logical inference. But we should still be able 
to describe our system in terms of predicate calculus axioms, and prove theorems 
about it. To do any less is to be sloppy. For example, we may want to manipulate 
numbers inside the computer by using the arithmetic instructions that are built into 
the CPU rather than by manipulating predicate calculus axioms, but when we write 
a square-root routine, it had better satisfy the axiom: 

y/x = y=^yxy = x 


<a id='page-464'></a>

Predicate calculus also serves another purpose: as a tool that can be used by a 
program rather than on a program. All programs need to manipulate data, and some 
programs will manipulate data that is considered to be in predicate calculus notation. 
It is this use that we will be concerned with. 

Predicate calculus makes it easy to start writing down facts about a domain. But 
the most straightforward version of predicate calculus suffers from a number of 
serious limitations: 

* Decidability—^ven a set of axioms and a goal, it may be that neither the goal nor 
its negation can be derived from the axioms. 
* Tractability—even when a goal is provable, it may take too long to find the proof 
using the available inferencing mechanisms. 
* Uncertainty—it can be inconvenient to deal with relations that are probable to a 
degree but not known to be definitely true or false. 
* Monotonicity—in pure predicate calculus, once a theorem is proved, it is true 
forever. But we would like a way to derive tentative theorems that rely on 
assumptions, and be able to retract them when the assumptions prove false. 
* Consistency—pure predicate calculus admits no contradictions. If by accident 
both P and &not;P are derived, then any theorem can be proved. In effect, a single 
contradiction corrupts the entire data base. 
* Omniscience—it can be difficult to distinguish what is provable from what should 
be proved. This can lead to the unfounded assumption that an agent believes 
all the consequences of the facts it knows. 
* Expressiveness—the first-order predicate calculus makes it awkward to talk 
about certain things, such as the relations and propositions of the language 
itself. 
The view held predominantly today is that it is best to approach these problems 
with a dual attack that is both within and outside of predicate calculus. It is considered 
a good idea to invent new notations to address the problems—both for convenience 
and to facilitate special-purpose reasoners that are more efficient than a general-
purpose theorem prover. However, it is also important to define scrupulously the 
meaning of the new notation in terms of familiar predicate-calculus notation. As 
Drew McDermott put it, "No notation without denotation!" (1978). 

In this chapter we show how new notations (and their corresponding meanings) 
can be used to extend an existing representation and reasoning system. Prolog is 
chosen as the language to extend. This is not meant as an endorsement for Prolog as 
the ultimate knowledge representation language. Rather, it is meant solely to give us 
a clear and familiar foundation from which to build. 


<a id='page-465'></a>
14.3 A Logical Language: Prolog 
Prolog has been proposed as the answer to the problem of programming in logic. Why 
isn't it accepted as the universal representation language? Probably because Prolog 
is a compromise between a representation language and a programming language. 
Given two specifications that are logically equivalent, one can be an efficient Prolog 
program, while the other is not. Kowalski's famous equation "algonthm = logic + 
control" expresses the limits of logic alone: logic = algorithm -control Many problems 
(especially in AI) have large or infinite search spaces, and if Prolog is not given some 
advice on how to search that space, it will not come up with the answer in any 
reasonable length of time. 

Prolog's problems fall into three classes. First, in order to make the language 
efficient, its expressiveness was restricted. It is not possible to assert that a person's 
name is either Jan or John in Prolog (although it is possible to ask if the person's 
name is one of those). Similarly, it is not possible to assert that a fact is false; 
Prolog does not distinguish between false and unknown. Second, Prolog's inference 
mechanism is neither sound nor complete. Because it does not check for circular 
unification, it can give incorrect answers, and because it searches depth-first it can 
miss correct answers. Third, Prolog has no good way of adding control information 
to the underlying logic, making it inefficient on certain problems. 

14.4 Problems with Prolog's Expressiveness 
If Prolog is programming in logic, it is not the full predicate logic we are familiar with. 
The main problem is that Prolog can't express certain kinds of indefinite facts. It can 
represent definite facts: the capital of Rhode Island is Providence. It can represent 
conjunctions of facts: the capital of Rhode Island is Providence and the capital of 
California is Sacramento. But it can not represent disjunctions or negations: that the 
capital of California is not Los Angeles, or that the capital of New York is either New 
York City or Albany. We could try this: 

(<- (not (capital LA CA))) 
(<- (or (capital Albany NY) (capital NYC NY))) 

but note that these last two facts concern the relation not and or, not the relation 
capital. Thus, they will not be considered when we ask a query about capital. Fortunately, 
the assertion "Either NYC or Albany is the capital of NY" can be rephrased 
as two assertions: "Albany is the capital of NY if NYC is not" and "NYC is the capital 
of NY if Albany is not:" 


<a id='page-466'></a>

(<- (capital Albany NY) (not (capital NYC NY))) 

(<- (capital NYC NY) (not (capital Albany NY))) 

Unfortunately, Prolog's not is different from logic's not. When Prolog answers "no" 
to a query, it means the query cannot be proven from the known facts. If everything 
is known, then the query must be false, but if there are facts that are not known, the 
query may in fact be true. This is hardly surprising; we can't expect a program to 
come up with answers using knowledge it doesn't have. But in this case, it causes 
problems. Given the previous two clauses and the query (capi tal ?c NY), Prolog 
will go into an infinite loop. If we remove the first clause, Prolog would fail to prove 
that Albany is the capital, and hence conclude that NYC is. If we remove the second 
clause, the opposite conclusion would be drawn. 

The problem is that Prolog equates "not proven" with "false." Prolog makes what 
is called the closed world assumption—it assumes that it knows everything that is true. 
The closed world assumption is reasonable for most programs, because the programmer 
does know all the relevant information. But for knowledge representation in 
general, we would like a system that does not make the closed world assumption 
and has three ways to answer a query: "yes," "no," or "unknown." In this example, 
we would not be able to conclude that the capital of NY is or is not NYC, hence we 
would not be able to conclude anything about Albany. 

As another example, consider the clauses: 

(<- (damned) (do)) 

(<- (damned) (not (do))) 

With these rules, the query (? (damned)) should logically be answered "yes." 
Furthermore, it should be possible to conclude (damned) without even investigating 
if (do) is provable or not. What Prolog does is first try to prove (do). If this succeeds, 
then (damned) is proved. Either way, Prolog then tries again to prove (do), and this 
time if the proof fails, then (damned) is proved. So Prolog is doing the same proof 
twice, when it is unnecessary to do the proof at all. Introducing negation wrecks 
havoc on the simple Prolog evaluation scheme. It is no longer sufficient to consider 
a single clause at a time. Rather, multiple clauses must be considered together if we 
want to derive all the right answers. 

Robert Moore 1982 gives a good example of the power of disjunctive reasoning. 
His problem concerned three colored blocks, but we will update it to deal with three 
countries. Suppose that a certain Eastern European country, E, has just decided if it 
will remain under communist rule or become a democracy, but we do not know the 
outcome of the decision. . is situated between the democracy D and the communist 
country C: 

D E C 


<a id='page-467'></a>

The question is: Is there a communist country next to a democracy? Moore points 
out that the answer is "yes," but discovering this requires reasoning by cases. If . is 
a democracy then it is next to C and the answer is yes. But if . is communist then 
it is next toD and the answer is still yes. Since those are the only two possibilities, 
the answer must be yes in any case. Logical reasoning gives us the right answer, but 
Prolog can not. We can describe the problem with the following seven assertions 
and one query, but Prolog can not deal with the or in the final assertion. 

(<- (next-to D E)) (<- (next-to . D)) 
(<- (next-to . .) (<- (next-to C E)) 
(<- (democracy D)) (<- (communist O) 
(<- (or (democracy E) (communist E))) 

(?- (next-to ?A ?B) (democracy ?A) (communist ?B)) 

We have seen that Prolog is not very good at representing disjunctions and negations. 
It also has difficulty representing existentials. Consider the following statement in 
English, logic, and Prolog: 

Jan likes everyone. 

VX person(x) => likesQan,x) 

(<- (likes Jan ?x) (person ?x)) 

The Prolog translation is faithful. But there is no good translation for "Jan likes 
someone." The closest we can get is: 

Jan likes someone. 

3 X person(x) => likesQan,x) 

(<- (likes Jan pD) 
(<- (person pD) 

Here we have invented a new symbol, pi, to represent the unknown person that Jan 
likes, and have asserted that pi is a person. Notice that pi is a constant, not a variable. 
This use of a constant to represent a specific but unknown entity is called a Skolem 
constant, after the logician Thoralf Skolem (1887-1963). The intent is that pi may be 
equal to some other person that we know about. If we find out that Adrian is the 
person Jan likes, then in logic we can just add the assertion pi = Adrian. But that does 
not work in Prolog, because Prolog implicitly uses the unique name assumption—d\\ 
atoms represent distinct individuals. 

A Skolem constant is really just a special case of a Skolem function - an unknown 

entity that depends on one or more variable. For example, to represent "Everyone 

likes someone" we could use: 


<a id='page-468'></a>

Everyone likes someone. 

V 2/ 3 X person(3:) => likes (y, x) 

(<- (likes ?y (p2 ?y))) 
(<- (person (p2 ?y))) 

Here .2 is a Skolem function that depends on the variable ?y. In other words, 
everyone likes some person, but not necessarily the same person. 

14.5 Problems with Predicate Calculus's 
Expressiveness 
In the previous section we saw that Prolog has traded some expressiveness for 
efficiency. This section explores the limits of predicate calculus's expressiveness. 
Suppose we want to assert that lions, tigers, and bears are kinds of animals. In 
predicate calculus or in Prolog we could write an impHcation for each case: 

(<- (animal ?x) (lion ?x)) 
(<- (animal ?x) (tiger ?x)) 
(<- (animal ?x) (bear ?x)) 

These implications allow us to prove that any known lion, tiger, or bear is in fact 
an animal. However, they do not allow us to answer the question "What kinds of 
animals are there?" It is not hard to imagine extending Prolog so that the query 

(?- (<- (animal ?x) ?proposition)) 

would be legal. However, this happens not to be valid Prolog, and it is not even 
valid first-order predicate calculus (or FOPC). In FOPC the variables must range over 
constants in the language, not over relations or propositions. Higher-order predicate 
calculus removes this limitation, but it has a more complicated proof theory. 

It is not even clear what the values of ?propos i ti on should be in the query above. 
Surely (1 ion ?x) would be a valid answer, but so would (animal ?x), (or (tiger 
?x) (bea r ?x)), and an infinite number of other propositions. Perhaps we should 
have two types of queries, one that asks about "kinds," and another that asks about 
propositions. 

There are other questions that we might want to ask about relations. Just as it is 
useful to declare the types of parameters to a Lisp function, it can be useful to declare 
the types of the parameters of a relation, and later query those types. For example, 
we might say that the 1 i kes relation holds between a person and an object. 

In general, a sentence in the predicate calculus that uses a relation or sentence as 
a term is called a higher-order sentence. There are some quite subtle problems that 


<a id='page-469'></a>
come into play when we start to allow higher-order expressions. Allowing sentences 
in the calculus to talk about the truth of other sentences can lead to a paradox: is the 
sentence "This sentence is false" true or false? 

Predicate calculus is defined in terms of a universe of individuals and their 
properties and relations. Thus it is well suited for a model of the world that picks out 
individuals and categorizes them - a person here, a building there, a sidewalk between 
them. But how well does predicate calculus fare in a world of continuous substances? 
Consider a body of water consisting of an indefinite number of subconstituents that 
are all water, with some of the water evaporating into the air and rising to form clouds. 
It is not at all obvious how to define the individuals here. However, Patrick Hayes 
has shown that when the proper choices are made, predicate calculus can describe 
this kind of situation quite well. The details are in Hayes 1985. 

The need to define categories is a more difficult problem. Predicate calculus 
works very well for crisp, mathematical categories: . is a triangle if and only if . is 
a polygon with three sides. Unfortunately, most categories that humans deal with 
in everyday life are not defined so rigorously. The category friend refers to someone 
you have mostly positive feelings for, whom you can usually trust, and so on. This 
"definition" is not a set of necessary and sufficient conditions but rather is an open-
ended list of ill-defined qualities that are highly correlated with the category friend. 
We have a prototype for what an ideal friend should be, but no clear-cut boundaries 
that separate friend from, say, acquaintance. Furthermore, the boundaries seem to 
vary from one situation to another: a person you describe as a good friend in your 
work place might be only an acquaintance in the context of your home life. 

There are versions of predicate calculus that admit quantifiers like "most" in 
addition to "for all" and "there exists," and there have been attempts to define 
prototypes and measure distances from them. However, there is no consensus on 
the way to approach this problem. 

14.6 Problems with Completeness 
Because Prolog searches depth-first, it can get caught in one branch of the search 
space and never examine the other branches. This problem can show up, for example, 
in trying to define a commutative relation, like si bl i ng: 

(<- (sibling lee kim)) 
(<- (sibling ?x ?y) (sibling ?y ?x)) 

With these clauses, we expect to be able to conclude that Lee is Kim's sibling, and 
Kim is Lee's. Let's see what happens: 


<a id='page-470'></a>

> (?- (sibling ?x ?y)) 
?X = LEE 
?Y = KIM; 
?X = KIM 
?Y = LEE; 
?X = LEE 
?Y = KIM; 
?X = KIM 
?Y = LEE. 
No. 

We get the expected conclusions, but they are deduced repeatedly, because the 
commutative clause for siblings is applied over and over again. This is annoying, but 
not critical. Far worse is when we ask (? - (sibling fred ?x)). This query loops 
forever. Happily, this particular type of example has an easy fix: just introduce two 
predicates, one for data-base level facts, and one at the level of axioms and queries: 

(<- (sibling-fact lee kim)) 
(<- (sibling ?x ?y) (sibling-fact ?x ?y)) 
(<- (sibling ?x ?y) (sibling-fact ?y ?x)) 

Another fix would be to change the interpreter to fail when a repeated goal was detected. 
This was the approach taken in GPS. However, even if we eliminated repeated 
goals, Prolog can still get stuck in one branch of a depth-first search. Consider the 
example: 

(<- (natural 0)) 
(<- (natural (1-.- ?n)) (natural ?n)) 

These rules define the natural numbers (the non-negative integers). We can use 
the rules either to confirm queries like (natural (1+ (1->- (1-.- 0)))) or to generate 
the natural numbers, as in the query (natural ?n). So far, everything is fine. But 
suppose we wanted to define all the integers. One approach would be this: 

(<- (integer 0)) 
(<- (integer ?n) (integer (1+ ?n))) 
(<- (integer a+ ?n)) (integer ?n)) 

These rules say that 0 is an integer, and any . is an integer if . -f 1 is, and . -h 1 is 
if . is. While these rules are correct in a logical sense, they don't work as a Prolog 
program. Asking (integer x) will result in an endless series of ever-increasing 
queries: (integer (1+ x)), (integer (1+ (1+ and so on. Each goal is 
different, so no check can stop the recursion. 


<a id='page-471'></a>

The occurs check may or may not introduce problems into Prolog, depending on 
your interpretation of infinite trees. Most Prolog systems do not do the occurs check. 
The reasoning is that unifying a variable with some value is the Prolog equivalent of 
assigning a value to a variable, and programmers expect such a basic operation to be 
fast. With the occurs check turned off, it will in fact be fast. With checking on, it 
takes time proportional to the size of the value, which is deemed unacceptable. 

With occurs checking off, the programmer gets the benefit of fast unification but 
can run into problems with circular structures. Consider the following clauses: 

(<- (parent ?x (mother-of ?x))) 

(<- (parent ?x (father-of ?x))) 

These clauses say that, for any person, the mother of that person and the father of 
that person are parents of that person. Now let us ask if there is a person who is his 
or her own parent: 

> (? (parent ?y ?y)) 
?Y = [Abort] 

The system has found an answer, where ?y = (mother-of ?y). The answer can't be 
printed, though, because deref (or subst-bindings in the interpreter) goes into an 
infinite loop trying to figure out what ?y is. Without the printing, there would be no 
infinite loop: 

(<- (self-parent) (parent ?y ?y)) 

> (? (self-parent)) 

Yes; 

Yes; 

No. 

The sel f-parent query succeeds twice, once with the mother clause and once with 
the father clause. Has Prolog done the right thing here? It depends on your interpretation 
of infinite circular trees. If you accept them as valid objects, then the answer 
is consistent. If you don't, then leaving out the occurs check makes Prolog unsound: 
it can come up with incorrect answers. 

The same problem comes up if we ask if there are any sets that include themselves 

as members. The query (member ?set ?set) will succeed, but we will not be able to 

print the value of ?set. 


<a id='page-472'></a>

14.7 Problems with Efficiency: Indexing 
Our Prolog compiler is designed to handle "programlike" predicates - predicates 
with a small number of rules, perhaps with complex bodies. The compiler does 
much worse on "tablelike" predicates-predicates with a large number of simple 
facts. Consider the predicate pb, which encodes phone-book facts in the form: 

(pb (name Jan Doe) (num 415 555 1212)) 

Suppose we have a few thousand entries of this kind. A typical query for this data 
base would be: 

(pb (name Jan Doe) ?num) 

It would be inefficient to search through the facts linearly, matching each one against 
the query. It would also be inefficient to recompile the whole pb/2 predicate every 
time a new entry is added. But that is just what our compiler does. 

The solutions to the three problems - expressiveness, completeness, and index-
ing-will be considered in reverse order, so that the most difficult one, expressiveness, 
will come last. 

14.8 A Solution to the Indexing Problem 
A better solution to the phone-book problem is to index each phone-book entry in 
some kind of table that makes it easy to add, delete, and retrieve entries. That is what 
we will do in this section. We will develop an extension of the trie or discrimination 
tree data structure built in section 10.5 ([page 344](chapter10.md#page-344)). 

Making a discrimination tree for Prolog facts is complicated by the presence of 
variables in both the facts and the query. Either facts with variables in them will have 
to be indexed in several places, or queries with variables will have to look in several 
places, or both. We also have to decide if the discrimination tree itself will handle 
variable binding, or if it will just return candidate matches which are then checked by 
some other process. It is not clear what to store in the discrimination tree: copies of 
the fact, functions that can be passed continuations, or something else. More design 
choices will come up as we proceed. 

It is difficult to make design choices when we don't know exactly how the system 
will be used. We don't know what typical facts will look like, nor typical queries. 
Therefore, we will design a fairly abstract tool, forgetting for the moment that it will 
be used to index Prolog facts. 


<a id='page-473'></a>

We will address the problem of a discrimination tree where both the keys and 
queries are predicate structures with wild cards. A wild card is a variable, but with 
the understanding thatjhere is no variable binding; each instance of a variable can 
match anything. A predicate structure is a list whose first element is a nonvariable 
symbol. The discrimination tree supports three operations: 

* index &ndash; add a key/value pair to the tree 
* fetch &ndash; find all values that potentially match a given key 
* unindex &ndash; remove all key/value pairs that match a given key 
To appreciate the problems, we need an example. Suppose we have the following 
six keys to index. For simplicity, the value of each key will be the key itself: 

1 (p a b) 

2 (p a c) 

3 (p a ?x) 

4 (p b c) 

5 (p b (f c)) 

6 (p a (f . ?x)) 

Now assume the query (. ?y c). This should match keys 2, 3, and 4. How could 
we efficiently arrive at this set? One idea is to list the key/value pairs under every 
atom that they contain. Thus, all six would be listed under the atom p, while 2, 
4, and 5 would be listed under the atom c. A unification check could eliminate 5, 
but we still would be missing 3. Key 3 (and every key with a variable in it) could 
potentially contain the atom c. So to get the right answers under this approach, 
we will need to index every key that contains a variable under every atom - not an 
appealing situation. 

An alternative is to create indices based on both atoms and their position. So now 
we would be retrieving all the keys that have a c in the second argument position: 2 
and 4, plus the keys that have a variable as the second argument: 3. This approach 
seems to work much better, at least for the example shown. To create the index, we 
essentially superimpose the list structure of all the keys on top of each other, to arrive 
at one big discrimination tree. At each position in the tree, we create an index of the 
keys that have either an atom or a variable at that position. Figure 14.1 shows the 
discrimination tree for the six keys. 

Consider the query (. ?y c). Either the . or the c could be used as an index. 
The . in the predicate position retrieves all six keys. But the c in the second argument 
position retrieves only three keys: 2 and 4, which are indexed under c itself, and 3, 
which is indexed under the variable in that position. 

Now consider the query (. ?y (f ?z)). Again, the . serves as an index to all 
six keys. The f serves as an index to only three keys: the 5 and 6, which are indexed 


<a id='page-474'></a>

. A 
(.A .) (.A .) 
(PAC) (PAC) 
(PA?) (PA?) 
(PBC) (. A (F.?)) . 
(.8 (FC)) 
(. A (F.?)) 
. 
(PBC) 
(. A (F.?)) (. . (FC)) 
(. . (FC)) (. . (F C)) 
(PA(F.?) ) 
. 
(. . .) 
C 
(PAC) 
(PBC) 
? 
(PA?) 

Figure 14.1: Discrimination Tree with Six Keys 

directly under f in that position, and 3, which is indexed under the variable in a 
position along the path that lead to f. In general, all the keys indexed under variables 
along the path must be considered. 

The retrieval mechanism can overretrieve. Given the query (. a (f ?x)),the 
atom . will again retrieve all six keys, the atom a retrieves 1,2,3, and 6, and f again 
retrieves 5, 6, and 3. So f retrieves the shortest list, and hence it will be used to 
determine the final result. But key 5 is (. b (f c)), which does not match the query 
(pa (f?x)). 

We could eliminate this problem by intersecting all the lists instead of just taking 
the shortest list. It is perhaps feasible to do the intersection using bit vectors, but 
probably too slow and wasteful of space to do it using lists. Even if we did intersect 
keys, we would still overretrieve, for two reasons. First, we don't use . i 1 as an index, 
so we are ignoring the difference between (f ?x) and (f . ?x). Second, we are 
using wild-card semantics, so the query (. ?x ?x) would retrieve all six keys, when 


<a id='page-475'></a>
it should only retrieve three. Because of these problems, we make a design choice: 
we will first build a data base retrieval function that retrieves potential matches, and 
later worry about the unification process that will eliminate mismatches. 

We are ready for a more complete specification of the indexing strategy: 

* The value will be indexed under each non-nil nonvariable atom in the key, with 
a separate index for each position. For example, given the preceding data base, 
the atom a in the first argument position would index values 1,2,3, and 6, while 
the atom b in the second argument position would index value 4 and 5. The 
atom . in the predicate position would index all six values. 
In addition, we will maintain a separate index for variables at each position. For 
example, value 3 would be stored under the index "variable in second argument 
position." 

* "Position" does not refer solely to the linear position in the top-level list. For 
example, value 5 would be indexed under atom f in the caaddr position. 
* It follows that a key with . atoms will be indexed in. different ways. 
For retrieval, the strategy is: 

* For each non-nil nonvariable atom in the retrieval key, generate a list of possible 
matches. Choose the shortest such list. 
* Each list of possible matches will have to be augmented with the values indexed 
under a variable at every position "above." For example, f in the ca add r position 
retrieves value 5, but it also must retrieve value 3, because the third key has a 
variable in the caddr position, and caddr is "above" caaddr. 
* The discrimination tree may return values that are not valid matches. The 
purpose of the discrimination tree is to reduce the number of values we will 
have to unify against, not to determine the exact set of matches. 
It is important that the retrieval function execute quickly. If it is slow, we might 
just as well match against every key in the table linearly. Therefore, we will take 
care to implement each part efficiently. Note that we will have to compare the length 
of lists to choose the shortest possibility. Of course, it is trivial to compare lengths 
using length, but length requires traversing the whole list. We can do better if we 
store the length of the list explicitly. A list with its length will be called an nl1 st. 
It will be implemented as a cons cell containing the number of elements and a list 
of the elements themselves. An alternative would be to use extensible vectors with 
fill pointers. 


<a id='page-476'></a>

An nlist is implemented as a (count . elements) pair: 

(defun make-empty-nlist () 
"Create a new, empty nlist." 
(cons 0 nil)) 

(defun nlist-n (x) "The number of elements in an nlist." (carx)) 
(defun nlist-list (x) "The elements in an nlist." (cdr x)) 

(defun nlist-push (item nlist) 
"Add a new element to an nlist." 
(incf (car nlist)) 
(push item (cdr nlist)) 
nlist) 

Now we need a place to store these nlists. We will build the data base out of 
discrimination tree nodes called dtree nodes. Each dtree node has a field to hold 
the variable index, the atom indices, and pointers to two subnodes, one for the first 
and one for the rest. We implement dtrees as vectors for efficiency, and because we 
will never need a dtree-. predicate. 

(defstruct (dtree (:type vector)) 
(first nil) (rest nil) (atoms nil) (var (make-empty-nlist))) 

A separate dtree will be stored for each predicate. Since the predicates must be 
symbols, it is possible to store the dtrees on the predicate's property list. In most 
implementations, this will be faster than alternatives such as hash tables. 

(let ((predicates nil)) 

(defun get-dtree (predicate) 
"Fetch (or make) the dtree for this predicate." 
(cond ((get predicate 'dtree)) 

(t (push predicate predicates) 
(setf (get predicate 'dtree) (make-dtree))))) 

(defun clear-dtrees () 
"Remove all the dtrees for all the predicates." 
(dolist (predicate predicates) 

(setf (get predicate 'dtree) nil)) 
(setf predicates nil))) 

The function i ndex takes a relation as key and stores it in the dtree for the predicate 
of the relation. It calls dtree - i ndex to do all the work of storing a value under the 
proper indices for the key in the proper dtree node. 

The atom indices are stored in an association Ust. Property lists would not 
work, because they are searched using eq and atoms can be numbers, which are not 


<a id='page-477'></a>
necessarily eq. Association lists are searched using eql by default. An alternative 

would be to use hash tables for the index, or even to use a scheme that starts with 

association lists and switches to a hash table when the number of entries gets large. I 

use 1 ookup to look up the value of a key in a property list. This function, and its setf 

method, are defined on [page 896](chapter25.md#page-896). 

(defun index (key) 
"Store key in a dtree node. Key must be (predicate . args); 
it is stored in the predicate's dtree." 
(dtree-index key key (get-dtree (predicate key)))) 

(defun dtree-index (key value dtree) 
"Index value under all atoms of key in dtree." 
(cond 

((consp key) ; index on both first and rest 
(dtree-index (first key) value 
(or (dtree-first dtree) 
(setf (dtree-first dtree) (make-dtree)))) 
(dtree-index (rest key) value 
(or (dtree-rest dtree) 

(setf (dtree-rest dtree) (make-dtree))))) 
((null key)) ; don't index on nil 
((variable-p key) ; index a variable 

(nlist-push value (dtree-var dtree))) 
(t Make sure there is an nlist for this atom, and add to it 
(nlist-push value (lookup-atom key dtree))))) 

(defun lookup-atom (atom dtree) 
"Return (or create) the nlist for this atom in dtree." 
(or (lookup atom (dtree-atoms dtree)) 

(let ((new (make-empty-nlist))) 
(push (cons atom new) (dtree-atoms dtree)) 
new))) 

Now we define a function to test the indexing routine. Compare the output with 
figure 14.1. 

(defun test-index () 
(let ((props '((p a b) (p a c) (p a ?x) (p b c) 

(p b (f c)) (p a (f . ?x))))) 
(clear-dtrees) 
(mapc #*index props) 
(write (list props (get-dtree '.)) 

icircle t rarray t :pretty t) 
(values))) 


<a id='page-478'></a>

> (test-index) 

((#1=(P A B) 
#2=(P A C) 
#3=(P A ?X) 
#4=(P . C) 
#5=(P . (F O) 
#6=(P A (F . ?X))) 

#(#(NIL NIL (P (6 #6# #5# #4# #3# #2# #!#)) (0)) 
#(#(NIL NIL (B (2 #5# #4#) A (4 #6# #3# #2# #!#)) (0)) 
#(#(#(NIL NIL (F (2 #6# #5#)) (0)) 
#(#(NIL NIL (C (1 #5#)) (0)) 

#(NIL NIL NIL (0)) NIL (1 #6#)) 
(C (2 #4# #2#) . (1 #!#)) 
(1 #3#)) 

#(NIL NIL NIL (0)) 
NIL (0)) 
NIL (0)) 
NIL (0))) 

The next step is to fetch matches from the dtree data base. The function fetch takes 
a query, which must be a valid relation, as its argument, and returns a list of possible 
matches. It calls dtree-fetch to do the work: 

(defun fetch (query) 
"Return a list of buckets potentially matching the query, 
which must be a relation of form (predicate . args)." 
(dtree-fetch query (get-dtree (predicate query)) 

nil 0 nil most-positive-fixnum)) 

dtree-fetch must be passed the query and the dtree, of course, but it is also passed 
four additional arguments. First, we have to accumulate matches indexed under 
variables as we are searching through the dtree. So two arguments are used to pass 
the actual matches and a count of their total number. Second, we want dtree - fetch 
to return the shortest possible index, so we pass it the shortest answer found so far, 
and the size of the shortest answer. That way, as it is making its way down the tree, 
accumulating values indexed under variables, it can be continually comparing the 
size of the evolving answer with the best answer found so far. 

We could use nlists to pass around count/values pairs, but nlists only support a 
push operation, where one new item is added. We need to append together lists of 
values coming from the variable indices with values indexed under an atom. Append 
is expensive, so instead we make a list-of-lists and keep the count in a separate 
variable. When we are done, dtree-fetch and hence fetch does a multiple-value 
return, yielding the list-of-lists and the total count. 


<a id='page-479'></a>
There are four cases to consider in dtree-fetch. If the dtree is null or the query 
pattern is either null or a variable, then nothing will be indexed, so we should just 
return the best answer found so far. Otherwise, we bind var-. and var-1 ist to 
the count and list-of-lists of variable matches found so far, including at the current 
node. If the count var-. is greater than the best count so far, then there is no 
sense continuing, and we return the best answer found. Otherwise we look at the 
query pattern. If it is an atom, we use dtree-atom-f etch to return either the current 
index (along with the accumulated variable index) or the accumulated best answer, 
whichever is shorter. If the query is a cons, then we use dtree-fetch on the first 
part of the cons, yielding a new best answer, which is passed along to the call of 
dtree-fetch on the rest of the cons. 

(defun dtree-fetch (pat dtree var-list-in var-n-in best-list best-n) 
"Return two values: a list-of-lists of possible matches to pat. 
and the number of elements in the list-of-lists." 
(if (or (null dtree) (null pat) (variable-p pat)) 

(values best-list best-n) 

(let* ((var-nlist (dtree-var dtree)) 
(var-n (+ var-n-in (nlist-n var-nlist))) 
(var-list (if (null (nlist-list var-nlist)) 

var-1 ist-i . 
(cons (nlist-list var-nlist) 
var-list-in)))) 

(cond 
((>= var-n best-n) (values best-list best-n)) 
((atom pat) (dtree-atom-fetch pat dtree var-list var-n 

best-list best-n)) 
(t (multiple-value-bind (listl nl) 
(dtree-fetch (first pat) (dtree-first dtree) 
var-list var-n best-list best-n) 
(dtree-fetch (rest pat) (dtree-rest dtree) 
var-list var-n listl nl))))))) 

(defun dtree-atom-fetch (atom dtree var-list var-n best-list best-n) 
"Return the answers indexed at this atom (along with the vars), 
or return the previous best answer, if it is better." 
(let ((atom-nlist (lookup atom (dtree-atoms dtree)))) 

(cond 
((or (null atom-nlist) (null (nlist-list atom-nlist))) 
(values var-list var-n)) 
((and atom-nlist (< (incf var-n (nlist-n atom-nlist)) best-n)) 
(values (cons (nlist-list atom-nlist) var-list) var-n)) 
(t (values best-list best-n))))) 

Here we see a call to fetch on the data base created by test - i ndex. It returns two 
values: a list-of-lists of facts, and the total number of facts, three. 


<a id='page-480'></a>

> (fetch '(. ? c)) 
(((. . . (. A .) 
((. . ?.))) 
3 

Now let's stop and see what we have accomplished. The functions fetch and 
dtree-fetch fulfill their contract of returning potential matches. However, we still 
need to integrate the dtree facility with Prolog. We need to go through the potential 
matches and determine which candidates are actual matches. For simplicity we will 
use the version of u.i f y with binding lists defined in section 11.2. (It is also possible to 
construct a more efficient version that uses the compiler and the destructive function 
unifyl.) 

The function mapc- retri eve calls fetch to get a Ust-of-Usts of potential matches 
and then calls uni fy to see if the match is a true one. If the match is true, it calls 
the supplied function with the binding list that represents the unification as the 
argument, mapc-retri eve is proclaimed inl ine so that functions passed to it can 
also be compiled in place. 

(proclaim '(inline mapc-retrieve)) 

(defun mapc-retrieve (fn query) 
"For every fact that matches the query, 
apply the function to the binding list. " 
(dolist (bucket (fetch query)) 

(dolist (answer bucket) 
(let ((bindings (unify query answer))) 
(unless (eq bindings fail) 
(funcall fn bindings)))))) 

There are many ways to use this retriever. The function retri eve returns a list of the 
matching binding hsts, and retri eve-matches substitutes each binding hst into the 
original query so that the result is a list of expressions that unify with the query. 

(defun retrieve (query) 
"Find all facts that match query. Return a list of bindings." 
(let ((answers nil)) 

(mapc-retrieve #'(lambda (bindings) (push bindings answers)) 
query) 
answers)) 

(defun retrieve-matches (query) 
"Find all facts that match query. 
Return a list of expressions that match the query." 
(mapcar #'(lambda (bindings) (subst-bindings bindings query)) 

(retrieve query))) 


<a id='page-481'></a>
There is one further complication to consider. Recall that in our original Prolog 
interpreter, the function prove had to rename the variables in each clause as it 
retrieved it from the data base. This was to insure that there was no conflict between 
the variables in the query and the variables in the clause. We could do that in 
retrieve. However, if we assume that the expressions indexed in discrimination 
trees are tablelike rather than rulelike and thus are not recursive, then we can get 
away with renaming the variables only once, when they are entered into the data 
base. This is done by changing i ndex: 

(defun index (key) 
"Store key in a dtree node. Key must be (predicate . args); 
it is stored in the predicate's dtree." 
(dtree-index key (rename-variables key) ; store unique vars 

(get-dtree (predicate key)))) 

With the new i ndex in place, and after calling test - i ndex to rebuild the data base, 
we are now ready to test the retrieval mechanism: 

> (fetch '(p ?x c)) 
(((P . C) (P A O) 
((PA 7X3408))) 
3 

> (retrieve '(p ?x c)) 

(((7X3408 . C) (7X . A)) 
((7X . A)) 
((7X . B))) 

> (retrieve-matches '(p 7x c)) 

((P A C) (P A C) (P . .) 

> (retrieve-matches *(p 7x (7fn c))) 

((P A (7FN O) (P A (F O) (P . (F C))) 

Actually, it is better to use mapc-retrieve when possible, since it doesn't cons up 

answers the way retrieve and retrieve-matches do. The macro query-bind is 

provided as a nice interface to mapc - ret r i eve. The macro takes as arguments a list of 

variables to bind, a query, and one or more forms to apply to each retrieved answer. 

Within this list of forms, the variables will be bound to the values that satisfy the 

query. The syntax was chosen to be the same as mul ti pi e - va 1 ue - bi nd. Here we see 

a typical use of query - bi nd, its result, and its macro-expansion: 


<a id='page-482'></a>

> (query-bind (?x ?fn) '(p ?x (?fn c)) 

(format t "~&P holds between ~a and ~a of c." ?x ?fn)) =. 
. holds between . and F of c. 
. holds between A and F of c. 
. holds between A and ?FN of c. 
NIL 

= (mapc-retrieve 
#'(lambda (#:bindings6369) 
(let ((?x (subst-bindings #:bindings6369 '?.)) 
(?fn (subst-bindings #:bindings6369 '?fn))) 
(format t "~&P holds between ~a and ~a of c." ?x ?fn))) 
'(p ?x (?fn c))) 

Here is the implementation: 

(defmacro query-bind (variables query &body body) 
"Execute the body for each match to the query. 
Within the body, bind each variable." 
(let* ((bindings (gensym "BINDINGS")) 

(vars-and-vals 
(mapcar 
#'(lambda (var) 
(list var '(subst-bindings .bindings ',var))) 
variables))) 
'(mapc-retrieve 
#'(lambda (.bindings) 
(let ,vars-and-vals 
.body)) 
.query))) 

14.9 A Solution to the Completeness Problem 
We saw in chapter 6 that iterative deepening is an efficient way to cover a search 
space without falling into an infinite loop. Iterative deepening can also be used to 
guide the search in Prolog. It will insiu-e that all valid answers are found eventually, 
but it won't turn an infinite search space into a finite one. 

In the interpreter, iterative deepening is implemented by passing an extra argument 
to prove and prove-a 11 to indicate the depth remaining to be searched. When 
that argument is zero, the search is cut off, and the proof fails. On the next iteration 
the bounds will be increased and the proof may succeed. If the search is never cut off 
by a depth bound, then there is no reason to go on to the next iteration, because all 


<a id='page-483'></a>
proofs have already been found. The special variable *sea r ch - cut - off* keeps track 
of this. 

(defvar *search-cut-off* nil "Has the search been stopped?") 

(defun prove-all (goals bindings depth) 
"Find a solution to the conjunction of goals." 
This version just passes the depth on to PROVE, 

(cond ((eq bindings fail) fail) 
((null goals) bindings) 
(t (prove (first goals) bindings (rest goals) depth)))) 

(defun prove (goal bindings other-goals depth) 
"Return a list of possible solutions to goal." 
:; Check if the depth bound has been exceeded 
(if (= depth 0) 

(progn (setf *search-cut-off* t) 
fail) 
(let ((clauses (get-clauses (predicate goal)))) 
(if (listp clauses) 
(some 
#'(lambda (clause) 
(let ((new-clause (rename-variables clause))) 

(prove-al1 
(append (clause-body new-clause) other-goals) 
(unify goal (clause-head new-clause) bindings) 
(- depth 1)))) 

clauses) 

The predicate's "clauses" can be an atom: 
;; a primitive function to call 
(funcall clauses (rest goal) bindings 

other-goals depth))))) 

prove and . rove - a 11 now implement search cutoff, but we need something to control 
the iterative deepening of the search. First we define parameters to control the 
iteration: one for the initial depth, one for the maximum depth, and one for the 
increment between iterations. Setting the initial and increment values to one will 
make the results come out in strict breadth-first order, but will duplicate more effort 
than a slightly larger value. 


<a id='page-484'></a>

(defparameter *depth-start* 5 
"The depth of the first round of iterative search.") 
(defparameter *depth-incr* 5 
"Increase each iteration of the search by this amount.") 
(defparameter *depth-max* most-positive-fixnum 
"The deepest we will ever search.") 

A new version of top-level - prove will be used to control the iteration. It calls 
prove-al 1 for all depths from the starting depth to the maximum depth, increasing 
by the increment. However, it only proceeds to the next iteration if the search was 
cut off at some point in the previous iteration. 

(defun top-level-prove (goals) 
(let ((all-goals 
*(,goals (show-prolog-vars ,@(variables-in goals))))) 
(loop for depth from *depth-start* to *depth-max* by *depth-incr* 

while (let ((*search-cut-off* nil)) 
(prove-all all-goals no-bindings depth) 
*search-cut-off*))) 

(format t "~&No.") 
(values)) 

There is one final complication. When we increase the depth of search, we may 
find some new proofs, but we will also find all the old proofs that were found on the 
previous iteration. We can modify show-prol og-vars to only print proofs that are 
found with a depth less than the increment - that is, those that were not found on the 
previous iteration. 

(defun show-prolog-vars (vars bindings other-goals depth) 
"Print each variable with its binding. 
Then ask the user if more solutions are desired." 
(if (> depth *depth-incr*) 

fail 
(progn 

(if (null vars) 
(format t "~&Yes") 
(dolist (var vars) 

(format t "~&~a = ~a" var 
(subst-bindings bindings var)))) 

(if (continue-p) 
fail 
(prove-all other-goals bindings depth))))) 

To test that this works, try setting *depth-max* to 5 and running the following 
assertions and query. The infinite loop is avoided, and the first four solutions 
are found. 


<a id='page-485'></a>
(<- (natural 0)) 
(<- (natural (1+ ?n)) (natural ?n)) 

> (?- (natural ?n)) 

?N = 0; 

?N = (1+ 0); 

?N = (1+ (1+ 0)); 

?N = (1+ (1+ (1+ 0))); 

No. 

14.10 Solutions to the Expressiveness Problems 
In this section we present solutions to three of the limitations described above: 

* Treatment of (limited) higher-order predications. 
* Introduction of a frame-based syntax. 
* Support for possible worlds, negation, and disjunction. 
We also introduce a way to attach functions to predicates to do forward-chaining 

and error detection, and we discuss ways to extend unification to handle Skolem 

constants and other problems. 

Higher-Order Predications 

First we will tackle the problem of answering questions like "What kinds of animals 
are there?" Paradoxically, the key to allowing more expressiveness in this case is to 
invent a new, more limited language and insist that all assertions and queries are 
made in that language. That way, queries that would have been higher-order in the 
original language become first-order in the restricted language. 

The language admits three types of objects: categones, relations, and individuals. 
A category corresponds to a one-place predicate, a relation to a two-place predicate, 
and an individual to constant, or zero-place predicate. Statements in the language 
musthaveoneof five primitive operators: sub, rel, ind. val , and and. They have 
the following form: 

(sub subcategorysupercategory) 
(rel relation domain-category range-category) 
(i nd individual category) 
(val relation individual value) 
(and assertion...) 


<a id='page-486'></a>

The following table gives some examples, along with English translations: 

(sub dog animal) Dog is a kind of animal. 
(rel birthday animal date) The birthday relation holds between each animal 
and some date. 
(ind fido dog) The individual Fido is categorized as a dog. 
(val birthday fido july-1) The birthday of Fido is July-1. 
(and AB) Both A and Bare true. 
For those who feel more comfortable with predicate calculus, the following table 
gives the formal definition of each primitive. The most complicated definition is for 
rel. The form (rel RAB) means that every R holds between an individual of A 
and an individual of B, and furthermore that every individual of A participates in at 

least one R relation. 
(sub AB) Va:: A(x) D 
(rel RAB) "rfx^y: R{x,y) D A{x) A B{y) 
A\/xA{x) D 3y : R{x, y) 
(ind IC) C{I) 
(val RIV) R{I,V) 
(and PQ...) PAQ.,. 

Queries in the language, not surprisingly, have the same form as assertions, 
except that they may contain variables as well as constants. Thus, to find out what 
kinds of animals there are, use the query (sub ?kind animal). To find out what 
individual animals there are, use the query (ind ?x animal). To find out what 
individual animals of what kinds there are, use: 

(and (sub ?kind animal) (ind ?x ?kind)) 

The implemention of this new language can be based directly on the previous implementation 
of dtrees. Each assertion is stored as a fact in a dtree, except that 
the components of an and assertion are stored separately. The function add-fact 
does this: 

(defun add-fact (fact) 

"Add the fact to the data base." 

(if (eq (predicate fact) 'and) 

(mapc #*add-fact (args fact)) 
(index fact))) 

Querying this new data base consists of querying the dtree just as before, but with 
a special case for conjunctive (and) queries. Conceptually, the function to do this, 
retri eve-fact, should be as simple as the following: 


<a id='page-487'></a>
(defun retrieve-fact (query) 
"Find all facts that match query. Return a list of bindings. 
Warning!! this version is incomplete." 
(if (eq (predicate query) 'and) 

(retrieve-conjunction (args query)) 
(retrieve query bindings))) 

Unfortunately, there are some complications. Think about what must be done in 
retrieve-conjunction. It is passed a list of conjuncts and must return a list of 
binding lists, where each binding list satisfies the query. For example, to find out 
what people were born on July 1st, we could use the query: 

(and (val birthday ?p july-1) (ind ?p person)) 

retrieve-conjunction could solve this problem by first calling retrieve-fact on 
(val birthday ?p july-1). Once that is done, there is only one conjunct remaining, 
but in general there could be several, so we need to call ret r i eve - conj uncti on recursively 
with two arguments: theremainingconjuncts,andtheresultthat retrieve-fact 
gave for the first solution. Since retrieve-fact returns a list of binding lists, it will 
be easiest if retri eve-conjunct i on accepts such a list as its second argument. Furthermore, 
when it comes time to call retri eve- fact on the second conjunct, we will 
want to respect the bindings set up by the first conjunct. So retri eve -fact must 
accept a binding list as its second argument. Thus we have: 

(defun retrieve-fact (query &optional (bindings no-bindings)) 
"Find all facts that match query. Return a list of bindings." 
(if (eq (predicate query) 'and) 

(retrieve-conjunction (args query) (list bindings)) 
(retrieve query bindings))) 

(defun retrieve-conjunction (conjuncts bindings-lists) 
"Return a list of binding lists satisfying the conjuncts." 
(mapcan 

#'(lambda (bindings) 

(cond ((eq bindings fail) nil) 
((null conjuncts) (list bindings)) 
(t (retrieve-conjunction 

(rest conjuncts) 

(retrieve-fact 
(subst-bindings bindings (first conjuncts)) 
bindings))))) 

bindings-lists)) 

Notice that retrieve and therefore mapc-retrieve now also must accept a binding 
list. The changes to them are shown in the following. In each case the extra argument 


<a id='page-488'></a>

is made optional so that previously written functions that call these functions without 
passing in the extra argument will still work. 

(defun mapc-retrieve (fn query &optional (bindings no-bindings)) 
"For every fact that matches the query, 
apply the function to the binding list. " 
(dolist (bucket (fetch query)) 

(dolist (answer bucket) 
(let ((new-bindings (unify query answer bindings))) 
(unless (eq new-bindings fail) 
(funcall fn new-bindings)))))) 

(defun retrieve (query &optional (bindings no-bindings)) 
"Find all facts that match query. Return a list of bindings." 
(let ((answers nil)) 

(mapc-retrieve #'(lambda (bindings) (push bindings ansviers)) 
query bindings) 
answers)) 

Now add - fact and ret r i eve - fact comprise all we need to implement the language. 
Here is a short example where add-fact is used to add facts about bears and dogs, 
both as individuals and as species: 

> (add-fact *(sub dog animal)) => . 
> (add-fact '(sub bear animal)) => . 
> (add-fact '(ind Fido dog)) => . 
> (add-fact '(ind Yogi bear)) . 
> (add-fact '(val color Yogi brown)) => . 
> (add-fact '(val color Fido golden)) . 
> (add-fact '(val latin-name bear ursidae)) => . 
> (add-fact '(val latin-name dog canis-familiaris)) => . 

Now retrieve -fact is used to answer three questions: What kinds of animals are 
there? What are the Latin names of each kind of animal? and What are the colors of 
each individual bear? 

> (retrieve-fact '(sub ?kind animal)) 
(((?KIND . DOG)) 
((?KIND . BEAR))) 

> (retrieve-fact '(and (sub ?kind animal) 
(val latin-name ?kind ?latin))) 
(((7LATIN . CANIS-FAMILIARIS) (7KIND . DOG)) 
((7LATIN . URSIDAE) (7KIND . BEAR))) 


<a id='page-489'></a>
> (retrieve-fact '(and (ind ?x bear) (val color ?x ?c))) 

(((?C . BROWN) (?X . YOGI))) 

Improvements 

There are quite a few improvements that can be made to this system. One direction 
is to provide different kinds of answers to queries. The following two functions 
are similar to retri eve-matches in that they return lists of solutions that match the 
query, rather than lists of possible bindings: 

(defun retrieve-bagof (query) 

"Find all facts that match query. 

Return a list of queries with bindings filled in." 

(mapcar #'(lambda (bindings) (subst-bindings bindings query)) 

(retrieve-fact query))) 

(defun retrieve-setof (query) 
"Find all facts that match query. 
Return a list of unique queries with bindings filled in. " 
(remove-duplicates (retrieve-bagof query) :test #'equal)) 

Another direction to take is to provide better error checking. The current system 
does not complain if a fact or query is ill-formed. It also relies on the user to input all 
facts, even those that could be derived automatically from the semantics of existing 
facts. Forexample, the semantics of sub imply that if (sub bear animal) and (sub 
polar-bear bear) are true, then (subpolar-bear animal) must also be true. This 
kind of implication can be handled in two ways. The typical Prolog approach would 
be to write rules that derive the additional sub facts by backward-chaining. Then 
every query would have to check if there were rules to run. The alternative is to use 
aforward-chaining approach, which caches each new sub fact by adding it to the data 
base. This latter alternative takes more storage, but because it avoids rederiving the 
same facts over and over again, it tends to be faster. 

The following version of add-fact does error checking, and it automatically 
caches facts that can be derived from existing facts. Both of these things are done by 
a set of functions that are attached to the primitive operators. It is done in a data-
driven style to make it easier to add new primitives, should that become necessary. 

The function add-fact checks that each argument to a primitive relation is a 
nonvariable atom, and it also calls fact-present-p to check if the fact is already 
present in the data base. If not, it indexes the fact and calls run-attached-f . to do 
additional checking and caching: 

(defparameter ^primitives* '(and sub ind rel val)) 


<a id='page-490'></a>

(defun add-fact (fact) 
"Add the fact to the data base." 
(cond ((eq (predicate fact) *and) 

(mapc #*add-fact (args fact))) 

((or (not (every #*atom (args fact))) 
(some #'variable-p (args fact)) 
(not (member (predicate fact) *primitives*))) 

(error "111-formed fact: ~a" fact)) 

((not (fact-present-p fact)) 
(index fact) 
(run-attached-fn fact))) 

t) 

(defun fact-present-p (fact) 
"Is this fact present in the data base?" 
(retrieve fact)) 

The attached functions are stored on the operator's property list under the indicator 

attached-fn: 

(defun run-attached-fn (fact) 
"Run the function associated with the predicate of this fact." 
(apply (get (predicate fact) 'attached-fn) (args fact))) 

(defmacro def-attached-fn (pred args &body body) 
"Define the attached function for a primitive." 
'(setf (get '.pred 'attached-fn) 

#'(lambda ,args ..body))) 

The attached functions for ind and val are fairly simple. If we know (sub bear 
ani mal), then when ( i nd Yogi bea r) is asserted, we have to also assert ( i nd Yogi 
animal). Similarly, the values in a val assertion must be individuals of the categories 
in the relation's rel assertion. That is, if ( rel bi rthday animal date) is a fact and 
(val birthday Lee ju1y-l) is added, then we can conclude (ind Lee animal) and 
(ind july-1 date). The followingfunctions add the appropriate facts: 

(def-attached-fn ind (individual category) 
Cache facts about inherited categories 
(query-bind (?super) '(sub .category ?super) 
(add-fact '(ind .individual .?super)))) 


<a id='page-491'></a>

(def-attached-fn val (relation indi ind2) 
Make sure the individuals are the right kinds 

(query-bind (?catl ?cat2) '(rel .relation ?catl ?cat2) 
(add-fact *(ind ,indl .?catl)) 
(add-fact '(ind .ind2 .?cat2)))) 

The attached function for rel simply runs the attached function for any individual of 
the given relation. Normally one would make all rel assertions before i nd assertions, 
so this will have no effect at all. But we want to be sure the data base stays consistent 
even if facts are asserted in an unusual order. 

(def-attached-fn rel (relation catl cat2) 
Run attached function for any IND's of this relation 
(query-bind (?a ?b) '(ind .relation ?a ?b) 
(run-attached-fn '(ind .relation .?a .?b)))) 

The most complicated attached function is for sub. Adding a fact such as (sub bear 
animal) causes the following to happen: 

* All of animal's supercategories (such as 1 iving-thing) become supercategories 
of all of bea r's subcategories (such as pol ar - bea r). 
* animal itself becomes a supercategory all of bear's subcategories. 
* bear itself becomes a subcategory of all of animal's supercategories. 
* All of the individuals of bear become individuals of animal and its supercategories. 
The following accomplishes these four tasks. It does it with four calls to 
index-new-fact, which is used instead of add-fact because we don't need to run 
the attached function on the new facts. We do, however, need to make sure that we 
aren't indexing the same fact twice. 

(def-attached-fn sub (subcat supercat) 
Cache SUB facts 

(query-bind (?super-super) '(sub .supercat ?super-super) 
(index-new-fact '(sub .subcat .?super-super)) 
(query-bind (?sub-sub) '(sub ?sub-sub .subcat) 

(index-new-fact '(sub .?sub-sub .?super-super)))) 
(query-bind (?sub-sub) '(sub ?sub-sub .subcat) 
(index-new-fact '(sub .?sub-sub .supercat))) 
Cache IND facts 
(query-bind (?super-super) '(sub .subcat ?super-super) 
(query-bind (?sub-sub) '(sub ?sub-sub .supercat) 
(query-bind (?ind) '(ind ?ind .?sub-sub) 
(index-new-fact '(ind .?ind .?super-super)))))) 


<a id='page-492'></a>

(defun index-new-fact (fact) 

"Index the fact in the data base unless it is already there." 

(unless (fact-present-p fact) 

(index fact))) 

The following function tests the attached functions. It shows that adding the single 
fact (sub bea r ani mal) to the given data base causes 18 new facts to be added. 

(defun test-bears () 

(clear-dtrees) 

(mapc #'add-fact 

'((sub animal living-thing) 

(sub living-thing thing) (sub polar-bear bear) 

(sub grizzly bear) (ind Yogi bear) (ind Lars polar-bear) 

(ind Helga grizzly))) 

(trace index) 

(add-fact '(sub bear animal)) 

(untrace index)) 

> (test-bears) 

(1 ENTER INDEX: (SUB BEAR ANIMAL)) 

(1 EXIT INDEX: T) 

(1 ENTER INDEX: (SUB BEAR THING)) 

(1 EXIT INDEX: T) 

(1 ENTER INDEX: (SUB GRIZZLY THING)) 

(1 EXIT INDEX: T) 

(1 ENTER INDEX: (SUB POLAR-BEAR THING)) 

(1 EXIT INDEX: T) 

(1 ENTER INDEX: (SUB BEAR LIVING-THING)) 

(1 EXIT INDEX: T) 

(1 ENTER INDEX: (SUB GRIZZLY LIVING-THING)) 

(1 EXIT INDEX: T) 

(1 ENTER INDEX: (SUB POLAR-BEAR LIVING-THING)) 

(1 EXIT INDEX: T) 

(1 ENTER INDEX: (SUB GRIZZLY ANIMAL)) 

(1 EXIT INDEX: T) 

(1 ENTER INDEX: (SUB POLAR-BEAR ANIMAL)) 

(1 EXIT INDEX: T) 

(1 ENTER INDEX: (IND LARS LIVING-THING)) 

(1 EXIT INDEX: T) 

(1 ENTER INDEX: (IND HELGA LIVING-THING)) 

(1 EXIT INDEX: T) 

(1 ENTER INDEX: (IND YOGI LIVING-THING)) 

(1 EXIT INDEX: T) 

(1 ENTER INDEX: (IND LARS THING)) 

(1 EXIT INDEX: T) 

(1 ENTER INDEX: (IND HELGA THING)) 


<a id='page-493'></a>
(1 EXIT INDEX: T) 
(1 ENTER INDEX: (IND YOGI THING)) 
(1 EXIT INDEX: T) 
(1 ENTER INDEX: (IND LARS ANIMAD) 
(1 EXIT INDEX: .) 
(1 ENTER INDEX: (IND HELGA ANIMAD) 
(1 EXIT INDEX: .) 
(1 ENTER INDEX: (IND YOGI ANIMAD) 
(1 EXIT INDEX: .) 
(INDEX) 

A Frame Language 

Another direction we can take is to provide an alternative syntax that will be easier 
to read and write. Many representation languages are based on the idea of frames, 
and their syntax reflects this. A frame is an object with slots. We will continue to use 
the same data base in the same format, but we will provide an alternative syntax that 
considers the individuals and categories as frames, and the relations as slots. 

Here is an example of the frame syntax for individuals, which uses the operator 

a.Note that it is more compact than the equivalent notation using the primitives. 
(a person (name Joe) (age 27)) = 

(and (ind personl person) 
(val name personl Joe) 
(val age personl 27)) 

The syntax also allows for nested expressions to appear as the values of slots. Notice 
that the Skolem constant personl was generated automatically; an alternative is 
to supply a constant for the individual after the category name. For example, the 
following says that Joe is a person of age 27 whose best friend is a person named Fran 
who is 28 and whose best friend is Joe: 

(a person pi (name Joe) (age 27) 
(best-friend (a person (name Fran) (age 28) 
(best-friend pi)))) = 

(and (ind pi person) (val name pi joe) (val age pi 27) 
(ind person2 person) (val name person2 fran) 
(val age person2 28) (val best-friend person2 pi) 
(val best-friend pi person2)) 


<a id='page-494'></a>

The frame syntax for categories uses the operator each. For example: 

(each person (isa animal) (name person-name) (age integer)) = 

(and (sub person animal) 
(rel name person person-name) 
(rel age person integer)) 

The syntax for queries is the same as for assertions, except that variables are used 
instead of the Skolem constants. This is true even when the Skolem constants are 
automatically generated, as in the following query: 

(a person (age 27)) = (AND (IND ?3 PERSON) (VAL AGE ?3 27)) 

To support the frame notation, we define the macros a and each to make assertions 
and ?? to make queries. 

(defmacro a (&rest args) 
"Define a new individual and assert facts about it in the data base." 
*(add-fact \(translate-exp (cons *a args)))) 

(defmacro each (&rest args) 
"Define a new category and assert facts about it in the data base." 
'(add-fact (translate-exp (cons 'each args)))) 

(defmacro ?? (&rest queries) 
"Return a list of answers satisfying the query or queries." 
*(retrieve-setof 

'.(translate-exp (maybe-add 'and (replace-?-vars queries)) 
rquery))) 

All three of these macros call on trans! ate - exp to translate from the frame syntax to 
the primitive syntax. Note that an a or ea ch expression is computing a conjunction of 
primitive relations, but it is also computing a term when it is used as the nested value 
of a slot. It would be possible to do this by returning multiple values, but it is easier to 
build translate - exp as a set of local functions that construct facts and push them on 
the local variable conj uncts. At the end, the list of conj uncts is returned as the value 
of the translation. The local functions trans! ate-a and trans! ate-each return the 
atom that represents the term they are translating. The local function translate 
translates any kind of expression, trans! ate -s! ot handles a slot, and co!! ect- f act 
is responsible for pushing a fact onto the list of conjuncts. The optional argument 
query-mode-p tells what to do if the individual is not provided in an a expression. If 
query-mode-p is true, the individual will be represented by a variable; otherwise it 
will be a Skolem constant. 


<a id='page-495'></a>
(defun translate-exp (exp &optional query-mode-p) 
"Translate exp into a conjunction of the four primitives." 
(let ((conjuncts nil)) 

(labels 
((collect-fact (&rest terms) (push terms conjuncts)) 

(translate (exp) 
Figure out what kind of expression this is 

(cond 
((atom exp) exp) 
((eq (first exp) *a) (translate-a (rest exp))) 
((eq (first exp) 'each) (translate-each (rest exp))) 
(t (apply #'collect-fact exp) exp))) 

(translate-a (args) 
translate (A category Cind] (rel filler)*) 
(let* ((category (pop args)) 
(self (cond ((and args (atom (first args))) 

(pop args)) 
(query-mode-p (gentemp "?")) 
(t (gentemp (string category)))))) 

(collect-fact 'ind self category) 
(dolist (slot args) 
(translate-slot 'val self slot)) 
self)) 

(translate-each (args) 
;; translate (EACH category [(isa cat*)] (slot cat)*) 
(let* ((category (pop args))) 

(when (eq (predicate (first args)) 'isa) 
(dolist (super (rest (pop args))) 
(collect-fact 'sub category super))) 
(dolist (slot args) 
(translate-slot 'rel category slot)) 
category)) 

(translate-slot (primitive self slot) 

translate (relation value) into a REL or SUB 
(assert (= (length slot) 2)) 
(collect-fact primitive (first slot) self 

(translate (second slot))))) 

Body of translate-exp: 
(translate exp) Build up the list of conjuncts 
(maybe-add 'and (nreverse conjuncts))))) 


<a id='page-496'></a>

The auxiliary functions maybe - add and repl ace -? - va r s are shown in the following: 

(defun maybe-add (op exps &optional if-nil) 
"For example, (maybe-add 'and exps t) returns 
t if exps is nil, (first exps) if there is only one. 
and (and expl exp2...) if there are several exps." 
(cond ((null exps) if-nil) 

((length=1 exps) (first exps)) 
(t (cons op exps)))) 

(defun length=1 (x) 
"Is X a list of length 1?" 
(and (consp x) (null (cdr x)))) 

(defun replace-?-vars (exp) 
"Replace each ? in exp with a temporary var: 7123" 
(cond ((eq exp '7) (gentemp "7")) 

((atom exp) exp) 

(t (reuse-cons (replace-7-vars (first exp)) 
(replace-7-vars (rest exp)) 
exp)))) 

Possible Worlds: Truth, Negation, and Disjunction 

In this section we address four problems: distinguishing unknown from f al se, representing 
negations, representing disjunctions, and representing multiple possible 
states of affairs. It turns out that all four problems can be solved by introducing 
two new techniques: possible worlds and negated predicates. The solution is not 
completely general, but it is practical in a wide variety of applications. 

There are two basic ways to distinguish unknown from false. The first possibility 
is to store a truth value - true or false - along with each proposition. The second 
possibility is to include the truth value as part of the proposition. There are several 
syntactic variations on this theme. The following table shows the possibilities for 
the propositions "Jan likes Dean is true" and "Jan likes Ian is false:" 

Approach True Prop. False Prop. 
(1) 
(2a) 
(likes(likes 
Jan Dean) 
true Jan Dean) 
-true 
(likes(likes 
Jan Ian) -false 
false Jan Ian) 
{2b) (likes Jan Dean) (not (likes Jan Dean)) 
(2c) (likes Jan Dean) (~likes Jan Dean) 

The difference between (1) and (2) shows up when we want to make a query. 
With (1), we make the single query (1 i kes JanDean) (or perhaps (1 i kes Jan ?x)), 
and the answers will tell us who Jan does and does not like. With (2), we make one 


<a id='page-497'></a>
query to find out what liking relationships are true, and another to find out which 
ones are false. In either approach, if there are no responses then the answer is truly 
unknown. 

Approach (1) is better for applications where most queries are of the form "Is 
this sentence true or false?" But applications that include backward-chaining rules 
are not like this. The typical backward-chaining rule says "Conclude X is true ifY is 
true." Thus, most queries will be of the type "Is Y true?" Therefore, some version of 
approach (2) is preferred. 

Representing true and false opens the door to a host of possible extensions. First, 
we could add multiple truth values beyond the simple "true" and "false." These 
could be symbolic values like "probably-true" or "false-by-default" or they could be 
numeric values representing probabilities or certainty factors. 

Second, we could introduce the idea of possible worlds. That is, the truth of a 
proposition could be unknown in the current world, but true if we assume p, and 
false if we assume q. In the possible world approach, this is handled by calling the 
current world W, and then creating a new world VFi, which is just like W except 
that . is true, and w2, which is just like W except that q is true. By doing reasoning 
in different worlds we can make predictions about the future, resolve ambiguitites 
about the current state, and do reasoning by cases. 

For example, possible worlds allow us to solve Moore's communism/democracy 
problem ([page 466](chapter14.md#page-466)). We create two new possible worlds, one where is a democracy 
and one where it is communist. In each world it is easy to derive that there is 
a democracy next to a communist country. The trick is to realize then that the 
two worlds form a partition, and that therefore the assertion holds in the original 
"real" world as well. This requires an interaction between the Prolog-based tactical 
reasoning going on within a world and the planning-based strategic reasoning that 
decides which worlds to consider. 

We could also add a truth maintenance system (or TMS) to keep track of the assumptions 
or justifications that lead to each fact being considered true. A truth 
maintenance system can lessen the need to backtrack in a search for a global solution. 
Although truth maintenance systems are an important part of AI programming, 
they will not be covered in this book. 

In this section we extend the dtree facility (section 14.8) to handle truth values 
and possible worlds. With so many options, it is difficult to make design choices. We 
will choose a fairly simple system, one that remains close to the simplicity and speed 
of Prolog but offers additional functionality when needed. We will adopt approach 
(2c) to truth values, using negated predicates. For example, the negated predicate of 
1 i kes is ~1 i kes, which is pronounced "not likes." 

We will also provide minimal support for possible worlds. Assume that there is 
always a current world, W, and that there is a way to create alternative worlds and 
change the current world to an alternative one. Assertions and queries will always be 
made with respect to the current world. Each fact is indexed by the atoms it contains. 


<a id='page-498'></a>

just as before. The difference is that the facts are also indexed by the current world. 
To support this, we need to modify the notion of the numbered list, or nlist, to 
include a numbered association list, or nal i st. The following is an nal i st showing 
six facts indexed under three different worlds: WO, Wl, and W2: 

(6 (WO #1# #2# #3#) (Wl #4#) (W2 #5# #6#)) 

The fetching routine will remain unchanged, but the postfetch processing will have 
to sort through the nalists to find only the facts in the current world. It would also be 
possible for fetch to do this work, but the reasoning is that most facts will be indexed 
under the "real world," and only a few facts will exist in alternative, hypothetical 
worlds. Therefore, we should delay the effort of sorting through the answers to 
eliminate those answers in the wrong world - it may be that the first answer fetched 
will suffice, and then it would have been a waste to go through and eliminate other 
answers. The following changes to i ndex and dtree -i ndex add support for worlds: 

(defvar *world* *W0 "The current world used by index and fetch.") 

(defun index (key &optional (world *world*)) 
"Store key in a dtree node. Key must be (predicate . args); 
it is stored in the dtree, indexed by the world." 
(dtree-index key key world (get-dtree (predicate key)))) 

(defun dtree-index (key value world dtree) 
"Index value under all atoms of key in dtree." 
(cond 

((consp key) ; index on both first and rest 
(dtree-index (first key) value world 
(or (dtree-first dtree) 
(setf (dtree-first dtree) (make-dtree)))) 
(dtree-index (rest key) value world 
(or (dtree-rest dtree) 
(setf (dtree-rest dtree) (make-dtree))))) 
((null key)) ; don't index on nil 

((variable-p key) ; index a variable 
(nalist-push world value (dtree-var dtree))) 
(t ;; Make sure there is an nlist for this atom, and add to it 
(nalist-push world value (lookup-atom key dtree))))) 

The new function nalist-push adds a value to an nalist, either by inserting the value 
in an existing key's list or by adding a new key/value list: 


<a id='page-499'></a>
(defun nalist-push (key val nalist) 
"Index val under key in a numbered al ist. " 
;; An nalist is of the form (count (key val*)*) 

Ex: (6 (nums 1 2 3) (letters a b c)) 
(incf (car nalist)) 
(let ((pair (assoc key (cdr nalist)))) 

(if pair 
(push val (cdr pair)) 
(push (list key val) (cdr nalist))))) 

In the following, fetch is used on the same data base created by tes t -i ndex, indexed 
under the world WO. This time the result is a list-of-lists of world/values a-lists. The 
count, 3, is the same as before. 

> (fetch '(p ?x c)) 
(((WO (P . C) (P A C))) 
((WO (P A ?X)))) 
3 

So far, worlds have been represented as symbols, with the implication that different 
symbols represent completely distinct worlds. That doesn't make worlds very easy 
to use. We would like to be able to use worlds to explore alternatives - create a 
new hypothetical world, make some assumptions (by asserting them as facts in the 
hypothetical world), and see what can be derived in that world. It would be tedious 
to have to copy all the facts from the real world into each hypothetical world. 

An alternative is to establish an inheritance hierarchy among worlds. Then a fact 

is considered true if it is indexed in the current world or in any world that the current 

world inherits from. 

To support inheritance, we will implement worlds as structures with a name field 
and a field for the list of parents the world inherits from. Searching through the 
inheritance lattice could become costly, so we will do it only once each time the user 
changes worlds, and mark all the current worlds by setting the current field on or 
off. Here is the definition for the world structure: 

(defstruct (world (:print-function print-world)) 
name parents current) 

We will need a way to get from the name of a world to the world structure. Assuming 
names are symbols, we can store the structure on the name's property list. The 
function get-worl d gets the structure for a name, or builds a new one and stores it. 
get - wor 1 d can also be passed a world instead of a name, in which case it just returns 
the world. We also include a definition of the default initial world. 


<a id='page-500'></a>

(defun get-world (name &optional current (parents (list *world*))) 
"Look up or create the world with this name. 
If the world is new, give it the list of parents." 
(cond ((world-p name) name) ; ok if it already is a world 

((get name 'world)) 
(t (setf (get name 'world) 
(make-world rname name .-parents parents 
.'current current))))) 

(defvar *world* (get-world 'WO nil nil) 
"The current world used by index and fetch.") 

The function use-worl d is used to switch to a new world. It first makes the current 
world and all its parents no longer current, and then makes the new chosen world and 
all its parents current. The function use-new-worl d is more efficient in the common 
case where you want to create a new world that inherits from the current world. It 
doesn't have to turn any worlds off; it j ust creates the new world and makes it current. 

(defun use-world (world) 
"Make this world current." 
;; If passed a name, look up the world it names 
(setf world (get-world world)) 
(unless (eq world *world*) 

Turn the old world(s) off and the new one(s) on, 
;; unless we are already using the new world 
(set-world-current *world* nil) 
(set-world-current world t) 
(setf *world* world))) 

(defun use-new-world () 
"Make up a new world and use it. 
The world inherits from the current world." 
(setf *world* (get-world (gensym "W"))) 
(setf (world-current *world*) t) 
*world*) 

(defun set-world-current (world on/off) 
"Set the current field of world and its parents on or off." 

nil is off, anything else is on. 
(setf (world-current world) on/off) 
(dolist (parent (world-parents world)) 

(set-world-current parent on/off))) 

We also add a print function for worlds, which just prints the world's name. 


<a id='page-501'></a>
(defun print-world (world &optional (stream t) depth) 
(declare (ignore depth)) 
(prinl (world-name world) stream)) 

The format of the dtree data base has changed to include worlds, so we need 
new retrieval functions to search through this new format. Here the functions 
mapc-retrieve, retrieve, and retrieve-bagof are modified to give new versions 
that treat worlds. To reflect this change, the new functions all have names ending in 

-in-world: 

(defun mapc-retrieve-in-world (fn query) 
"For every fact in the current world that matches the query, 
apply the function to the binding list. " 
(dolist (bucket (fetch query)) 

(dolist (world/entries bucket) 
(when (world-current (first world/entries)) 
(dolist (answer (rest world/entries)) 
(let ((bindings (unify query answer))) 
(unless (eq bindings fail) 
(funcall fn bindings)))))))) 

(defun retrieve-in-world (query) 
"Find all facts that match query. Return a list of bindings." 
(let ((answers nil)) 

(mapc-retrieve-in-world 
#'(lambda (bindings) (push bindings answers)) 
query) 

answers)) 

(defun retrieve-bagof-in-world (query) 
"Find all facts in the current world that match query. 
Return a list of queries with bindings filled in. " 
(mapcar #'(lambda (bindings) (subst-bindings bindings query)) 

(retrieve-in-world query))) 

Now let's see how these worlds work. First, in WO we see that the facts from 
test -i ndex are still in the data base: 

> *world* ^ WO 

> (retrieve-bagof-in-world *(p ?z c)) ^ 
((P A C) (P A C) (P . .) 


<a id='page-502'></a>

Now we create and use a new world that inherits from WO. Two new facts are added 
to this new world: 

> (use-new-world) W7031 
> (index *(p new c)) => . 
> (index 'Cp b b)) => . 

We see that the two new facts are accessible in this world: 

> (retrieve-bagof-in-world '(p ?z c)) 
((P A C) (P A C) (P . C) (P NEW O) 

> (retrieve-bagof-in-world '(^p ?x ?y)) ^ 
((~P . .)) 

Now we create another world as an alternative to the current one by first switching 
back to the original WO, then creating the new world, and then adding some facts: 

> (use-world *W0) WO 

> (use-new-world) W7173 

> (index *(p newest c)) ^ . 
> (index '(~p c newest)) . 

Here we see that the facts entered in W7031 are not accessible, but the facts in the new 
world and in WO are: 

> (retrieve-bagof-in-world '(p ?z c)) => 
((P A C) (P A C) (P . C) (P NEWEST O) 

> (retrieve-bagof-in-world '(^p ?x ?y)) 
ir? C NEWEST)) 

Unification, Equality, Types, and Skolem Constants 

The lesson of the zebra puzzle in section 11.4 was that unification can be used to 
lessen the need for backtracking, because an uninstantiated logic variable or partially 
instantiated term can stand for a whole range of possible solutions. However, this 
advantage can quickly disappear when the representation forces the problem solver 
to enumerate possible solutions rather than treating a whole range of solutions as one. 
For example, consider the following query in the frame language and its expansion 
into primitives: 


<a id='page-503'></a>
(a person (name Fran)) 
= (and (ind ?p person) (val name ?p fran)) 

The way to answer this query is to enumerate all individuals ?p of type person and 
then check the name slot of each such person. It would be more efficient if (i nd ?p 
person) did not act as an enumeration, but rather as a constraint on the possible 
values of ?p. This would be possible if we changed the definition of variables (and 
of the unification function) so that each variable had a type associated with it. In 
fact, there are at least three sources of information that have been implemented as 
constraints on variables terms: 

* The type or category of the term. 
* The members or size of a term considered as a set or list. 
* Other terms this term is equal or not equal to. 
Note that with a good solution to the problem of equality, we can solve the problem 
of Skolem constants. The idea is that a regular constant unifies with itself but no 
other regular constant. On the other hand, a Skolem constant can potentially unify 
with any other constant (regular or Skolem). The equality mechanism is used to keep 
track of each Skolem variable's possible bindings. 

14.11 History and References 
Brachman and Levesque (1985) collect thirty of the key papers in knowledge representation. 
Included are some early approaches to semantic network based (Quillian 
1967) and logic-based (McCarthy 1968) representation. Two thoughtful critiques 
of the ad hoc use of representations without defining their meaning are by Woods 
(1975) and McDermott (1978). It is interesting to contrast the latter with McDermott 
1987, which argues that logic by itself is not sufficient to solve the problems of AI. 
This argument should not be surprising to those who remember the slogan logic = 
algonthm -control. 

Genesereth and Nilsson's textbook (1987) cover the predicate-calculus-based approach 
to knowledge representation and AI in general. Ernest Davis (1990) presents 
a good overview of the field that includes specialized representations for time, space, 
qualitative physics, propositional attitudes, and the interaction between agents. 

Many representation languages focus on the problem of defining descriptions for 
categories of objects. These have come to be known as term-subsumption languages. 
Examples include KL-ONE (Schmolze and Lipkis 1983) and KRYPTON (Brachman, 
Fikes, and Levesque 1983). See Lakoff 1987 for much more on the problem of 
categories and prototypes. 


<a id='page-504'></a>

Hector Levesque (1986) points out that the areas Prolog has difficulty with - 
disjunction, negation, and existentials - all involve a degree of vagueness. In his 
term, they lack vividness. A vivid proposition is one that could be represented 
directly in a picture: the car is blue; she has a martini in her left hand; Albany is the 
capital of New York. Nonvivid propositions cannot be so represented: the car is not 
blue; she has a martini in one hand; either Albany or New York City is the capital 
of New York. There is interest in separating vivid from nonvivid reasoning, but no 
current systems are actually built this way. 

The possible world approach of section 14.10 was used in the MRS system (Russell 
1985). More recent knowledge representation systems tend to use truth maintenance 
systems instead of possible worlds. This approach was pioneered by Doyle (1979) 
and McAllester (1982). Doyle tried to change the name to "reason maintenance," in 
(1983), but it was too late. The version in widest used today is the assumption-based 
truth maintenance system, or ATMS, developed by de Kleer (1986a,b,c). Charniak 
et al. (1987) present a complete Common Lisp implementation of a McAllesterstyleTMS. 


There is little communication between the logic programming and knowledge 
representation communities, even though they cover overlapping territory. Colmerauer 
(1990) and Cohen (1990) describe Logic Programming languages that address 
some of the issues covered in this chapter. Key papers in equality reasoning include 
Caller and Fisher 1974, Kornfeld 1983,^ Jaffar, Lassez, and Maher 1984, and van 
Emden and Yukawa 1987. H&ouml;dobler's book (1987) includes an overview of the area. 
Papers on extending unification in ways other than equality include Ait-Kaci et al. 
1987 and Staples and Robinson 1988. Finally, papers on extending Prolog to cover 
disjunction and negation (i.e., non-Horn clauses) include Loveland 1987, Plaisted 
1988, and Stickell988. 

14.12 Exercises 
&#9635; Exercise 14.1 [m] Arrange to store dtrees in a hash table rather than on the property 
list of predicates. 

&#9635; Exercise 14.2 [m] Arrange to store the dtree-atoms in a hash table rather than in 
an association list. 

&#9635; Exercise 14.3 [m] Change the dtree code so that .i 1 is used as an atom index. Time 
the performance on an application and see if the change helps or hurts. 

^ A commentary on this paper appears in Elcock and Hoddinott 1986. 


<a id='page-505'></a>
&#9635; Exercise 14.4 [m] Consider the query (. a b c d e f g). If the index under a 
returns only one or two keys, then it is probably a waste of time for dtree-fetc h 
to consider the other keys in the hope of finding a smaller bucket. It is certainly 
a waste if there are no keys at all indexed under a. Make appropriate changes to 
dtree-fetch . 

&#9635; Exercise 14.5 [h] Arrange to delete elements from a dtree. 

&#9635; Exercise 14.6 [h] Implement iterative-deepening search in the Prolog compiler. 
You will have to change each function to accept the depth as an extra argument, and 
compile in checks for reaching the maximum depth. 

&#9635; Exercise 14.7 [d] Integrate the Prolog compiler with the dtree data base. Use 
the dtrees for predicates with a large number of clauses, and make sure that each 
predicate that is implemented as a dtree has a Prolog primitive accessing the dtree. 

&#9635; Exercise 14.8 [d] Add support for possible worlds to the Prolog compiler with 
dtrees. This support has already been provided for dtrees, but you will have to 
provide it for ordinary Prolog rules. 

&#9635; Exercise 14.9 [h] Integrate the language described in section 14.10 and the frame 
syntax from section 14.10 with the extended Prolog compiler from the previous 
exercise. 

&#9635; Exercise 14.10 [d] Build a strategic reasoner that decides when to create a possible 
world and does reasoning by cases over these worlds. Use it to solve Moore's problem 
([page 466](chapter14.md#page-466)). 


<a id='page-506'></a>

14.13 Answers 
Answer 14.1 

(let ((dtrees (make-hash-table :test #'eq))) 

(defun get-dtree (predicate) 
"Fetch (or make) the dtree for this predicate." 
(setf (gethash predicate dtrees) 

(or (gethash predicate dtrees) 
(make-dtree)))) 

(defun clear-dtrees () 
"Remove all the dtrees for all the predicates." 
(clrhash dtrees))) 

Answer 14.5 Hint: here is the code for nl i st - del ete. Now figure out how to find 
all the nlists that an item is indexed under. 

(defun nlist-delete (item nlist) 
"Remove an element from an nlist . 
Assumes that item is present exactly once." 
(decf (car nlist)) 
(setf (cdr nlist) (delete item (cdr nlist) rcount D) 
nlist) 


## Chapter 15
<a id='page-509'></a>

Symbolic Mathematics 
with Canonical Forms 

Anything simple always interests me. 

-David Hockney 

C
C
hapter 8 started with high hopes: to take an existing pattern matcher, copy down some 
mathematical identities out of a reference book, and come up with a usable symbolic 
algebra system. The resulting system was usable for some purposes, and it showed 
that the technique of rule-based translation is a powerful one. However, the problems of 
section 8.5 show that not everything can be done easily and efficiently within the rule-based 
pattern matching framework. 

There are important mathematical transformations that are difficult to express in the rule-
based approach. For example, dividing two polynomials to obtain a quotient and remainder is 
a task that is easier to express as an algorithm - a program - than as a rule or set of rules. 


<a id='page-510'></a>

In addition, there is a problem with efficiency. Pieces of the input expressions are 
simplified over and over again, and much time is spent interpreting rules that do not 
apply. Section 9.6 showed some techniques for speeding up the program by a factor 
of 100 on inputs of a dozen or so symbols, but for expressions with a hundred or so 
symbols, the speed-up is not enough. We can do better by designing a specialized 
representation from the ground up. 

Serious algebraic manipulation programs generally enforce a notion of canonical 
simplification. That is, expressions are converted into a canonical internal format that 
may be far removed from the input form. They are then manipulated, and translated 
back to external form for output. Of course, the simplifier we have already does this 
kind of translation, to some degree. It translates (3 + . + -3+ y) into (+ . y) 
internally, and then outputs it as (. + y). But a canonical representation must have 
the property that any two expressions that are equal have identical canonical forms. 
In our system the expression (5 + y + x+ -5)is translated to the internal form (+ 
y .), which is not identical to (+ x y), even though the two expressions are equal. 
Thus, our system is not canonical. Most of the problems of the previous section stem 
from the lack of a canonical form. 

Adhering to canonical form imposes grave restrictions on the representation. For 
example, -1 and {x -l){x are equal, so they must be represented identically. 
One way to insure this is to multiply out all factors and collect similar terms. So 
{x-l){x-\-l)isx^ -x + x-l , which simplifies to x^ -1, in whatever the canonical 
internal form is. This approach works fine for x^ - 1, but for an expression like 
{x -1 )1000^ multiplying out all factors would be quite time- (and space-) consuming. 
It is hard to find a canonical form that is ideal for all problems. The best we can do is 
choose one that works well for the problems we are most likely to encounter. 

15.1 A Canonical Form for Polynomials 
This section will concentrate on a canonical form for polynomials. Mathematically 
speaking, a polynomial is a function (of one or more variables) that can be computed 
using only addition and multiplication. We will speak of a polynomial's main variable, 
coefficents, and degree. In the polynomial: 

5xx^-\-hxx^-\-cxx-\-l 

the main variable isx, the degree is 3 (the highest power of x), and the coefficients 
are 5,6, c and 1. We can define an input format for polynomials as follows: 

1. Any Lisp number is a polynomial. 
2. Any Lisp symbol is a polynomial. 

<a id='page-511'></a>
3. lip and q are polynomials, so are (p + ^) and (p* q). 
4. If . is a polynomial and . is a positive integer, then (p " .) is a polynomial. 
Hov^ever, the input format cannot be used as the canonical form, because it would 
admit both (X + y)and(y + x), and both 4 and (2 + 2). 

Before considering a canonical form for polynomials, let us see why polynomials 
were chosen as the target domain. First, the volume of programming needed to support 
canonical forms for a larger class of expressions grows substantially. To make 
things easier, we have eliminated complications like log and trig functions. Polynomials 
are a good choice because they are closed under addition and multiplication: 
the sum or product of any two polynomials is a polynomial. If we had allowed division, 
the result would not be closed, because the quotient of two polynomials need 
not be a polynomial. As a bonus, polynomials are also closed under differentiation 
and integration, so we can include those operators as well. 

Second, for sufficiently large classes of expressions it becomes not just difficult 
but impossible to define a canonical form. This may be surprising, and we don't 
have space here to explain exactly why it is so, but here is an argument: Consider 
what would happen if we added enough functionality to duplicate all of Lisp. Then 
"converting to canonical form" would be the same as "running a program." But it 
is an elementary result of computability theory that it is in general impossible to 
determine the result of running an arbitrary program (this is known as the halting 
problem). Thus, it is not surprising that it is impossible to canonicalize complex 
expressions. 

Our task is to convert a polynomial as previously defined into some canonical 
f orm.^ Much of the code and some of the commentary on this format and the routines 
to manipulate it was written by Richard Fateman, with some enhancements made 
by Peter Klier. 

The first design decision is to assume that we will be dealing mostly with dense 
polynomials, rather than sparse ones. That is, we expect most of the polynomials 
to be like ax^ -f bx^ .-cx -\-d, not like ax^^^ -|-bx^^ -h c. For dense polynomials, 
we can save space by representing the main variable {x in these examples) and the 
individual coefficients (a, 6, c, and d in these examples) explicitly, but representing 
the exponents only implicitly, by position. Vectors will be used instead of Usts, to 
save space and to allow fast access to any element. Thus, the representation of 

+ lOx^ + 20a: + 30 wiU be the vector: 
#(x 30 20 10 5) 

^ In fact, the algebraic properties of polynomial arithmetic and its generalizations fit so well 
with ideas in data abstraction that an extended example (in Scheme) on this topic is provided 
inStructure and Interpretation of Computer Programs by Abelson and Sussman (see section 2.4.3, 
pages 153-166). We'll pursue a slightly different approach here. 


<a id='page-512'></a>

The main variable, x, is in the 0th element of the vector, and the coefficient of the 
ith power of x is in element i + 1 of the vector. A single variable is represented as a 
vector whose first coefficient is 1, and a number is represented as itself: 

#(x 30 20 10 5) represents5x^ + lOx^ + 20x + 30 

#(x 0 1) representsX 

5 represents 5 

The fact that a number is represented as itself is a possible source of confusion. The 
number 5, for example, is a polynomial by our mathematical definition of polynomials. 
But it is represented as 5, not as a vector, so (typep 5 ' pol ynomi al) will be 
false. The word "polynomial" is used ambiguously to refer to both the mathematical 
concept and the Lisp type, but it should be clear from context which is meant. 

A glossary for the canonical simplifier program is given in figure 15.1. 

The functions defining the type polynomial follow. Because we are concerned 
with efficiency, we proclaim certain short functions to be compiled inline, use the 
specific function svref (simple-vector reference) rather than the more general aref, 
and provide declarations for the polynomials using the special form the. More details 
on efficiency issues are given in Chapter 9. 

(proclaim '(inline main-var degree coef 
var= var> poly make-poly)) 

(deftype polynomial () 'simple-vector) 

(defun main-var (p) (svref (the polynomial p) 0)) 

(defun coef (p i) (svref (the polynomial p) (+ i 1))) 

(defun degree (p) (- (length (the polynomial p)) 2)) 

We had to make another design decision in defining coef, the function to extract a 
coefficient from a polynomial. As stated above, the zth coefficient of a polynomial is 
in element i + 1 of the vector. If we required the caller of coef to pass in . .-1 to get 
2, we might be able to save a few addition operations. The design decision was that 
this would be too confusing and error prone. Thus, coef expects to be passed i and 
does the addition itself. 

For our format, we will insist that main variables be symbols, while coefficients 
can be numbers or other polynomials. A "production" version of the program might 
have to account for main variables like (sin .), as well as other complications like + 
and * with more than two arguments, and noninteger powers. 

Now we can extract information from a polynomial, but we also need to build 
and modify polynomials. The function poly takes a variable and some coefficients 
and builds a vector representing the polynomial, make-pol y takes a variable and a 
degree and produces a polynomial with all zero coefficients. 


<a id='page-513'></a>
canon-simplifier 
canon 

polynomial 

prefix->canon 
canon->prefix 
poly+poly 
poly*poly 
poly^n 
deriv-poly 

poly 
make-poly 
coef 
main-var 
degree 
var= 
var> 
poly-ipoly-
k->-poly 
k*poly 
poly+same 
poly*same 
normalize-poly 
exponent->prefix 
args->prefix 
rat-numerator 
rat-denominator 
rat*rat 
rat->-rat 
rat/rat 

Top-Level Fimctions 
A read-canonicalize-print loop. 
Canonicalize argument and convert it back to infix. 
Data Types 
A vector of main variable and coefficients. 
Major Functions 
Convert a prefix expression to canonical polynomial. 
Convert a canonical polynomial to a prefix expression. 
Add two polynomials. 
Multiply two polynomials. 
Raise polynomial . to the nth power, n>=0. 
Return the derivative, dp/dx, of the polynomial p. 
Auxiliary Fimctions 
Construct a polynomial with given coefficients. 
Construct a polynomial of given degree. 
Pick out the ith coefficient of a polynomial. 
The main variable of a polynomial. 
Thedegreeof a polynomial; (degree x^) = 2. 
Are two variables identical? 
Is one variable ordered before another? 
Unary or binary polynomial addition. 
Unary or binary polynomial subtraction. 
Add a constant k to a polynomial p. 
Multiply a polynomial . by a constant k. 
Add two polynomials with the same main variable. 
Multiply two polynomials with the same main variable. 
Alter a polynomial by dropping trailing zeros. 
Used to convert to prefix. 
Used to convert to prefix. 
Select the numerator of a rational. 

Select the denominator of a rational. 
Multiply two rationals. 
Add two rationals. 
Divide two rationals. 

Figure 15.1: Glossary for the Symbolic Manipulation Program 


<a id='page-514'></a>

(defun poly (x &rest coefs) 
"Make a polynomial with main variable . 
and coefficients in increasing order." 
(apply #.vector . coefs)) 

(defun make-poly (x degree) 
"Make the polynomial 0 + 0*x + 0*x''2 + ... 0*x"degree" 
(let ((p (make-array (+ degree 2) :initial-element 0))) 

(setf (main-var p) x) 

P)) 

A polynomial can be altered by setting its main variable or any one of its coefficients 
using the following defsetf forms. 

(defsetf main-var (p) (val) 
*(setf (svref (the polynomial ,p) 0) ,val)) 

(defsetf coef (p i) (val) 

'(setf (svref (the polynomial ,p) (+ .i D) .val)) 
The function pol y constructs polynomials in a fashion similar to 1 i st or vector: with 
an explicit list of the contents, make-poly, on the other hand, is like make-a may: it 
makes a polynomial of a specified size. 

We provide setf methods for modifying the main variable and coefficients. Since 
this is the first use of defsetf, it deserves some explanation. A defsetf form takes 
a function (or macro) name, an argument list, and a second argument list that must 
consist of a single argument, the value to be assigned. The body of the form is an 
expression that stores the value in the proper place. So the defsetf for ma 1 .- va r says 
that (setf (main-var p) val) is equivalent to (setf (svref (the polynomial p) 

0) val). A defsetf is much like a defmacro, but there is a little less burden placed 
on the writer of defsetf. Instead of passing . and val directly to the setf method. 
Common Lisp binds local variables to these expressions, and passes those variables 
to the setf method. That way, the writer does not have to worry about evaluating 
the expressions in the wrong order or the wrong number of times. It is also possible 
to gain finer control over the whole process with def i ne-setf-method, as explained 
on [page 884](chapter25.md#page-884). 
The functions poly+poly, poly*poly and poly'n perform addition, multiplication, 
and exponentiation of polynomials, respectively. They are defined with several 
helping functions. k*poly multipHes a polynomial by a constant, k, which may 
be a number or another polynomial that is free of polynomial p's main variable. 
poly*same is used to multiply two polynomials with the same main variable. For 
addition, the functions k+poly and poly+same serve analogous purposes. With that 
in mind, here's the function to convert from prefix to canonical form: 


<a id='page-515'></a>
(defun prefix->canon (x) 
"Convert a prefix Lisp expression to canonical form. 
Exs; (+ X 2) (* 3 x)) => #(x 0 3 1) 

(- (* (-X 1) (+ X D) (- (^ X 2) D) => 0" 

(cond ((numberp x) x) 
((symbolp x) (poly . 0 D) 
((and (exp-p x) (get (exp-op x) 'prefix->canon)) 

(apply (get (exp-op x) 'prefix->canon) 
(mapcar #'prefix->canon (exp-args x)))) 
(t (error "Not a polynomial: ~a" x)))) 

It is data-driven, based on the pref ix->canon property of each operator. In the 
following we install the appropriate functions. The existing functions poly*poly 
and poly'n can be used directly. But other operators need interface functions. The 
operators + and - need interface functions that handle both unary and binary. 

(dolist (item '((+ poly+) (- poly-) (* poly*poly) 
(" poly^n) (D deriv-poly))) 
(setf (get (first item) *prefix->canon) (second item))) 

(defun poly+ (&rest args) 
"Unary or binary polynomial addition." 
(ecase (length args) 

(1 (first args)) 

(2 (poly+poly (first args) (second args))))) 

(defun poly- (&rest args) 
"Unary or binary polynomial subtraction." 
(ecase (length args) 

(1 (poly*poly -1 (first args))) 
(2 (poly+poly (first args) (poly*poly -1 (second args)))))) 

The function pref ix->canon accepts inputs that were not part of our definition of 
polynomials: unary positive and negation operators and binary subtraction and 
differentiation operators. These are permissible because they can all be reduced to 
the elementary + and * operations. 

Remember that our problems with canonical form all began with the inability to 
decide which was simpler: (+ . y) or (+ y .). In this system, we define a canonical 
form by imposing an ordering on variables (we use alphabetic ordering as defined by 
stri ng>). The rule is that a polynomial . can have coefficients that are polynomials 
in a variable later in the alphabet than p's main variable, but no coefficients that 
are polynomials in variables earlier than p's main variable. Here's how to compare 
variables: 

(defun var= (x y) (eq . y)) 
(defun var> (x y) (string> . y)) 


<a id='page-516'></a>

The canonical form of the variable . will be #(x 0 1), which is 0 x x<sup>0</sup> + 1 x x<sup>1</sup>. The 
canonical form of (+ . y) is #(x #(y 0 1) 1). It couldn't be #(y #(x 0 1) 1), 
because then the resulting polynomial would have a coefficient with a lesser main 
variable. The policy of ordering variables assures canonicality, by properly grouping 
like variables together and by imposing a particular ordering on expressions that 
would otherwise be commutative. 

Here, then, is the code for adding two polynomials: 

(defun poly+poly (p q) 
"Add two polynomials." 
(normal ize-poly 

(cond 
((numberp p) (k+poly . q)) 
((numberp q) (k+poly q p)) 
((var= (main-var p) (main-var q)) (poly+same . q)) 
((var> (main-var q) (main-var p)) (k+poly q p)) 
(t (k+poly . q))))) 

(defun k+poly (k p) 
"Add a constant k to a polynomial p." 
(cond ((eql k 0) p) 0 + . = . 

((and (numberp k)(numberp p)) 
(+ k p)) Add numbers 

(t (let ((r (copy-poly p))) Add k to x"0 term of . 
(setf (coef r 0) (poly+poly (coef r 0) k)) 
r)))) 

(defun poly+same (p q) 
"Add two polynomials with the same main variable." 
First assure that q is the higher degree polynomial 

(if (> (degree p) (degree q)) 
(poly+same q p) 
;; Add each element of . into r (which is a copy of q). 
(let ((r (copy-poly q))) 

(loop for i from 0 to (degree p) do 
(setf (coef r i) (poly+poly (coef r i) (coef . i)))) 
r))) 

(defun copy-poly (p) 
"Make a copy a polynomial." 
(copy-seq p)) 


<a id='page-517'></a>
and the code for multiplying polynomials: 

(defun poly*poly (p q) 
"Multiply two polynomials." 
(normal ize-poly 

(cond 
((numberp p) (k*poly . q)) 
((numberp q) (k*poly q p)) 
((var= (main-var p) (main-var q)) (poly*same . q)) 
((var> (main-var q) (main-var p)) (k*poly q p)) 
(t (k*poly . q))))) 

(defun k*poly (k p) 
"Multiply a polynomial . by a constant factor k." 
(cond 

((eql k 0) 0) ;; 0 * . = 0 
((eql k 1) p) ;; 1 * . = . 
((and (numberp k) 

(numberp p)) (* k p)) Multiply numbers 
(t Multiply each coefficient 
(let ((r (make-poly (main-var p) (degree p)))) 
Accumulate result in r; rCi] = k*pCi] 
(loop for i from 0 to (degree p) do 
(setf (coef r i) (poly*poly k (coef . i)))) 
r)))) 

The hard part is multiplying two polynomials with the same main variable. This 
is done by creating a new polynomial, r, whose degree is the sum of the two input 
polynomials . and q. Initially, all of r's coefficients are zero. A doubly nested 
loop multiplies each coefficient of . and q and adds the result into the appropriate 
coefficient of r. 

(defun poly*same (p q) 
"Multiply two polynomials with the same variable." 
rCi] = pCO]*qCi] + pCl]*qCi-l] + ... 
(let* ((r-degree (+ (degree p) (degree q))) 
(r (make-poly (main-var p) r-degree))) 
(loop for i from 0 to (degree p) do 
(unless (eql (coef . i) 0) 
(loop for j from 0 to (degree q) do 
(setf (coef r (+ i j)) 
(poly+poly (coef r (+ i j)) 
(poly*poly (coef . i) 
(coef q j))))))) 
r)) 


<a id='page-518'></a>

Both poly+poly and poly*poly make use of the function normal ize-poly to "normalize" 
the result. The idea is that (- . 5) (". 5)) should return O, not 
#(xOOOOOO). Note that normal ize-poly is a destructive operation: it calls 
del ete, which can actually alter its argument. Normally this is a dangerous thing, 
but since norma1 i ze - poly is replacing something with its conceptual equal, no harm 
is done. 

(defun normalize-poly (p) 
"Alter a polynomial by dropping trailing zeros." 
(if (numberp p) 

. 
(let ((p-degree (- (position 0 . itest (complement #'eql) 
:from-end t) 
1))) 
(cond ((<= p-degree 0) (normalize-poly (coef . 0))) 
((< p-degree (degree p)) 
(delete 0 . .-start p-degree)) 
(t p))))) 

There are a few loose ends to clean up. First, the exponentiation function: 

(defun poly'n (p n) 
"Raise polynomial . to the nth power, n>=0." 
(check-type . (integer 0 *)) 
(cond ((= . 0) (assert (not (eql . 0))) 1) 

((integerp p) (expt . .)) 
(t (poly*poly . (poly^n . (-. 1)))))) 

15.2 Differentiating Polynomials 
The differentiation routine is easy, mainly because there are only two operators (+ 
and *) to deal with: 

(defun deriv-poly (p x) 
"Return the derivative, dp/dx, of the polynomial p." 
;; If . is a number or a polynomial with main-var > x, 

then . is free of x, and the derivative is zero; 
;; otherwise do real work. 
;; But first, make sure X is a simple variable, 
;; of the form #(X 0 1). 
(assert (and (typep . 'polynomial) (= (degree x) 1) 

(eql (coef . 0) 0) (eql (coef . 1) 1))) 


<a id='page-519'></a>

(cond 
((numberp p) 0) 
((var> (main-var p) (main-var x)) 0) 
((var= (main-var p) (main-var x)) 

d(a + bx + cx^2 + dx^3)/dx = b + 2cx + 3dx'^2 

So, shift the sequence . over by 1, then 
;; put . back in, and multiply by the exponents 
(let ((r (subseq . 1))) 

(setf (main-var r) (main-var x)) 
(loop for i from 1 to (degree r) do 
(setf (coef r i) (poly*poly (+ i 1) (coef r i)))) 
(normalize-poly r))) 

(t Otherwise some coefficient may contain x. Ex: 
d(z + 3x + 3zx^2 + z^2x^3)/dz 
= 1 + 0 + 3x'^2 + 2zx"3 
So copy p, and differentiate the coefficients, 

(let ((r (copy-poly p))) 
(loop for i from 0 to (degree p) do 
(setf (coef r i) (deriv-poly (coef r i) x))) 
(normalize-poly r))))) 

&#9635; Exercise 15.1 [h] Integrating polynomials is not much harder than differentiating 
them. For example: 

&int; ax<sup>2</sup> + bx dx = ax<sup>3</sup>/3 + bx<sup>2</sup>/2 + c.

Write a function to integrate polynomials and install it in pref ix->canon. 

&#9635; Exercise 15.2 [m] Add support for definite integrals, such as y dx. You will 
need to make up a suitable notation and properly install it in both infix->prefix 
and prefix->canon. A full implementation of this feature would have to consider 
infinity as a bound, as well as the problem of integrating over singularities. You need 
not address these problems. 

15.3 Converting between Infix and Prefix 
All that remains is converting from canonical form back to prefix form, and from 
there back to infix form. This is a good point to extend the prefix form to allow 
expressions with more than two arguments. First we show an updated version of 
pref i x->i nf i . that handles multiple arguments: 


<a id='page-520'></a>

(defun prefix->infix (exp) 
"Translate prefix to infix expressions. 
Handles operators with any number of args." 
(if (atom exp) 

exp 

(intersperse 
(exp-op exp) 
(mapcar #'prefix->infix (exp-args exp))))) 

(defun intersperse (op args) 
"Place op between each element of args. 
Ex: (intersperse '+ '(a b c)) => '(a + b + c)" 
(if (length=1 args) 

(first args) 

(rest (loop for arg in args 
collect op 
collect arg)))) 

Now we need only convert from canonical form to prefix: 

(defun canon->prefix (p) 
"Convert a canonical polynomial to a lisp expression." 
(if (numberp p) 

. 

(args->prefix 
'+ 0 
(loop for i from (degree p) downto 0 

collect (args->prefix 
'* 1 
(list (canon->prefix (coef pi)) 

(exponent->prefix 
(main-var p) i))))))) 

(defun exponent->prefix (base exponent) 
"Convert canonical base^exponent to prefix form." 
(case exponent 

(0 1) 
(1 base) 
(t *(" .base .exponent)))) 

(defun args->prefix (op identity args) 
"Convert argl op arg2 op ... to prefix form." 
(let ((useful-args (remove identity args))) 

(cond ((null useful-args) identity) 
((and (eq op '*) (member 0 args)) 0) 
((length=1 args) (first useful-args)) 
(t (cons op (mappend 

#*(lambda (exp) 


<a id='page-521'></a>
(if (starts-with exp op) 
(exp-args exp) 
(list exp))) 

useful-args)))))) 

Finally, here's a top level to make use of all this: 

(defun canon (infix-exp) 
"Canonicalize argument and convert it back to infix" 
(prefix->infix 

(canon->prefix 
(prefix->canon 
(infix->prefix infix-exp))))) 

(defun canon-simplifier () 
"Read an expression, canonicalize it. and print the result." 
(loop 

(print 'canon>) 
(print (canon (read))))) 

and an example of it in use: 

> (canon-simplifier) 
CANON> (3 + X + 4 - X) 
7 
CANON> (X + y + y + X) 
((2 * .) + (2 * Y)) 
CANON> (3 * X + 4 * X) 
(7 * X) 
CANON> (3*x + y + x + 4*x) 
((8 * X) + Y) 
CANON> (3*x + y + z + x + 4*x) 
((8 * X) + (Y + Z)) 
CANON> ((X + 1) ^ 10) 
((X ^ 10) + (10 * (X ^ 9)) + (45 * (X ^ 8)) + (120 * (X ^ 7)) 

+ (210 * (X ^ 6)) + (252 * (X ^ 5)) + (210 * (X ^ 4)) 
+ (120 * (X 3)) + (45 * (X ^ 2)) + (10 * X) + 1) 
CAN0N> ((X + 1) 10 + (X - 1) ^ 10) 
((2 * (X ^ 10)) + (90 * (X ^ 8)) + (420 * (X ^ 6)) 
+ (420 * (X ^ 4)) + (90 * (X ^ 2)) + 2) 
CAN0N> ((X + 1) ^ 10 - (X - 1) ^ 10) 
((20 * (X ^ 8)) + (240 * (X ^ 7)) + (504 * (X ^ 5)) 
+ (240 * (X ^ 3)) + (20 * X)) 
CAN0N> (3 * X ^ 3 + 4 * X * y * (X - 1) + X ^ 2 * (X + y)) 
((4 * (X ^ 3)) + ((5 * Y) * (X ^ 2)) + ((-4 * Y) * X)) 
CAN0N> (3*x^3 + 4*x*w*(x-l)+x^2*( x + w)) 
((((5 * (X ^ 2)) + (-4 * X)) * W) + (4 * (X ^ 3))) 

<a id='page-522'></a>

CANON> (d (3 * X ^ 2 + 2 * X + 1) / d X) 
((6 * X) + 2) 
CANON> (d(z +3*x+3*z*x^2+z^2*x^3)/dz) 
(((2 * Z) * (X ^ 3)) + (3 * (X ^ 2)) + 1) 
CANON> [Abort] 

15.4 Benchmarking the Polynomial Simplifier 
Unlike the rule-based program, this version gets all the answers right. Not only is the 
program correct (at least as far as these examples go), it is also fast. We can compare 
it to the canonical simplifier originally written for MACSYMA by William Martin (circa 
1968), and modified by Richard Fateman. The modified version was used by Richard 
Gabriel in his suite of Common Lisp benchmarks (1985). The benchmark program 
is called f rpo1y, because it deals with polynomials and was originally written in 
the dialect Franz Lisp. The f rpoly benchmark encodes polynomials as lists rather 
than vectors, and goes to great lengths to be efficient. Otherwise, it is similar to the 
algorithms used here (although the code itself is quite different, using progs and gos 
and other features that have fallen into disfavor in the intervening decades). The 
particular benchmark we will use here is raising 1-\- . -\-y -\-ziothe 15th power: 

(defun rl5-test () 

(let ((r (prefix->canon *(+ 1 (+ . (+ y .)))))) 

(time (poly^n r 15)) 

nil)) 

This takes .97 seconds on our system. The equivalent test with the original f rpoly 
code takes about the same time: .98 seconds. Thus, our program is as fast as 
production-quaUty code. In terms of storage space, vectors use about half as much 
storage as lists, because half of each cons cell is a pointer, while vectors are all useful 
data.2 

How much faster is the polynomial-based code than the rule-based version? 
Unfortunately, we can't answer that question directly. We can time (simp ' ((1 

+ x+ y + z) " 15))). This takes only a tenth of a second, but that is because 
it is doing no work at all - the answer is the same as the input! Alternately, we 
can take the expression computed by (poly^n r 15), convert it to prefix, and pass 
that to simpli fy. simpl i fy takes 27.8 seconds on this, so the rule-based version is 
^Note: systems that use ''cdr-coding" take about the same space for lists that are allocated 
all at once as for vectors. But cdr-coding is losing favor as RISC chips replace microcoded 
processors. 


<a id='page-523'></a>
much slower. Section 9.6 describes ways to speed up the rule-based program, and a 
comparison of timing data appears on [page 525](chapter15.md#page-525). 

There are always surprises when it comes down to measuring timing data. For 
example, the alert reader may have noticed that the version of pol y defined above 
requires . multiplications. Usually, exponentiation is done by squaring a value when 
the exponent is even. Such an algorithm takes only log. multiplications instead of 

n. We can add a line to the definition of poly to get an 0(log n) algorithm: 
(defun poly^n (p n) 
"Raise polynomial . to the nth power. n>=0." 
(check-type . (integer 0 *)) 
(cond ((= . 0) (assert (not (eql . 0))) 1) 

((integerp p) (expt . n)) 
((evenp n) (poly^2 (poly^n . (/ . 2)))) 
(t (poly*poly . (poly^n . (-. 1)))))) 

(defun poly"2 (p) (poly*poly . .)) 

The surprise is that this takes longer to raise *r* to the 15th power. Even though it 
does fewer pol y*pol y operations, it is doing them on more complex arguments, and 
there is more work altogether. If we use this version of poly'n, then rl5-test takes 

1.6seconds instead of .98 seconds. 
By the way, this is a perfect example of the conceptual power of recursive functions. 
We took an existing function, poly "n, added a single cond clause, and changed 
it from an 0(n) to O(logn) algorithm. (This turned out to be a bad idea, but that's 
beside the point. It would be a good idea for raising integers to powers.) The reasoning 
that allows the change is simple: First, is certainly equal to (p^^^^)^ when 
. is even, so the change can't introduce any wrong answers. Second, the change 
continues the policy of decrementing . on every recursive call, so the function must 
eventually terminate (when . = 0). If it gives no wrong answers, and it terminates, 
then it must give the right answer. 

In contrast, making the change for an iterative algorithm is more complex. The 
initial algorithm is simple: 

(defun poly^n (p n) 

(let ((result D) 
(loop repeat . do (setf result (poly*poly . result))) 
result)) 

But to change it, we have to change the repeat loop to a whi 1 e loop, explicitly put in 
the decrement of n, and insert a test for the even case: 


<a id='page-524'></a>

(defun poly^n (p n) 
(let ((result D) 
(loop while (> . 0) 
do (if (evenp n) 
(setf . (poly^2 p) 
. (/ . 2)) 
(setf result (poly*poly . result) 
. (- . 1)))) 
result)) 

For this problem, it is clear that thinking recursively leads to a simpler function that 
is easier to modify. 

It turns out that this is not the final word. Exponentiation of polynomials can be 
done even faster, with a little more mathematical sophistication. Richard Fateman's 
1974 paper on Polynomial Multiplication analyzes the complexity of a variety of 
exponentiation algorithms. Instead of the usual asymptotic analysis (e.g. 0(n) 
or (9(n^)), he uses a fine-grained analysis that computes the constant factors (e.g. 
1000 X . or 2 X n^). Such analysis is crucial for small values of n. It turns out that for a 
variety of polynomials, an exponentiation algorithm based on the binomial theorem 
is best. The binomial theorem states that 

(a + b)<sup>n = &Sigma; TK


i=0 

for example. 

(a + b)<sup>3</sup> = b<sup>3</sup> + 3ab<sup>2</sup> + 3a<sup>2</sup> + a<sup>3</sup>

We can use this theorem to compute a power of a polynomial all at once, instead 
of computing it by repeated multiplication or squaring. Of course, a polynomial will 
in general be a sum of more than two components, so we have to decide how to split it 
into the a and b pieces. There are two obvious ways: either cut the polynomial in half, 
so that a and 6 will be of equal size, or split off one component at a time. Fateman 
shows that the latter method is more efficient in most cases. In other words, a 
polynomial k^x'^ -fk2x'^~~^ +k^x^''^ -h . . will be treated as the sum a + b where 
a = k\x'^ and b is the rest of the polynomial. 

Following is the code for binomial exponentiation. It is somewhat messy, because 
the emphasis is on efficiency. This means reusing some data and using . - add - i nto 1 
instead of the more general poly+poly. 

(defun poly'^n (p n) 

"Raise polynomial . to the nth power, n>=0." 

;; Uses the binomial theorem 

(check-type . (integer 0 *)) 

(cond 

((= . 0) 1) 


<a id='page-525'></a>
((integerp p) (expt . .)) 
(t ;; First: split the polynomial . = a + b, where 
a = k*x^d and b is the rest of . 
(let ((a (make-poly (main-var p) (degree p))) 
(b (normalize-poly (subseq . 0 (- (length p) 1)))) 

Allocate arrays of powers of a and b: 
(a^n (make-array (+ . 1))) 
(b^n (make-array {+ . 1))) 

Initialize the result: 

(result (make-poly (main-var p) (* (degree p) n)))) 
(setf (coef a (degree p)) (coef . (degree p))) 
;; Second: Compute powers of a^i and b^i for i up to . 
(setf (aref a^n 0) 1) 
(setf (aref b^n 0) 1) 
(loop for i from 1 to . do 

(setf (aref a^n i) (poly*poly a (aref a^n (- i 1)))) 
(setf (aref b^n i) (poly*poly b (aref b^n (-i 1))))) 
;; Third: add the products into the result. 
so that resultCi] = (n choose i) * a'^i * b"(n-i) 
(let ((c 1)) c helps compute (n choose i) incrementally 
(loop for i from 0 to . do 
(p-add-into! result c 
(poly*poly (aref a'^n i) 
(aref b^n (- . i)))) 
(setf c (/ (* c (- . i)) (+ i 1))))) 
(normalize-poly result))))) 

(defun p-add-into! (result c p) 
"Destructively add c*p into result." 
(if (or (numberp p) 

(not (var= (main-var p) (main-var result)))) 
(setf (coef result 0) 
(poly+poly (coef result 0) (poly*poly c p))) 
(loop for i from 0 to (degree p) do 
(setf (coef result i) 
(poly+poly (coef result i) (poly*poly c (coef . i)))))) 
result) 

Using this version of pol y "n, rl5 - test takes only .23 seconds, four times faster than 
the previous version. The following table compares the times for rl5 -test with 
the three versions of poly'n, along with the times for applying simply to the rl5 
polynomial, for various versions of s i mpl i f y: 


<a id='page-526'></a>

program sees speed-up 
rule-based versions 
1 original 27.8 -2 memoization 7.7 4 
3 memo+index 4.0 7 
4 compilation only 2.5 11 
5 memo+compilation 1.9 15 
canonical versions 
6 squaring pol y'n 1.6 17 
7 iterative poly' n .98 28 
8 binomial poly' n .23 120 

As we remarked earlier, the general techniques of memoization, indexing, and 
compilation provide for dramatic speed-ups. However, in the end, they do not lead 
to the fastest program. Instead, the fastest version was achieved by throwing out the 
original rule-based program, replacing it with a canonical-form-based program, and 
fine-tuning the algorithms within that program, using mathematical analysis. 

Now that we have achieved a sufficiently fast system, the next two sections 
concentrate on making it more powerful. 

15.5 A Canonical Form for Rational Expressions 
A rational number is defined as a fraction: the quotient of two integers. A rational 
expression is hereby defined as the quotient of two polynomials. This section presents 
a canonical form for rational expressions. 

First, a number or polynomial will continue to be represented as before. The 
quotient of two polynomials will be represented as a cons cells of numerator and 
denominator pairs. However, just as Lisp automatically reduces rational numbers 
to simplest form (6/8 is represented as 3/4), we must reduce rational expressions. 
So, for example, {x^ -l)/{x -1) must be reduced to . + 1, not left as a quotient of 
two polynomials. 

The following functions build and access rational expressions but do not reduce 
to simplest form, except in the case where the denominator isa number. Building up 
the rest of the functionality for full rational expressions is left to a series of exercises: 

(defun make-rat (numerator denominator) 

"Build a rational: a quotient of two polynomials." 

(if (numberp denominator) 

(k*poly (/ 1 denominator) numerator) 

(cons numerator denominator))) 


<a id='page-527'></a>
(defun rat-numerator (rat) 
"The numerator of a rational expression." 
(typecase rat 

(cons (car rat)) 
(number (numerator rat)) 
(t rat))) 

(defun rat-denominator (rat) 
"The denominator of a rational expression.' 
(typecase rat 

(cons (cdr rat)) 
(number (denominator rat)) 
(t 1))) 

&#9635; Exercise 15.3 [s] Modify pref i x->canon to accept input of the form . / y and to 
return rational expressions instead of polynomials. Also allow for input of the form 
. " - n. 

&#9635; Exercise 15.4 [m] Add arithmetic routines for multiplication, addition, and division 
of rational expressions. Call them rat*rat, rat+rat, and rat/rat respectively. 
They will call upon poly*poly. poly+poly and a new function, pol y/poly, which is 
defined in the next exercise. 

&#9635; Exercise 15.5 [h] Define poly-gcd, which computes the greatest common divisor 
of two polynomials. 

&#9635; Exercise 15.6 [h] Using poly-gcd, define the function pol y/poly, which will implement 
division for polynomials. Polynomials are closed under addition and multiplication, 
so poly+poly and poly*poly both returned polynomials. Polynomials are 
not closed under division, so pol y /pol y will return a rational expression. 

15.6 Extending Rational Expressions 
Now that we can divide polynomials, the final step is to reinstate the logarithmic, 
exponential, and trigonometric functions. The problem is that if we allow all these 
functions, we get into problems with canonical form again. For example, the following 
three expressions are all equivalent: 


<a id='page-528'></a>

sin(x) 

cos (x- ^) 

2i 

If we are interested in assuring we have a canonical form, the safest thing is to 
allow only and log(x). All the other functions can be defined in terms of these two. 
With this extension, the set of expressions we can form is closed under differentiation, 
and it is possible to canonicalize expressions. The result is a mathematically sound 
construction known as a differentiable field. This is precisely the construct that is 
assumed by the Risch integration algorithm (Risch 1969,1979). 

The disadvantage of this minimal extension is that answers may be expressed in 
unfamiliar terms. The user asks for d sin(x^)/dx, expecting a simple answer in terms 
of cos, and is surprised to see a complex answer involving e*^. Because of this problem, 
most computer algebra systems have made more radical extensions, allowing 
sin, cos, and other functions. These systems are treading on thin mathematical ice. 
Algorithms that would be guaranteed to work over a simple differentiable field may 
fail when the domain is extended this way. In general, the result will not be a wrong 
answer but rather the failure to find an answer at all. 

15.7 History and References 
A brief history of symbolic algebra systems is given in chapter 8. Fateman (1979), 
Martin and Fateman (1971), and Davenport et al. (1988) give more details on the MACSYMA 
system, on which this chapter is loosely based. Fateman (1991) discusses the 
frpoly benchmark and introduces the vector implementation used in this chapter. 

15.8 Exercises 
&#9635; Exercise 15.7 [h] Implement an extension of the rationals to include logarithmic, 
exponential, and trigonometric functions. 

&#9635; Exercise 15.8 [m] Modify deri . to handle the extended rational expressions. 

&#9635; Exercise 15.9 [d] Adapt the integration routine from section 8.6 ([page 252](chapter8.md#page-252)) to the 
rational expression representation. Davenport et al. 1988 may be useful. 


<a id='page-529'></a>

&#9635; Exercise 15.10 [s] Give several reasons why constant polynomials, like 3, are represented 
as integers rather than as vectors. 

15.9 Answers 
Answer 15.4 

(defun rat*rat (x y) 
"Multiply rationals: a/b * c/d= a*c/b*d" 
(poly/poly (poly*poly (rat-numerator x) 

(rat-numerator y)) 
(poly*poly (rat-denominator x) 
(rat-denominator y)))) 

(defun rat+rat (x y) 
"Add rationals: a/b + c/d= (a*d + c*b)/b*d" 
(let ((a (rat-numerator x)) 

(b (rat-denominator x)) 
(c (rat-numerator y)) 
(d (rat-denominator y))) 

(poly/poly (poly+poly (poly*poly a d) (poly*poly c b)) 
(poly*poly b d)))) 

(defun rat/rat (x y) 
"Divide rationals: a/b / c/d= a*d/b*c" 
(rat*rat . (make-rat (rat-denominator y) (rat-numerator y)))) 

Answer 15.6 

(defun poly/poly (p q) 
"Divide . by q: if d is the greatest common divisor of . and q 
then p/q = (p/d) / (q/d). Note if q=l. then p/q = p." 
(if (eql q 1) 

. 
(let ((d (poly-gcd . q))) 
(make-rat (poly/poly . d) 
(poly/poly q d))))) 

Answer 15.10 (1) An integer takes less time and space to process. (2) Representing 
numbers as a polynomial would cause an infinite regress, because the coefficients 
would be numbers. (3) Unless a policy was decided upon, the representation would 
not be canonical, since #(. 3) and #(y 3) both represent 3. 


## Chapter 16
<a id='page-530'></a>

Expert Systems 

An expert is one who knows more and more 
about less and less. 

-Nicholas Murray Butler (1862-1947) 

I
I
n the 1970s there was terrific interest in the area of knowledge-based expert systems. An expert 
system or knowledge-based system is one that solves problems by applying knowledge 
that has been garnered from one or more experts in a field. Since these experts will not in 
general be programmers, they will very probably express their expertise in terms that cannot 
immediately be translated into a program. It is the goal of expert-system research to come up 
with a representation that is flexible enough to handle expert knowledge, but still capable of 
being manipulated by a computer program to come up with solutions. 


<a id='page-531'></a>

A plausible candidate for this representation is as logical facts and rules, as in 
Prolog. However, there are three areas where Prolog provides poor support for a 
general knowledge-based system: 

* Reasoning with uncertainty. Prolog only deals with the black-and-white world 
of facts that are clearly true or false (and it doesn't even handle false very well). 
Often experts will express rules of thumb that are "likely" or "90% certain." 
* Explanation. Prolog gives solutions to queries but no indication of how those 
solutions were derived. A system that can explain its solutions to the user in 
understandable terms will be trusted more. 
* Flexible flow of control. Prolog works by backward-chaining from the goal. In 
some cases, we may need more varied control strategy. For example, in medical 
diagnosis, there is a prescribed order for acquiring certain information about 
the patient. A medical system must follow this order, even if it doesn't fit in 
with the backward-chaining strategy. 
The early expert systems used a wide variety of techniques to attack these problems. 
Eventually, it became clear that certain techniques were being used frequently, 
and they were captured in expert-system shells: specialized programming environments 
that helped acquire knowledge from the expert and use it to solve problems 
and provide explanations. The idea was that these shells would provide a higher 
level of abstraction than just Lisp or Prolog and would make it easy to write new 
expert systems. 

The MYCIN expert system was one of the earliest and remains one of the best 
known. It was written by Dr. Edward Shortliffe in 1974 as an experiment in medical 
diagnosis. MYCIN was designed to prescribe antibiotic therapy for bacterial blood 
infections, and when completed it was judged to perform this task as well as experts 
in the field. Its name comes from the common suffix in drugs it prescribes: erythromycin, 
clindamycin, and so on. The following is a slightly modified version of 
one of MYCIN'S rules, along with an English paraphrase generated by the system: 

(defrule 52 

if (site culture is blood) 
(gram organism is neg) 
(morphology organism is rod) 
(burn patient is serious) 

then .4 
(identity organism is Pseudomonas)) 


<a id='page-532'></a>

Rule 52: 
If 

1) THE SITE OF THE CULTURE IS BLOOD 
2) THE GRAM OF THE ORGANISM IS NEG 
3) THE MORPHOLOGY OF THE ORGANISM IS ROD 
4) THE BURN OF THE PATIENT IS SERIOUS 
Then there is weakly suggestive evidence (0.4) that 
1) THE IDENTITY OF THE ORGANISM IS PSEUDOMONAS 
MYCIN lead to the development of the EMYCIN expert-system shell. EMYCIN stands 
for "essential MYCIN," although it is often mispresented as "empty MYCIN." Either 
way, the name refers to the shell for acquiring knowledge, reasoning with it, and 
explaining the results, without the specific medical knowledge. 

EMYCIN is a backward-chaining rule interpreter that has much in common with 
Prolog. However, there are four important differences. First, and most importantly, 
EMYCIN deals with uncertainty. Instead of insisting that all predications be true or 
false, EMYCIN associates a certainty factor with each predication. Second, EMYCIN 
caches the results of its computations so that they need not be duplicated. Third, 
EMYCIN provides an easy way for the system to ask the user for information. Fourth, 
it provides explanations of its behavior. This can be summed up in the equation: 

EMYCIN = Prolog -h uncertainty + caching + questions -h explanations 

We will first cover the ways EMYCIN is different from Prolog. After that we will 
return to the main core of EMYCIN, the backward-chaining rule interpreter. Finally, 
we will show how to add some medical knowledge to EMYCIN to reconstruct MYCIN. 
A glossary of the program is in figure 16.1. 

16.1 Dealing with Uncertainty 
EMYCIN deals with uncertainty by replacing the two boolean values, true and false, 
with a range of values called certainty factors. These are numbers from -1 (false) to 
+1 (true), with 0 representing a complete unknown. In Lisp: 

(defconstant true +1.0) 
(defconstant false -1.0) 
(defconstant unknown 0.0) 

To define the logic of certainty factors, we need to define the logical operations, 
such as and, or, and not. The first operation to consider is the combination of two 
distinct pieces of evidence expressed as certainty factors. Suppose we are trying to 


<a id='page-533'></a>
emycin 
mycin 

defcontext 
defparm 
defrule 

true 
false 
unknown 
cf-cut-off 

context 
parm 
rule 
yes/no 

get-context-data 
find-out 
get-db 
use-rules 
use-rule 
new-instance 
report-findings 

cf-or 
cf-and 
true-p 
false-p 
cf-p 
put-db 
clear-db 
get-vals 
get-cf 
update-cf 
ask-vals 
prompt-and-read-vals 
inst-name 
check-reply 
parse-reply 
parm-type 
get-parm 
put-rule 
get-rules 
clear-rules 
satisfy-premises 
eval-condition 
reject-premise 
conclude 
is 
check-conditions 
print-rule 
print-conditions 
print-condition 
cf->english 
print-why 

Top-Level Functions for the Client 
Run the shell on a list of contexts representing a problem. 
Run the shell on the microbial infection domain. 
Top-Level Fimctions for the Expert 
Define a context. 
Define a parameter. 
Define a rule. 
Constants 
A certainty factor of +1. 
A certainty factor of -1. 
A certainty factor of 0. 
Below this certainty we cut off search. 
Data Types 
A subdomain concerning a particular problem. 
A parameter. 
A backward-chaining rule with certainty factors. 
The type with members yes and no. 
Major Functions within Emycin 
Collect data and draw conclusions. 
Determine values by knowing, asking, or using rules. 
Retrieve a fact from the data base. 

Apply all rules relevent to a parameter. 

Apply one rule. 
Create a new instance of a context. 
Print the results. 
Auxiliary Functions 
Combine certainty factors (CPs) with OR. 

Combine certainty factors (CPs) with AND. 
Is this CP true for purposes of search? 
Is this CP false for purposes of search? 
Is this a certainty factor? 
Place a fact in the data base. 
Clear all facts from the data base. 
Get value and CP for a parameter/instance. 
Get CP for a parameter/instance/value triplet. 
Change CP for a parameter/instance/value triplet. 
Ask the user for value/CP for a parameter/instance. 
Print a prompt and read a reply. 
The name of an instance. 
See if reply is valid list of CP/values. 
Convert reply into list of CP/values. 
Values of this parameter must be of this type. 
Find or make a parameter structure for this name. 
Add a new rule, indexed under each conclusion. 
Retrieve rules that help determine a parameter. 
Remove all rules. 
Calculate the combined CP for the premises. 
Determine the CP for a condition. 
Rule out a premise if it is clearly false. 
Add a parameter/instance/value/CP to the data base. 
An alias for equal. 
Make sure a rule is valid. 
Print a rule. 
Print a list of conditions. 
Print a single condition. 
Convert .7 to "suggestive evidence," etc. 
Say why a rule is being used. 

Figure 16.1: Glossary for the EMYCIN Program 


<a id='page-534'></a>

determine the chances of a patient having disease X. Assume we have a population 
of prior patients that have been given two lab tests. One test says that 60% of the 
patients have the disease and the other says that 40% have it. How should we 
combine these two pieces of evidence into one? Unfortunately, there is no way to 
answer that question correctly without knowing more about the dependence of the two 
sources on each other. Suppose the first test says that 60% of the patients (who all 
happen to be male) have the disease, and the second says that 40% (who all happen 
to be female) have it. Then we should conclude that 100% have it, because the two 
tests cover the entire population. On the other hand, if the first test is positive only 
for patients that are 70 years old or older, and the second is positive only for patients 
that are 80 or older, then the second is just a subset of the first. This adds no new 
information, so the correct answer is 60% in this case. 

In section 16.9 we will consider ways to take this kind of reasoning into account. 
For now, we will present the combination method actually used in EMYCIN. It is 
defined by the formula: 

combine (A, B) = 

A + B-AB; A,B>0 

A-^B-\-AB; A,B <0 

A-\-B 

; otherwise 

l-min{\Al\B\) 

According to this formula, combine(.60,.40) = .76, which is a compromise between 
the extremes of .60 and 1.00. It is the same as the probability p(A or B), assuming that 
A and . are independent. 

However, it should be clear that certainty factors are not the same thing as 
probabilities. Certainty factors attempt to deal with disbelief as well as belief, but 
they do not deal with dependence and independence. The EMYCIN combination 
function has a number of desirable properties: 

* It always computes a number between -1 and +1. 
* Combining unknown (zero) with anything leaves it unchanged. 
* Combining true with anything (except false) gives true. 
* Combining true and false is an error, 
* Combining two opposites gives unknown. 
* Combining two positives (except true) gives a larger positive. 
* Combining a positive and a negative gives something in between. 

<a id='page-535'></a>
So far we have seen how to combine two separate pieces of evidence for the same 
hypothesis. In other words, if we have the two rules: 

and we know A with certainty factor (cf) .6 and . with cf .4, then we can conclude C 
with cf .76. But consider a rule with a conjunction in the premise: 

AandB=>C 

Combining A and . in this case is quite different from combining them when they are 
in separate rules. EMYCIN chooses to combine conjunctions by taking the minimum of 
each conjunct's certainty factor. If certainty factors were probabilities, this would be 
equivalent to assumming dependence between conjuncts in a rule. (If the conjuncts 
were independent, then the product of the probabilities would be the correct answer.) 
So EMYCIN is making the quite reasonable (but sometimes incorrect) assumption that 
conditions that are tied together in a single rule will be dependent on one another, 
while conditions in separate rules are independent. 

The final complication is that rules themselves may be uncertain. That is, MYCIN 
accommodates rules that look like: 

AandB=^.9C 

to say that A and . imply C with .9 certainty. EMYCIN simply multiplies the rule's cf 
by the combined cf of the premise. So if A has cf .6 and . has cf .4, then the premise 
as a whole has cf .4 (the minimum of A and B), which is multiplied by .9 to get .36. 
The .36 is then combined with any exisiting cf for C. If C is previously unknown, then 
combining .36 with 0 will give .36. If C had a prior cf of .76, then the new cf would be 
.36 -h .76 - (.36 X .76) = .8464. 

Here are the EMYCIN certainty factor combination functions in Lisp: 

(defun cf-or (a b) 
"Combine the certainty factors for the formula (A or B). 
This is used when two rules support the same conclusion." 
(cond ((and (> a 0) (> b 0)) 

(+ a b (* -1 a b))) 
((and (<a 0) (<b 0)) 
(+ a b (* a b))) 
(t (/ (+ a b) 
(- 1 (min (abs a) (abs b))))))) 

(defun cf-and (a b) 
"Combine the certainty factors for the formula (A and B). " 
(min a b)) 

Certainty factors can be seen as a generalization of truth values. EMYCIN is a 


<a id='page-536'></a>

backward-chaining rule system that combines certainty factors according to the 
functions laid out above. But if we only used the certainty factors true and f al se, 
then EMYCIN would behave exactly like Prolog, returning only answers that are definitely 
true. It is only when we provide fractional certainty factors that the additional 
EMYCIN mechanism makes a difference. 

Truth values actually serve two purposes in Prolog. They determine the final 
answer, yes, but they also determine when to cut off search: if any one of the 
premises of a rule is false, then there is no sense looking at the other premises. If 
in EMYCIN we only cut off the search when one of the premises was absolutely false, 
then we might have to search through a lot of rules, only to yield answers with very 
low certainty factors. Instead, EMYCIN arbitrarily cuts off the search and considers a 
premise false when it has a certainty factor below .2. The following functions support 
this arbitrary cutoff point: 

(defconstant cf-cut-off 0.2 
"Below this certainty we cut off search.") 

(defun true-p (cf) 
"Is this certainty factor considered true?" 
(and (cf-p cf) (> cf cf-cut-off))) 

(defun false-p (cf) 
"Is this certainty factor considered false?" 
(and (cf-p cf) (< cf (- cf-cut-off 1.0)))) 

(defun cf-p (x) 
"Is X a valid numeric certainty factor?" 
(and (numberp x) (<= false . true))) 

&#9635; Exercise 16.1 [m] Suppose you read the headline "Elvis Alive in Kalamazoo" in a 
tabloid newspaper to which you attribute a certainty factor of .01. If you combine certainties 
using EMYCIN'S combination rule, how many more copies of the newspaper 
would you need to see before you were .95 certain Elvis is alive? 

16.2 Caching Derived Facts 
The second thing that makes EMYCIN different from Prolog is that EMYCIN caches all 
the facts it derives in a data base. When Prolog is asked to prove the same goal twice, 
it performs the same computation twice, no matter how laborious. EMYCIN performs 
the computation the first time and just fetches it the second time. 


<a id='page-537'></a>
We can implement a simple data base by providing three functions: put - db to add 
an association between a key and a value, get-db to retrieve a value, and cl ear-db 
to empty the data base and start over: 

(let ((db (make-hash-table :test #'equal))) 
(defun get-db (key) (gethash key db)) 
(defun put-db (key val) (setf (gethash key db) val)) 
(defun clear-db () (clrhash db))) 

This data base is general enough to hold any association between key and value. 
However, most of the information we will want to store is more specific. EMYCIN 
is designed to deal with objects (or instances) and attributes (or parameters) of those 
objects. For example, each patient has a name parameter. Presumably, the value of 
this parameter will be known exactly. On the other hand, each microscopic organism 
has an i denti ty parameter that is normally not known at the start of the consultation. 
Applying the rules will lead to several possible values for this parameter, each 
with its own certainty factor. In general, then, the data base will have keys of the 
form (parameter instance) with values of the form ((vah cf\) (vah c/2)...). In the 
following code, get - va1 s returns the Ust of value/cf pairs for a given parameter and 
instance, get-cf returns the certainty factor for a parameter/instance/value triplet, 
and upda te - cf changes the certainty factor by combining the old one with a new one. 
Note that the first time update-cf is called on a given parameter/instance/value 
triplet, get-cf will return un known (zero). Combining that with the given cf yields cf 
itself. Also note that the data base has to be an equal hash table, because the keys 
may include freshly consed lists. 

(defun get-vals (parm inst) 
"Return a list of (val cf) pairs for this (parm inst)." 
(get-db (list parm inst))) 

(defun get-cf (parm inst val) 
"Look up the certainty factor or return unknown." 
(or (second (assoc val (get-vals parm inst))) 

unknown)) 

(defun update-cf (parm inst val cf) 
"Change the certainty factor for (parm inst is val), 
by combining the given cf with the old. " 
(let ((new-cf (cf-or cf (get-cf parm inst val)))) 

(put-db (list parm inst) 
(cons (list val new-cf) 
(remove val (get-db (list parm inst)) 
:key #*first))))) 

The data base holds all information related to an instance of a problem. For example. 


<a id='page-538'></a>

in the medical domain, the data base would hold all information about the current 
patient. When we want to consider a new patient, the data base is cleared. 

There are three other sources of information that cannot be stored in this data 
base, because they have to be maintained from one problem to the next. First, the 
rule base holds all the rules defined by the expert. Second, there is a structure to 
define each parameter; these are indexed under the name of each parameter. Third, 
we shall see that the flow of control is managed in part by a list ofcontexts to consider. 
These are structures that will be passed to the myci . function. 

16.3 Asking Questions 
The third way that EMYCIN differs from Prolog is in providing an automatic means of 
asking the user questions when answers cannot be derived from the rules. This is not 
a fundamental difference; after all, it is not too hard to write Prolog rules that print 
a query and read a reply. EMYCIN lets the knowledge-base designer write a simple 
declaration instead of a rule, and will even assume a default declaration if none is 
provided. The system also makes sure that the same question is never asked twice. 

The following function ask-val s prints a query that asks for the parameter of an 
instance, and reads from the user the value or a list of values with associated certainty 
factors. The function first looks at the data base to make sure the question has not 
been asked before. It then checks each value and certainty factor to see if each is of 
the correct type, and it also allows the user to ask certain questions. A ? reply will 
show what type answer is expected. Rul e will show the current rule that the system 
is working on. Why also shows the current rule, but it explains in more detail what the 
system knows and is trying to find out. Finally, hel . prints the following summary: 

(defconstant help-string 

"~&Type one of the following: 

? - to see possible answers for this parameter 

rule - to show the current rule 

why - to see why this question is asked 

help - to see this list 

xxx - (for some specific xxx) if there is a definite answer 

(XXX .5 yyy .4) - If there are several answers with 
different certainty factors.") 
Here is a s k - va 1 s. Note that the why and rule options assume that the current rule has 
been stored in the data base. The functions pri nt-why, parm-type, and check- repl y 
will be defined shortly. 


<a id='page-539'></a>
(defun ask-vals (parm inst) 
"Ask the user for the value(s) of inst's parm parameter, 
unless this has already been asked. Keep asking until the 
user types UNKNOWN (return nil) or a valid reply (return t)." 
(unless (get-db '(asked ,parm .inst)) 

(put-db '(asked .parm .inst) t) 
(loop 
(let ((ans (prompt-and-read-vals parm inst))) 

(case ans 
(help (format t help-string)) 
(why (print-why (get-db 'current-rule) parm)) 
(rule (princ (get-db 'current-rule))) 
((unk unknown) (RETURN nil)) 
(? (format t "~&A ~a must be of type ~a" 

parm (parm-type parm)) nil) 

(t (if (check-reply ans parm inst) 
(RETURN t) 
(format t "~&I1legal reply. ~ 

Type ? to see legal ones.")))))))) 

The following is prompt - and - read- va 1 s, the function that actually asks the query and 

reads the reply. It basically calls format to print a prompt and read to get the reply, but 

there are a few subtleties. First, it calls finish- output. Some Lisp implementations 

buffer output on a line-by-line basis. Since the prompt may not end in a newline, 

f i ni sh - output makes sure the output is printed before the reply is read. 

So far, all the code that refers to a parm is really referring to the name of a 
parameter - a symbol. The actual parameters themselves will be implemented as 
structures. We use get-parm to look up the structure associated with a symbol, and 
the selector functions parm-prompt to pick out the prompt for each parameter and 
pa rm- reader to pick out the reader function. Normally this will be the function read, 
but read -1 i ne is appropriate for reading string-valued parameters. 

The macro def parm (shown here) provides a way to define prompts and readers 
for parameters. 

(defun prompt-and-read-vals (parm inst) 
"Print the prompt for this parameter (or make one up) and 
read the reply." 
(fresh-line) 
(format t (parm-prompt (get-parm parm)) (inst-name inst) parm) 
(princ " ") 
(finish-output) 
(funcall (parm-reader (get-parm parm)))) 


<a id='page-540'></a>

(defun inst-name (inst) 
"The name of this instance." 
The stored name is either like (("Jan Doe" 1.0)) or nil 
(or (first (first (get-vals 'name inst))) 
inst)) 

The function check- repl y uses parse - repl y to convert the user's reply into a canonical 
form, and then checks that each value is of the right type, and that each certainty 
factor is valid. If so, the data base is updated to reflect the new certainty factors. 

(defun check-reply (reply parm inst) 
"If reply is valid for this parm, update the DB. 
Reply should be a val or (vail cfl val2 cf2 ...) . 
Each val must be of the right type for this parm." 
(let ((answers (parse-reply reply))) 

(when (every #'(lambda (pair) 
(and (typep (first pair) (parm-type parm)) 
(cf-p (second pair)))) 
answers) 
Add replies to the data base 
(dolist (pair answers) 
(update-cf parm inst (first pair) (second pair))) 
answers))) 

(defun parse-reply (reply) 
"Convert the reply into a list of (value cf) pairs." 
(cond ((null reply) nil) 

((atom reply) *((,reply .true))) 
(t (cons (list (first reply) (second reply)) 
(parse-reply (rest2 reply)))))) 

Parameters are implemented as structures with six slots: the name (a symbol), the 
context the parameter is for, the prompt used to ask for the parameter's value, 
a Boolean that tells if we should ask the user before or after using rules, a type 
restriction describing the legal values, and finally, the function used to read the 
value of the parameter. 

Parameters are stored on the property list of their names under the pa rm property, 
so getting the pa rm- type of a name requires first getting the parm structure, and then 
selecting the type restriction field. By default, a parameter is given type t, meaning 
that any value is valid for that type. We also define the type yes/no, which comes in 
handy for Boolean parameters. 

We want the default prompt to be "What is the PARM of the INST?" But most 
user-defined prompts will want to print the inst, and not the parm. To make it easy 
to write user-defined prompts, prompt-and-read-vals makes the instance be the 
first argument to the format string, with the parm second. Therefore, in the default 


<a id='page-541'></a>
prompt we need to use the format directive " ~*" to skip the instance argument, and 
"'^2:*" to back up two arguments to get back to the instance. (These directives are 
common in cerror calls, where one list of arguments is passed to two format strings.) 

defparm is a macro that calls new-parm, the constructor function defined in the 
parm structure, and stores the resulting structure under the parm property of the 
parameter's name. 

(defstruct (parm (.-constructor 
new-parm (name &optional context type-restrict!on 

prompt ask-first reader))) 
name (context nil) (prompt "~&What is the ~*~a of ~2:*~a?") 
(ask-first nil) (type-restriction t) (reader 'read)) 

(defmacro defparm (parm &rest args) 
"Define a parameter." 
'(setf (get *,parm *parm) (apply #*new-parm *,parm *.args))) 

(defun parm-type (parm-name) 
"What type is expected for a value of this parameter?" 
(parm-type-restriction (get-parm parm-name))) 

(defun get-parm (parm-name) 
"Look up the parameter structure with this name." 
If there is none, make one 
(or (get parm-name 'parm) 
(setf (get parm-name 'parm) (new-parm parm-name)))) 

(deftype yes/no () '(member yes no)) 

16.4 Contexts Instead of Variables 
Earlier we gave an equation relating EMYCIN to Prolog. That equation was not quite 
correct, because EMYCIN lacks one of Prolog's most important features: the logic 
variable. Instead, EMYCIN uses contexts. So the complete equation is: 

EMYCIN = Prolog + uncertainty -f caching -f questions + explanations 
-f contexts - variables 

A context is defined by the designers of MYCIN as a situation within which the 
program reasons. But it makes more sense to think of a context simply as a data 
type. So the list of contexts supplied to the program will determine what types of 
objects can be reasoned about. The program keeps track of the most recent instance 
of each type, and the rules can refer to those instances only, using the name of the 


<a id='page-542'></a>

type. In our version of MYCIN, there are three types or contexts: patients, cultures, 
and organisms. Here is an example of a rule that references all three contexts: 

(defrule 52 

if (site culture is blood) 
(gram organism is neg) 
(morphology organism is rod) 
(burn patient is serious) 

then .4 
(identity organism is Pseudomonas)) 

Ignoring certainty factors for the moment, this MYCIN rule is equivalent to a Prolog 
rule of the form: 

(<- (identity ?o ?pseudomonas) 

(and (culture ?c) (site ?c blood) 
(organism ?o) (gram ?o neg) (morphology ?o rod) 
(patient ?p) (burn ?p serious))) 

The context mechanism provides sufficient flexibility to handle many of the cases 
that would otherwise be handled by variables. One important thing that cannot 
be done is to refer to more than one instance of the same context. Only the most 
recent instance can be referred to. Contexts are implemented as structures with the 
following definition: 

(defstruct context 
"A context is a sub-domain, a type." 
name (number 0) initial-data goals) 

(defmacro defcontext (name &optional initial-data goals) 
"Define a context." 

'(make-context :name '.name :initial-data *.initial-data 
igoals '.goals)) 
The name field is something like patient or organism. Instances of contexts are 
numbered; the number field holds the number of the most recent instance. Each 
context also has two lists of parameters. The i ni ti al -data parameters are asked for 
when each instance is created. Initial data parameters are normally known by the 
user. For example, a doctor will normally know the patient's name, age, and sex, and 
as a matter of training expects to be asked these questions first, even if they don't 
factor into every case. The goal parameters, on the other hand, are usually unknown 
to the user. They are determined through the backward-chaining process. 

The following function creates a new instance of a context, writes a message, and 
stores the instance in two places in the data base: under the key current -i nstance. 


<a id='page-543'></a>
and also under the name of the context. The contexts form a tree. In our example, 
the pa ti ent context is the root of the tree, and the current patient is stored in the data 
base under the key pati ent. The next level of the tree is for cultures taken from the 
patient; the current culture is stored under the cul ture key. Finally, there is a level 
for organisms found in each culture. The current organism is stored under both the 
organi sm and current -i nstance keys. The context tree is shown in figure 16.2. 

(defun new-instance (context) 
"Create a new instance of this context." 
(let ((instance (format nil "~a-~d" 

(context-name context) 
(incf (context-number context))))) 

(format t "~& ~a ~&" instance) 
(put-db (context-name context) instance) 
(put-db 'current-instance instance))) 

Patient: Sylvia Fischer 

CULTURE-1 CULTURE-2 

ORGANISM-1 ORGANISM-2 

Figure 16.2: A Context Tree 

16.5 Backward-Chaining Revisited 
Now that we have seen how EMYCIN is different from Prolog, we are ready to tackle 
the way in which it is the same: the backward-chaining rule interpreter. Like Prolog, 
EMYCIN is given a goal and applies rules that are appropriate to the goal. Applying a 
rule means treating each premise of the rule as a goal and recursively applying rules 
that are appropriate to each premise. 


<a id='page-544'></a>

There are still some remaining differences. In Prolog, a goal can be any expression, 
and appropriate rules are those whose heads unify with the goal. If any appropriate 
rule succeeds, then the goal is known to be true. In EMYCIN, a rule might give a goal 
a certainty of .99, but we still have to consider all the other rules that are appropriate 
to the goal, because they might bring the certainty down below the cutoff threshold. 
Thus, EMYCIN always gathers all evidence relating to a parameter/instance pair first, 
and only evaluates the goal after all the evidence is in. For example, if the goal was 
(temp pa ti ent > 98.6), EMYCIN would first evaluate all rules with conclusions about 
the current patient's temperature, and only then compare the temperature to 98.6. 

Another way of looking at it is that Prolog has the luxury of searching depth-first, 
because the semantics of Prolog rules is such that if any rule says a goal is true, then it 
is true. EMYCIN must search breadth-first, because a goal with certainty of .99 might 
turn out to be false when more evidence is considered. 

We are now ready to sketch out the design of the EMYCIN rule interpreter: To 
fi nd-out a parameter of an instance: If the value is already stored in the data base, 
use the known value. Otherwise, the two choices are using the rules or asking the 
user. Do these in the order specified for this parameter, and if the first one succeeds, 
don't bother with the second. Note that ask-val s (defined above) will not ask the 
same question twice. 

To use - rul es, find all the rules that concern the given parameter and evaluate 
them with use - rul e. After each rule has been tried, if any of them evaluate to true, 
then succeed. 

To use - rul e a rule, first check if any of the premises can be rejected outright. If 
we did not have this check, then the system could start asking the user questions that 
were obviously irrelevant. So we waste some of the program's time (checking each 
premise twice) to save the more valuable user time. (The function eval -condi ti on 
takes an optional argument specifying if we should recursively ask questions in trying 
to accept or reject a condition.) 

If no premise can be rejected, then evaluate each premise in turn with 
eval uate- condi ti on, keeping track of the accumulated certainty factor with cf - and 
(which is currently just mi n), and cutting off evaluation when the certainty factor 
drops below threshold. If the premises evaluate true, then add the conclusions to 
the data base. The calling sequence looks like this. Note that the recursive call to 
f i nd - out is what enables chaining to occur: 

f i nd - out ; To find out a parameter for an instance: 

get- db ; See if it is cached in the data base 
a s k-V a 1 s ; See if the user knows the answer 
use - rul es ; See if there is a rule for it: 
reject-premise ; See if the rule is outright false 
satisfy-premises ; Or see if each condition is true: 
eval-condition ; Evaluate each condition 
f i nd - out ; By finding the parameter's values 


<a id='page-545'></a>
Before showing the interpreter, here is the structure definition for rules, along with 
the functions to maintain a data base of rules: 

(defstruct (rule (:print-function print-rule)) 
number premises conclusions cf) 

(let ((rules (make-hash-table))) 

(defun put-rule (rule) 
"Put the rule in a table, indexed under each 
parm in the conclusion." 
(dolist (concl (rule-conclusions rule)) 

(push rule (gethash (first concl) rules))) 
rule) 

(defun get-rules (parm) 
"A list of rules that help determine this parameter." 
(gethash parm rules)) 

(defun clear-rules () (clrhash rules))) 

Here, then, is the interpreter, f i nd-out. It can find out the value(s) of a parameter 
three ways. First, it looks to see if the value is already stored in the data base. Next, 
it tries asking the user or using the rules. The order in which these two options are 
tried depends on the parm-ask-first property of the parameter. Either way, if an 
answer is determined, it is stored in the data base. 

(defun find-out (parm &optional (inst (get-db 'current-instance))) 
"Find the value(s) of this parameter for this instance, 
unless the values are already known. 
Some parameters we ask first; others we use rules first." 
(or (get-db '(known .parm .inst)) 

(put-db '(known .parm .inst) 

(if (parm-ask-first (get-parm parm)) 
(or (ask-vals parm inst) (use-rules parm)) 
(or (use-rules parm) (ask-vals parm inst)))))) 

(defun use-rules (parm) 
"Try every rule associated with this parameter. 
Return true if one of the rules returns true." 
(some #'true-p (mapcar #'use-rule (get-rules parm)))) 


<a id='page-546'></a>

(defun use-rule (rule) 
"Apply a rule to the current situation." 
;; Keep track of the rule for the explanation system: 
(put-db 'current-rule rule) 
;; If any premise is known false, give up. 
;; If every premise can be proved true, then 

draw conclusions (weighted with the certainty factor), 
(unless (some #'reject-premise (rule-premises rule)) 
(let ((cf (satisfy-premises (rule-premises rule) true))) 
(when (true-p cf) 
(dolist (conclusion (rule-conclusions rule)) 
(conclude conclusion (* cf (rule-cf rule)))) 
cf)))) 

(defun satisfy-premises (premises cf-so-far) 
"A list of premises is satisfied if they are all true. 
A combined cf is returned." 
;; cf-so-far is an accumulator of certainty factors 
(cond ((null premises) cf-so-far) 

((not (true-p cf-so-far)) false) 

(t (satisfy-premises 
(rest premises) 
(cf-and cf-so-far 

(eval-condition (first premises))))))) 

The function eval - condition evaluates a single condition, returning its certainty 
factor. If f i nd - out - . is true, it first calls f i nd - out, which may either query the user 
or apply appropriate rules. If f i nd-out-p is false, it evaluates the condition using 
the current state of the data base. It does this by looking at each stored value for 
the parameter/instance pair and evaluating the operator on it. For example, if the 
condition is (temp patient > 98.6) and the values for temp for the current patient 
are((98 .3) (99 .6) (100 .1)), then eval-condition will test each of the values 
98,99, and 100 against 98.6 using the > operator. This test will succeed twice, so the 
resulting certainty factor is .6 -h .1 = .7. 

The function reject -premi se is designed as a quick test to eliminate a rule. As 
such, it calls eva 1- condi 11 on with f 1 nd - out - . nil, so it will reject a premise only if it 
is clearly false without seeking additional information. 

