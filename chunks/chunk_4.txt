If a rule's premises are true, then the conclusions are added to the data base by 
concl ude. Note that is is the only operator allowed in conclusions, is is just an alias 
for equal. 

(defun eval-condition (condition &optional (find-out-p t)) 
"See if this condition is true, optionally using FIND-OUT 
to determine unknown parameters." 
(multiple-value-bind (parm inst op val) 

(parse-condition condition) 


<a id='page-547'></a>
(when find-out-p 
(find-out parm inst)) 
Add up all the (val cf) pairs that satisfy the test 

(loop for pair in (get-vals parm inst) 
when (funcall op (first pair) val) 
sum (second pair)))) 

(defun reject-premise (premise) 
"A premise is rejected if it is known false, without 
needing to call find-out recursively." 
(false-p (eval-condition premise nil))) 

(defun conclude (conclusion cf) 
"Add a conclusion (with specified certainty factor) to DB." 
(multiple-value-bind (parm inst op val) 

(parse-condition conclusion) 
(update-cf parm inst val cf))) 

(defun is (a b) (equal a b)) 

All conditions are of the form: (parameter instance operator value). For example: 
(morphology organism is rod). Thefunctionparse-conditionturnsalistofthis 
form into four values. The trick is that it uses the data base to return the current 
instance of the context, rather than the context name itself: 

(defun parse-condition (condition) 
"A condition is of the form (parm inst op val). 
So for (age patient is 21), we would return 4 values: 
(age patient-1 is 21), where patient-1 is the current patient." 
(values (first condition) 

(get-db (second condition)) 
(third condition) 
(fourth condition))) 

At this point a call like (find-out 'identity Organism-1) would do the right 
thing only if we had somehow entered the proper information on the current patient, 
culture, and organism. The function get - context - da ta makes sure that each context 
is treated in order. First an instance is created, then f i nd-out is used to determine 
both the initial data parameters and the goals. The findings for each goal are printed, 
and the program asks if there is another instance of this context. Finally, we also 
need a top-level function, emycin, which just clears the data base before calling 
get-context-data. 


<a id='page-548'></a>

(defun emycin (contexts) 
"An Expert-System Shell. Accumulate data for instances of each 
context, and solve for goals. Then report the findings." 
(clear-db) 
(get-context-data contexts)) 

(defun get-context-data (contexts) 
"For each context, create an instance and try to find out 
required data. Then go on to other contexts, depth first, 
and finally ask if there are other instances of this context." 
(unless (null contexts) 

(let* ((context (first contexts)) 

(inst (new-instance context))) 
(put-db 'current-rule 'initial) 
(mapc #'find-out (context-initial-data context)) 
(put-db 'current-rule 'goal) 
(mapc #'find-out (context-goals context)) 
(report-findings context inst) 
(get-context-data (rest contexts)) 
(when (y-or-n-p "Is there another ~a?" 

(context-name context)) 
(get-context-data contexts))))) 

16.6 Interacting with the Expert 
At this point all the serious computational work is done: we have defined a backward-
chaining rule mechanism that deals with uncertainty, caching, questions, and contexts. 
But there is still quite a bit of work to do in terms of input/output interaction. A 
programming language needs only to interface with programmers, so it is acceptable 
to make the programmer do all the work. But an expert-system shell is supposed to 
alleviate (if not abolish) the need for programmers. Expert-system shells really have 
two classes of users: the experts use the shell when they are developing the system, 
and the end users or clients use the resulting expert system when it is completed. 
Sometimes the expert can enter knowledge directly into the shell, but more often 
it is assumed the expert will have the help of a knowledge engineer - someone who is 
trained in the use of the shell and in eliciting knowledge, but who need not be either 
an expert in the domain or an expert programmer. 

In our version of EMYCIN, we provide only the simplest tools for making the 
expert's job easier. The macros defcontext and defparm, defined above, are a little 
easier than calling make - context and make - pa rm explicitly, but not much. The macro 
def rul e defines a rule and checks for some obvious errors: 


<a id='page-549'></a>

(defmacro defrule (number &body body) 
"Define a rule with conditions, a certainty factor, and 
conclusions. Example: (defrule ROOl if ... then .9 ...) " 
(assert (eq (first body) 'if)) 
(let* ((then-part (member 'then body)) 

(premises (Idiff (rest body) then-part)) 
(conclusions (rest2 then-part)) 
(cf (second then-part))) 

Do some error checking: 
(check-conditions number premises 'premise) 
(check-conditions number conclusions 'conclusion) 
(when (not (cf-p cf)) 

(warn "Rule "a: Illegal certainty factor: "a" number cf)) 
Now build the rule: 
*(put-rule 
(make-rule :number ',number :cf ,cf:premises ',premises 
:conclusions ',conclusions)))) 

The function check-condi ti ons makes sure that each rule has at least one premise 
and conclusion, that each condition is of the right form, and that the value of the 
condition is of the right type for the parameter. It also checks that conclusions use 
only the operator i s: 

(defun check-conditions (rule-num conditions kind) 
"Warn if any conditions are invalid." 
(when (null conditions) 

(warn "Rule "a: Missing "a" rule-num kind)) 
(dolist (condition conditions) 
(when (not (consp condition)) 
(warn "Rule ~a: Illegal '"a: ''a" rule-num kind condition)) 
(multiple-value-bind (parm inst op val) 

(parse-condition condition) 
(declare (ignore inst)) 
(when (and (eq kind 'conclusion) (not (eq op 'is))) 

(warn "Rule ~a: Illegal operator (~a) in conclusion: "a" 
rule-num op condition)) 
(when (not (typep val (parm-type parm))) 
(warn "Rule ~a: Illegal value (~a) in ~a: ~a" 
rule-num val kind condition))))) 

The real EMYCIN had an interactive environment that prompted the expert for each 
context, parameter, and rule. Randall Davis (1977, 1979, Davis and Lenat 1982) 
describes the TEIRESIAS program, which helped experts enter and debug rules. 


<a id='page-550'></a>

16.7 Interacting with the Client 
Once the knowledge is in, we need some way to get it out. The client wants to run 
the system on his or her own problem and see two things: a solution to the problem, 
and an explanation of why the solution is reasonable. EMYCIN provides primitive 
facilities for both of these. The function report-f i ndi ngs prints information on all 
the goal parameters for a given instance: 

(defun report-findings (context inst) 
"Print findings on each goal for this instance." 
(when (context-goals context) 

(format t "~&Findings for ~a;" (inst-name inst)) 
(dolist (goal (context-goals context)) 

(let ((values (get-vals goal inst))) 
;; If there are any values for this goal, 
;; print them sorted by certainty factor, 
(if values 

(format t "~& ~a:~{~{ ~a (~,3f) "}"}" goal 
(sort (copy-list values) #'> :key #'second)) 
(format t "~& ~a: unknown" goal)))))) 

The only explanation facility our version of EMYCIN offers is a way to see the current 
rule. If the user types rul e in response to a query, a pseudo-EngUsh translation of 
the current rule is printed. Here is a sample rule and its translation: 

(defrule 52 

if (site culture is blood) 
(gram organism is neg) 
(morphology organism is rod) 
(burn patient is serious) 

then .4 
(identity organism is Pseudomonas)) 

Rule 52: 
If 

1) THE SITE OF THE CULTURE IS BLOOD 
2) THE GRAM OF THE ORGANISM IS NEG 
3) THE MORPHOLOGY OF THE ORGANISM IS ROD 
4) THE BURN OF THE PATIENT IS SERIOUS 
Then there is weakly suggestive evidence (0.4) that 
1) THE IDENTITY OF THE ORGANISM IS PSEUDOMONAS 
The function pri nt - rul e generates this translation: 


<a id='page-551'></a>
(defun print-rule (rule &optional (stream t) depth) 
(declare (ignore depth)) 
(format stream "~&Rule ~a:~& If" (rule-number rule)) 
(print-conditions (rule-premises rule) stream) 
(format stream "~& Then "a (~a) that" 

(cf->english (rule-cf rule)) (rule-cf rule)) 
(print-conditions (rule-conclusions rule) stream)) 

(defun print-conditions (conditions &optional 

(stream t) (num 1)) 
"Print a list of numbered conditions." 
(dolist (condition conditions) 

(print-condition condition stream num))) 

(defun print-condition (condition stream number) 
"Print a single condition in pseudo-English." 
(format stream "~& ~d)~{ ~a~}" number 

(let ((parm (first condition)) 
(inst (second condition)) 
(op (third condition)) 
(val (fourth condition))) 

(case val 
(YES '(the ,inst ,op .parm)) 
(NO '(the .inst .op not .parm)) 
(T '(the .parm of the .inst .op .val)))))) 

(defun cf->english (cf) 
"Convert a certainy factor to an English phrase." 
(cond ((= cf 1.0) "there is certain evidence") 

((> cf .8) "there is strongly suggestive evidence") 
((> cf .5) "there is suggestive evidence") 
((> cf 0.0) "there is weakly suggestive evidence") 
((= cf 0.0) "there is NO evidence either way") 
((< cf 0.0) (concatenate 'string (cf->english (- cf)) 

" AGAINST the conclusion")))) 

If the user types why in response to a query, a more detailed account of the same 
rule is printed. First, the premises that are already known are displayed, followed 
by the remainder of the rule. The parameter being asked for will always be the first 
prennse in the remainder of the rule. The current - rul e is stored in the data base by 
use-rul e whenever a rule is applied, but it is also set by get-context -data to the 
atom i ni ti al or goal when the system is prompting for parameters, pri nt-why 
checks for this case as well. Note the use of the pa rt it i on - i f function from [page 256](chapter8.md#page-256). 


<a id='page-552'></a>

(defun print-why (rule parm) 
"Tell why this rule is being used. Print what is known, 
what we are trying to find out, and what we can conclude." 
(format t "~&CWhy is the value of '"a being asked for?]" parm) 
(if (member rule '(initial goal)) 

(format t "~&~a is one of the ~a parameters." 
parm rule) 
(multiple-value-bind (knowns unknowns) 
(partition-if #'(lambda (premise) 
(true-p (eval-condition premise nil))) 
(rule-premises rule)) 

(when knowns 
(format t """Alt is known that:") 
(print-conditions knowns) 
(format t "~&Therefore,")) 

(let ((new-rule (copy-rule rule))) 
(setf (rule-premises new-rule) unknowns) 
(print new-rule))))) 

That completes the definition of emyci n. We are now ready to apply the shell to a 
specific domain, yielding the beginnings of an expert system. 

16.8 MYCIN, A Medical Expert System 
This section applies emycin to MYCIN'S original domain: infectious blood disease. 
In our version of MYCIN, there are three contexts: first we consider a patient, then 
any cultures that have been grown from samples taken from the patient, and finally 
any infectious organisms in the cultures. The goal is to determine the identity of 
each organism. The real MYCIN was more complex, taking into account any drugs 
or operations the patient may previously have had. It also went on to decide the 
real question: what therapy to prescribe. However, much of this was done by 
special-purpose procedures to compute optimal dosages and the like, so it is not 
included here. The original MYCIN also made a distinction between current versus 
prior cultures, organisms, and drugs. All together, it had ten contexts to consider, 
while our version only has three: 

(defun mycin () 
"Determine what organism is infecting a patient." 
(emycin 

(list (defcontext patient (name sex age) ()) 
(defcontext culture (site days-old) ()) 
(defcontext organism () (identity))))) 


<a id='page-553'></a>
These contexts declare that we will first ask each patient's name, sex, and age, and 
each culture's site and the number of days ago it was isolated. Organisms have no 
initial questions, but they do have a goal: to determine the identity of the organism. 

The next step is to declare parameters for the contexts. Each parameter is given 
a type, and most are given prompts to improve the naturalness of the dialogue: 

Parameters for patient: 
(defparm name patient t "Patient's name: " t read-line) 
(defparm sex patient (member male female) "Sex:" t) 
(defparm age patient number "Age:" t) 
(defparm burn patient (member no mild serious) 

"Is ~a a burn patient? If so, mild or serious?" t) 
(defparm compromised-host patient yes/no 
"Is ~a a compromised host?") 

;;; Parameters for culture: 
(defparm site culture (member blood) 
"From what site was the specimen for ~a taken?" t) 
(defparm days-old culture number 
"How many days ago was this culture (~a) obtained?" t) 

;;; Parameters for organism: 
(defparm identity organism 
(member Pseudomonas klebsiel la enterobacteriaceae 
staphylococcus bacteroides streptococcus) 
"Enter the identity (genus) of ~a:" t) 
(defparm gram organism (member acid-fast pos neg) 
"The gram stain of ~a:" t) 
(defparm morphology organism (member rod coccus) 

"Is ~a a rod or coccus (etc.):") 
(defparm aerobicity organism (member aerobic anaerobic)) 
(defparm growth-conformation organism 

(member chains pairs clumps)) 

Now we need some rules to help determine the identity of the organisms. The 
following rules are taken from Shortliffe 1976. The rule numbers refer to the pages 
on which they are listed. The real MYCIN had about 400 rules, dealing with a much 
wider variety of premises and conclusions. 

(clear-rules) 

(defrule 52 

if (site culture is blood) 
(gram organism is neg) 
(morphology organism is rod) 
(burn patient is serious) 

then .4 
(identity organism is Pseudomonas)) 


<a id='page-554'></a>

(defrule 71 

if (gram organism is pos) 
(morphology organism is coccus) 
(growth-conformation organism is clumps) 

then .7 
(identity organism is staphylococcus)) 

(defrule 73 

if (site culture is blood) 
(gram organism is neg) 
(morphology organism is rod) 
(aerobicity organism is anaerobic) 

then .9 
(identity organism is bacteroides)) 

(defrule 75 

if (gram organism is neg) 
(morphology organism is rod) 
(compromised-host patient is yes) 

then .6 
(identity organism is Pseudomonas)) 

(defrule 107 

if (gram organism is neg) 
(morphology organism is rod) 
(aerobicity organism is aerobic) 

then .8 
(identity organism is enterobacteriaceae)) 

(defrule 165 

if (gram organism is pos) 
(morphology organism is coccus) 
(growth-conformation organism is chains) 

then .7 

(identity organism is streptococcus)) 

Here is an example of the program in use: 

> (mycin) 
PATIENT-1 
Patient's name: Sylvia Fischer 
Sex: female 
Age: 27 
CULTURE-1 
From what site was the specimen for CULTURE-1 taken? blood 
How many days ago was this culture (CULTURE-1) obtained? 3 
- ORGANISM-1 
Enter the identity (genus) of ORGANISM-1: unknown 
The gram stain of ORGANISM-1: ? 


<a id='page-555'></a>

A GRAM must be of type (MEMBER ACID-FAST POS NEG) 
The gram stain of ORGANISM-1: neg 

The user typed ? to see the list of valid responses. The dialog continues: 

Is ORGANISM-1 a rod or coccus (etc.): rod 
What is the AEROBICITY of ORGANISM-1? why 
[Why is the value of AEROBICITY being asked for?] 
It is known that: 

1) THE GRAM OF THE ORGANISM IS NEG 
2) THE MORPHOLOGY OF THE ORGANISM IS ROD 
Therefore, 
Rule 107: 
If 

1) THE AEROBICITY OF THE ORGANISM IS AEROBIC 
Then there is suggestive evidence (0.8) that 
1) THE IDENTITY OF THE ORGANISM IS ENTEROBACTERIACEAE 
The user wants to know why the system is asking about the organism's aerobicity. 
The reply shows the current rule, what is already known about the rule, and the fact 
that if the organism is aerobic, then we can conclude something about its identity. In 
this hypothetical case, the organism is in fact aerobic: 

What is the AEROBICITY of ORGANISM-1? aerobic 
Is Sylvia Fischer a compromised host? yes 
Is Sylvia Fischer a burn patient? If so. mild or serious? why 
[Why is the value of BURN being asked for?] 
It is known that: 

1) THE SITE OF THE CULTURE IS BLOOD 
2) THE GRAM OF THE ORGANISM IS NEG 
3) THE MORPHOLOGY OF THE ORGANISM IS ROD 
Therefore, 
Rule 52: 
If 

1) THE BURN OF THE PATIENT IS SERIOUS 
Then there is weakly suggestive evidence (0.4) that 
1) THE IDENTITY OF THE ORGANISM IS PSEUDOMONAS 
Is Sylvia Fischer a burn patient? If so, mild or serious? serious 
Findings for ORGANISM-1: 
IDENTITY: ENTEROBACTERIACEAE (0.800) PSEUDOMONAS (0.760) 

The system used rule 107 to conclude the identity might be enterobacteriaceae. 
The certainty is .8, the certainty for the rule itself, because all the conditions were 
known to be true with certainty. Rules 52 and 75 both support the hypothesis of 
Pseudomonas. The certainty factors of the two rules, .6 and .4, are combined by the 


<a id='page-556'></a>

formula .6 + .4 - (.6 . .4) = .76. After printing the findings for the first organism, 
the system asks if another organism was obtained from this culture: 

Is there another ORGANISM? (Y or N) Y 

ORGANISM-2 

Enter the identity (genus) of ORGANISM-2: unknown 

The gram stain of ORGANISM-2: (neg .8 pos .2) 

Is ORGANISM-2 a rod or coccus (etc.): rod 

What is the AEROBICITY of ORGANISM-2? anaerobic 

For the second organism, the lab test was inconclusive, so the user entered a qualified 
answer indicating that it is probably gram-negative, but perhaps gram-positive. This 
organism was also a rod but was anaerobic. Note that the system does not repeat 
questions that it already knows the answers to. In considering rules 75 and 52 
it already knows that the culture came from the blood, and that the patient is a 
compromised host and a serious burn patient. In the end, rule 73 contributes to the 
bacteroides conclusion, and rules 75 and 52 again combine to suggest Pseudomonas, 
although with a lower certainty factor, because the neg finding had a lower certainty 
factor: 

Findings for ORGANISM-2: 
IDENTITY: BACTEROIDES (0.720) PSEUDOMONAS (0.646) 

Finally, the program gives the user the opportunity to extend the context tree with 
new organisms, cultures, or patients: 

Is there another ORGANISM? (Y or N) . 
Is there another CULTURE? (Y or N) . 
Is there another PATIENT? (Y or N) . 

The set of rules listed above do not demonstrate two important features of the 
system: the ability to backward-chain, and the ability to use operators other than i s 
in premises. 

If we add the following three rules and repeat the case shown above, then evaluating 
rule 75 will back-chain to rule 1, 2, and finally 3 trying to determine if the 
patient is a compromised host. Note that the question asked will be "What is Sylvia 
Fischer's white blood cell count?" and not "Is the white blood cell count of Sylvia 
Fischer < 2.5?" The latter question would suffice for the premise at hand, but it 
would not be as useful for other rules that might refer to the WBC. 

(defparm wbc patient number 
"What is ~a's white blood cell count?") 


<a id='page-557'></a>
(defrule 1 

if (immunosuppressed patient is yes) 

then 1.0 (compromised-host patient is yes)) 

(defrule 2 

if (leukopenia patient is yes) 

then 1.0 (immunosuppressed patient is yes)) 

(defrule 3 

if (wbc patient < 2.5) 

then .9 (leukopenia patient is yes)) 

16.9 Alternatives to Certainty Factors 
Certainty factors are a compromise. The good news is that a system based on rules 
with certainty factors requires the expert to come up with only a small set of numbers 
(one for each rule) and will allow fast computation of answers. The bad news is that 
the answer computed may lead to irrational decisions. 

Certainty factors have been justified by their performance (MYCIN performed as 
well or better than expert doctors) and by intuitive appeal (they satisfy the criteria 
listed on [page 534](chapter16.md#page-534)). However, they are subject to paradoxes where they compute 
bizarre results (as in Exercise 16.1, [page 536](chapter16.md#page-536)). If the rules that make up the knowledge 
base are designed in a modular fashion, then problems usually do not arise, but it is 
certainly worrisome that the answers may be untrustworthy. 

Before MYCIN, most reasoning with uncertainty was done using probability theory. 
The laws of probability - in particular, Bayes's law - provide a well-founded 
mathematical formalism that is not subject to the inconsistencies of certainty factors. 
Indeed, probability theory can be shown to be the only formalism that leads 
to rational behavior, in the sense that if you have to make a series of bets on some 
uncertain events, combining information with probability theory will give you the 
highest expected value for your bets. Despite this, probability theory was largely set 
aside in the mid-1970s. The argument made by Shortliffe and Buchanan (1975) was 
that probability theory required too many conditional probabilities, and that people 
were not good at estimating these. They argued that certainty factors were intuitively 
easier to deal with. Other researchers of the time shared this view. Shaf er, with later 
refinements by Dempster, created a theory of belief functions that, like certainty 
factors, represented a combination of the belief for and against an event. Instead of 
representing an event by a single probability or certainty, Dempster-Shafer theory 
maintains two numbers, which are analagous to the lower and upper bound on the 
probability. Instead of a single number like .5, Dempster-Shafer theory would have 
an interval like [.4,.6] to represent a range of probabilities, A complete lack of knowledge 
would be represented by the range [0,1]. A great deal of effort in the late 1970s 


<a id='page-558'></a>

and early 1980s was invested in these and other nonprobabilistic theories. Another 
example is Zadeh's fuzzy set theory, which is also based on intervals. 

There is ample evidence that people have difficulty with problems involving 
probability. In a very entertaining and thought-provoking series of articles, Tversky 
and Kahneman (1974, 1983, 1986) show how people make irrational choices when 
faced with problems that are quite simple from a mathematical viewpoint. They 
liken these errors in choice to errors in visual perception caused by optical illusions. 
Even trained doctors and statisticians are subject to these errors. 

As an example, consider the following scenario. Adrian and Dominique are to be 
married. Adrian goes for a routine blood test and is told that the results are positive 
for a rare genetic disorder, one that strikes only 1 in 10,000 people. The doctor 
says that the test is 99% accurate - it gives a false positive reading in only 1 in 100 
cases. Adrian is despondent, being convinced that the probability of actually having 
the disease is 99%. Fortunately, Dominique happens to be a Bayesian, and quickly 
reassures Adrian that the chance is more like 1 %. The reasoning is as follows: Take 
10,001 people at random. Of these, only 1 is expected to have the disease. That 
person could certainly expect to test positive for the disease. But if the other 10,000 
people all took the blood test, then 1 % of them, or 100 people would also test positive. 
Thus, the chance of actually having the disease given that one tests positive is 1/101. 
Doctors are trained in this kind of analysis, but unfortunately many of them continue 
to reason more like Adrian than Dominique. 

In the late 1980s, the tide started to turn back to subjective Bayesian probability 
theory. Cheeseman (1985) showed that, while Dempster-Shafer theory looks like 
it can, in fact it cannot help you make better decisions than probability theory. 
Heckerman (1986) re-examined MYCIN'S certainty factors, showing how they could 
be interpreted as probabilities. Judea Pearl's 1988 book is an eloquent defense of 
probability theory. He shows that there are efficient algorithms for combining and 
propagating probabilities, as long as the network of interdependencies does not 
contain loops. It seems likely that uncertain reasoning in the 1990s will be based 
increasingly on Bayesian probability theory. 

16.10 History and References 
The MYCIN project is well documented in Buchanan and Shortliffe 1984. An earlier 
book, Shortliffe 1976, is interesting mainly for historical purposes. Good introductions 
to expert systems in general include Weiss and Kulikowski 1984, Waterman 
1986, Luger and Shibblefield 1989, and Jackson 1990. 

Dempster-Shafer evidence theory is presented enthusiastically in Gordon and 
Shortliffe 1984 and in a critical light in Pearl 1989/1978. Fuzzy set theory is presented 
in Zadeh 1979 and Dubois and Prade 1988. 


<a id='page-559'></a>
Pearl (1988) captures most of the important points that lead to the renaissance 
of probability theory. Shafer and Pearl 1990 is a balanced collection of papers on all 
kinds of uncertain reasoning. 
16.11 Exercises 
&#9635; Exercise 16.2 [s] Suppose the rule writer wanted to be able to use symbolic certainty 
factors instead of numbers. What would you need to change to support rules like 
this: 
(defrule(defrule 
100 if 
101 if 
.. . 
.. . 
then true ... ) 
then probably ... ) 

&#9635; Exercise 16.3 [m] Change prompt-and-read-va l s so that it gives a better prompt 
for parameters of type yes/no. 

&#9635; Exercise 16.4 [m] Currently, the rule writer can introduce a new parameter without 
defining it first. That is handy for rapid testing, but it means that the user of the system 
won't be able to see a nice English prompt, nor ask for the type of the parameter. In 
addition, if the rule writer simply misspells a parameter, it will be treated as a new 
one. Make a simple change to fix these problems. 

&#9635; Exercise 16.5 [d] Write rules in a domain you are an expert in, or find and interview 
an expert in some domain, and write down rules coaxed from the expert. Evaluate 
your resulting system. Was it easier to develop your system with EMYCI N than it 
would have been without it? 

&#9635; Exercise 16.6 [s] It is said that an early version of MYCI N asked if the patient was 
pregnant, even though the patient was male. Write a rule that would fix this problem. 

&#9635; Exercise 16.7 [m] To a yes/no question, what is the difference between yes and (no 
-1) ? What does this suggest? 

&#9635; Exercise 16.8 [m] What happens if the user types why to the prompt about the 
patient's name? What happens if the expert wants to have more than one context 
with a name parameter? If there is a problem, fix it. 


<a id='page-560'></a>

The remaining exercises discuss extensions that were in the original EMYCIN, but 
were not implemented in our version. Implementing all the extensions will result in a 
system that is very close to the full power of EMYCIN. These extensions are discussed 
in chapter 3 of Buchanan and Shortliffe 1984. 

&#9635; Exercise 16.9[h] Add a spelling corrector to ask-vals. If the user enters an invalid 
reply, and the parameter type is a member expression, check if the reply is "close" in 
spelling to one of the valid values, and if so, use that value. That way, the user can type 
just entero instead of enterobacter i aceae. You may experiment with the definition 
of "close," but you should certainly allow for prefixes and at least one instance of a 
changed, missing, inserted, or transposed letter. 

&#9635; Exercise 16.10 [m] Indent the output for each new branch in the context tree. In 
other words, have the prompts and findings printed like this: 

RATIENT-1 
Patient's name: Sylvia Fischer 
Sex: female 
Age: 27 

CULTURE-1 
From what site was the specimen for CULTURE-1 taken? blood 
How many days ago was this culture (CULTURE-1) obtained? 3 

ORGANISM-1 
Enter the identity (genus) of ORGANISM-1: unknown 
The gram stain of ORGANISM-1: neg 

Findings for ORGANISM-1: 
IDENTITY: ENTEROBACTERIACEAE (0.800) PSEUDOMONAS (0.760) 
Is there another ORGANISM? (Y or N) . 
Is there another CULTURE? (Y or N) . 
Is there another PATIENT? (Y or N) . 

&#9635; Exercise 16.11 [h] We said that our emycin looks at all possible rules for each 
parameter, because there is no telling how a later rule may affect the certainty factor. 
Actually, that is not quite true. If there is a rule that leads to a conclusion with 
certainty 1, then no other rules need be considered. This was called a unity path. 
Modify the program to look for unity paths first. 

&#9635; Exercise 16.12 [m] Depending on whether a parameter is in i ni ti al -data or not, 
all the relevant rules are run either before or after asking the user for the value 
of the parameter. But there are some cases when not all initial data parameters 


<a id='page-561'></a>

should be asked for. As an example, suppose that identity and gram were initial 
data parameters of organi sm. If the user gave a positive answer for i denti ty, then it 
would be wasteful to ask for the gram parameter, since it could be determined directly 
from rules. After receiving complaints about this problem, a system of antecedent 
rules was developed. These rules were always run first, before asking questions. 
Implement antecedent rules. 

&#9635; Exercise 16.13 [h] It is useful to be able to write default rules that fill in a value after 
all other rules have failed to determine one. A default rule looks like this: 

(defrule . if (parm inst unknown) then (parm inst is default)) 

It may also have other conjuncts in the premise. Beside details like writing the 
unknown operator, the difficult part is in making sure that these rules get run at the 
right time (after other rules have had a chance to fill in the parameter), and that 
infinite loops are avoided. 

&#9635; Exercise 16.14 [h] The context tree proved to be a limitation. Eventually, the need 
arose for a rule that said, 'Tf any of the organisms in a culture has property X, then the 
culture has property Y." Implement a means of checking for some or every instance 
of a context. 

&#9635; Exercise 16.15 [m] As the rule base grew, it became increasingly hard to remember 
the justification for previous rules. Implement a mechanism that keeps track of the 
author and date of creation of each rule, and allows the author to add documentation 
explaining the rationale for the rule. 

&#9635; Exercise 16.16 [m] It is difficult to come up with the perfect prompt for each parameter. 
One solution is not to insist that one prompt fits all users, but rather to allow 
the expert to supply three different prompts: a normal prompt, a verbose prompt (or 
reprompt) for when the user replies with a ?, and a terse prompt for the experienced 
user. Modify defparm to accommodate this concept, add a command for the user to 
ask for the terse prompts, and change ask-val s to use the proper prompt. 

The remaining exercises cover three additional replies the user can make: how, 
stop, and change. 

&#9635; Exercise 16.17 [d] In addition to why replies, EMYCIN also allowed for how questions. 
The user can ask how the value of a particular parameter/instance pair was determined, 
and the system will reply with a list of rules and the evidence they supplied for 


<a id='page-562'></a>

or against each value. Implement this mechanism. It will require storing additional 
information in the data base. 

&#9635; Exercise 16.18 [m] There was also a stop command that immediately halted the 
session. Implement it. 

&#9635; Exercise 16.19 [d] The original EMYCIN also had a change command to allow the 
user to change the answer to certain questions without starting all over. Each question 
was assigned a number, which was printed before the prompt. The command change, 
followed by a list of numbers, causes the system to look up the questions associated 
with each number and delete the answer to these questions. The system also throws 
away the entire context tree and all derived parameter values. At that point the 
entire consultation is restarted, using only the data obtained from the unchanged 
questions. Although it may seem wasteful to start over from the beginning, it will 
not be wasteful of the user's time, since correct answers will not be asked again. 
Identify what needs to be altered to implement change and make the alterations. 

&#9635; Exercise 16.20 [h] Change the definition of cf - a nd and cf -or to use fuzzy set theory 
instead of certainty factors. Do the same for Dempster-Shafer theory. 

16.12 Answers 
Answer 16.1 Because EMYCIN assumes independence, each reading of the same 
headline would increase the certainty factor. The following computation shows 
that 298 more copies would be needed to reach .95 certainty. A more sophisticated 
reasoner would realize that multiple copies of a newspaper are completely dependent 
on one another, and would not change the certainty with each new copy. 

> (loop for cf = .01 then (cf-or .01 cf) 
until (> cf .95) 
count t) 

298 

Answer 16.2 Thedef rule expands to (make-rule rnumber .01 :cf true ...); 
that is, the certainty factor is unquoted, so it is already legal to use true as a certainty 
factor! To support probabl y and other hedges, just define new constants. 


<a id='page-563'></a>
Answer 16.4 Just make the defauh parameter type be ni 1 (by changing t to ni1 
in parm-type). Then any rule that uses an undefined parameter will automatically 
generate a warning. 

Answer 16.6 

(defrule 4 
if (sex patient is male) 
then -1 (pregnant patient is yes)) 

Answer 16.7 Logically, there should be no difference, but to EMYCIN there is a big 
difference. EMYCIN wouldnotcomplainif you answered (yes 1 no 1). This suggests 
that the system should have some way of dealing with mutually exclusive answers. 
One way would be to accept only yes responses for Boolean parameters, but have the 
input routine translate no to (yes -1) and (no cf) to (yes 1-cf). Another possibility 
would be to have u pd a t e -cf check to see if any certainty factor on a mutually exclusive 
value is 1, and if so, change the other values to -1. 

Answer 16.18 Add the clause (stop (throw 'stop nil)) to the case statement 
in ask-vals and wrap a (catch 'stop ...) around the code in emycin. 


## Chapter 17
<a id='page-564'></a>

Line-Diagram 
Labeling by Constraint 
Satisfaction 

It is wrong to think of Waltz's work only as a 
statement of the epistemology of line drawings of 
polyhedra. Instead I think it is an elegant case study 
of a paradigm we can expect to see again and again. 

- Patrick Winston 

The Psychology of Computer Vision (1975) 

I I 1 his book touches only the areas of AI that deal with abstract reasoning. There is another 

I side of AI, the field of robotics, that deals with interfacing abstract reasoning with the real 

JL world through sensors and motors. A robot receives input from cameras, microphones, 
sonar, and touch-sensitive devices, and produces "ouput" by moving its appendages or generating 
sounds. The real world is a messier place than the abstract worlds we have been covering. 
A robot must deal with noisy data, faulty components, and other agents and events in the world 
that can affect changes in the environment. 


<a id='page-565'></a>

Computer vision is the subfield of robotics that deals with interpreting visual 
information. Low-level vision takes its input directly from a camera and detects 
lines, regions and textures. We will not be concerned with this. High-level vision 
uses the findings of the low-level component to build a three-dimensional model of 
the objects depicted in the scene. This chapter covers one small aspect of high-level 
vision. 

17.1 The Line-Labeling Problem 
In this chapter we look at the line-diagram labeling problem: Given a list of lines and 
the Vertexes at which they intersect, how can we determine what the lines represent? 
For example, given the nine lines in figure 17.1, how can we interpret the diagram as 
a cube? 

Figure 17.1: A Cube 

Before we can arrive at an interpretation, we have to agree on what the candidates 
are. After all, figure 17.1 could be just a hexagon with three lines in the middle. For 
the purposes of this chapter, we will consider only diagrams that depict one or more 
po/y/zedra - three-dimensional solid figures whose surfaces are flat faces bounded by 
straight lines. In addition, we will only allow trihedral VERTEXES. That is, each vertex 
must be formed by the intersection of three faces, as in the corner of a cube, where 
the top, front, and side of the cube come together. A third restriction on diagrams is 
that no so-called accidental Vertexes are allowed. For example, figure 17.1 might be 
a picture of three different cubes hanging in space, which just happen to line up so 
that the edge of one is aligned with the edge of another from our viewpoint. We will 
assume that this is not the case. 


<a id='page-566'></a>

Given a diagram that fits these three restrictions, our goal is to identify each line, 
placing it in one of three classes: 

1. A convex line separates two visible faces of a polyhedron such that a line from 
one face to the other would lie inside the polyhedron. It will be marked with a 
plus sign: -h. 
2. A concave line separates two faces of two polyhedra such that a line between 
the two spaces would pass through empty space. It will be marked with a 
minus sign: -. 
3. A boundary line denotes the same physical situation as a convex line, but the 
diagram is oriented in such a way that only one of the two faces of the polyhedron 
is visible. Thus, the line marks the boundary between the polyhedron 
and the background. It will be marked with an arrow: - Traveling along the 
line from the tail to the point of the arrow, the polyhedron is on the right, and 
the background is on the left. 
Figure 17.2 shows a labeling of the cube using these conventions. Vertex A is 
the near corner of the cube, and the three lines coming out of it are all convex lines. 
Lines GD and DF are concave lines, indicating the junction between the cube and 
the surface on which it is resting. The remaining lines are boundary lines, indicating 
that there is no physical connection between the cube and the background there, but 
that there are other sides of the cube that cannot be seen. 

Figure 17.2: A Line-labeled Cube 

The line-labeling technique developed in this chapter is based on a simple idea. 
First we enumerate all the possible Vertexes, and all the possible labelings for each 


<a id='page-567'></a>

vertex. It turns out there are only four different vertex types in the trihedral polygon 
world. We call them L, Y, W, and . Vertexes, because of their shape. The Y and W 
Vertexes are also known as forks and arrows, respectively. The Vertexes are listed in 
figure 17.3. Each vertex imposes some constraints on the lines that compose it. For 
example, in a W vertex, the middle line can be labeled with a + or - but not with 
an arrow. 

NX ./ \y 

\ 
\ 
1 . 2 
1 

3 

3 

3 
V 2 

\ V 

(V L 1 2) (V Y 1 2 3) (V . 1 2 3) (VWL 23) 

Figure 17.3: The Possible Vertexes and Labels 

Each line connects two Vertexes, so it must satisfy both constraints. This suggests 
a simple algorithm for labeling a diagram based on constraint propagation: First, 
label each vertex with all the possible labelings for the vertex type. An L vertex has 
six possibilities, Y has five, . has four, and W has three. Next, pick a vertex, V. 
Consider a neighboring vertex, . (that is, . and V are connected by a line). . will 
also have a set of possible labelings. If . and V agree on the possible labelings for the 
line between them, then we have gained nothing. But if the intersection of the two 
possibility sets is smaller than V's possibility set, then we have found a constraint on 


<a id='page-568'></a>

the diagram. We adjust . and V's possible labehngs accordingly. Every time we add 
a constraint at a vertex, we repeat the whole process for all the neighboring Vertexes, 
to give the constraint a chance to propagate as far as possible. When every vertex 
has been visited at least once and there are no more constraints to propagate, then 
we are done. 

Figure 17.4 illustrates this process. On the left we start with a cube. All Vertexes 
have all possible labelings, except that we know line GD is concave (-), indicating that 
the cube is resting on a surface. This constrains vertex D in such a way that line DA 
must be convex (+). In the middle picture the constraint on vertex D has propagated 
to vertex A, and in the right-hand picture it propagates to vertex B. Soon, the whole 
cube will be uniquely labeled. 

Figure 17.4: Propagating Constraints 

Many diagrams will be labeled uniquely by this constraint propagation process. 
Some diagrams, however, are ambiguous. They will still have multiple labelings 
after constraint propagation has finished. In this case, we can search for a solution. 
Simply choose an ambiguous vertex, choose one of the possible labelings for that 
vertex, and repeat the constraint propagation/search process. Keep going until the 
diagram is either unambiguous or inconsistent. 

That completes the sketch of the line-labeling algorithm. We are now ready to 
implement a labeling program. It's glossary is in figure 17.5. 

The two main data structures are the di agram and the vertex. It would have been 
possible to implement a data type for 1 i nes, but it is not necessary: lines are defined 
implicitly by the two Vertexes at their end points. 

A diagram is completely specified by its list of Vertexes, so the structure di agram 
needs only one slot. A vertex, on the other hand, is a more complex structure. Each 
vertex has an identifying name (usually a single letter), a vertex type (L, Y, W, or T), a 


<a id='page-569'></a>

Top-Level Functions 
print-labelings Label the diagram by propagating constraints and then searching. 

Data Types 
diagram A diagram is a list of VERTEXES. 
vertex A vertex has a name, type, and list of neighbors and labelings. 

Major Functions 
find-labelings Do the same constraint propagation, but don't print anything. 
propagate-constraints Reduce the number of labelings on vertex by considering neighbors. 
consistent-labelings Return the set of labelings that are consistent with neighbors. 
search-solutions Try all labelings for one ambiguous vertex, and propagate. 
defdiagram (macro) Define a diagram. 
diagram Retrieve a diagram stored by name. 
ground Attach the line between the two VERTEXES to the ground. 

Auxiliary Functions 
labels-for Return all the labels for the line going to vertex. 
reverse -label Reverse left and right on arrow labels. 
ambiguous-vertex -p A vertex is ambiguous if it has more than one labeling. 
number-of-labelings Number of labels on a vertex. 
find-vertex Find the vertex with the given name. 
matrix-transpose Turn a matrix on its side. 
possible-labelings The list of possible labelings for a given vertex type. 
print-vertex Print a vertex in the short form. 
show-vertex Print a vertex in a long form, on a new line. 
show-diagram Print a diagram in a long form. Include a title. 
construct-diagram Build a new diagram from a set of vertex descriptions. 
construct-vertex Build a new vertex from a vertex description. 
make-copy-diagram Make a copy of a diagram, preserving connectivity. 
check-diagram Check if the description appears consistent. 

Figure 17.5: Glossary for the Line-Labeling Program 

list of neighboring Vertexes, and a list of possible labelings. A labeling is a list of line 
labels. For example, a Y vertex will initially have a list of five possible labelings. If it 
is discovered that the vertex is the interior of a concave corner, then it will have the 
single labeling ( -- -). We give type information on the slots of vertex because it 
is a compUcated data type. The syntax of defstruct is such that you cannot specify 

a : type without first specifying a default value. We chose L as the default value for 
the type slot at random, but note that it would have been an error to give nil as the 
default value, because ni1 is not of the right type. 
(defstruct diagram "A diagram is a list of Vertexes." Vertexes) 

(defstruct (vertex (:print-function print-vertex)) 
(name nil :type atom) 
(type 'L :type (member L Y W T)) 
(neighbors nil :type list) ; of vertex 
(labelings nil :type list)) ; of lists of (member + -L R))))) 


<a id='page-570'></a>

An ambiguous vertex will have several labelings, while an unambiguous vertex has 
exactly one, and a vertex with no labelings indicates an impossible diagram. Initially 
we don't know which Vertexes are what, so they all start with several possible labelings. 
Note that a labeling is a list, not a set: the order of the labels is significant and 
matches the order of the neighboring Vertexes. The function possi bl e-1 abel i ngs 
gives a list of all possible labelings for each vertex type. We use R and L instead of 
arrows as labels, because the orientation of the arrows is significant. An R means 
that as you travel from the vertex to its neighbor, the polyhedron is on the right and 
the background object is on the left. Thus, an R is equivalent to an arrow pointing 
away from the vertex. The L is just the reverse. 

(defun ambiguous-vertex-p (vertex) 
"A vertex is ambiguous if it has more than one labeling." 
(> (number-of-labelings vertex) D) 

(defun number-of-labelings (vertex) 
(length (vertex-labelings vertex))) 

(defun impossible-vertex-p (vertex) 
"A vertex is impossible if it has no labeling." 
(null (vertex-labelings vertex))) 

(defun impossible-diagram-p (diagram) 
"An impossible diagram is one with an impossible vertex." 
(some #*impossible-vertex-p (diagram-Vertexes diagram))) 

(defun possible-labelings (vertex-type) 
"The list of possible labelings for a given vertex type." 
;; In these labelings, R means an arrow pointing away from 

the vertex, L means an arrow pointing towards it. 

(case vertex-type 
((L) '((R L) (L R) (+ R) (L +) (- L) (R -))) 
((Y) '((+ + +) ( ) (L R -) (- L R) (R - L))) 
((T) '((R L -H) (R L -) (R L L) (R L R))) 
((W) '((L R +) (-- +) (+ + -))))) 

17.2 Combining Constraints and Searching 
The main function print-1 abel ings takes a diagram as input, reduces the number 
of labelings on each vertex by constraint propagation, and then searches for all 
consistent interpretations. Output is printed before and after each step. 


<a id='page-571'></a>

(defun print-labelings (diagram) 
"Label the diagram by propagating constraints and then 
searching for solutions if necessary. Print results." 
(show-diagram diagram "~&The initial diagram is:") 
(every #*propagate-constraints (diagram-vertexes diagram)) 
(show-diagram diagram 

"~2&After constraint propagation the diagram is:") 

(let* ((solutions (if (impossible-diagram-p diagram) 
nil 
(search-solutions diagram))) 

(n (length solutions))) 

(unless (= . 1) 
(format t "~2&There are ~r solution~:p:" n) 
(mapc #'show-diagram solutions))) 

(values)) 
The function propagate-constraints takes a vertex and considers the constraints 
imposed by neighboring Vertexes to get a list of all the cons i stent -1 abel i ngs for the 
vertex. If the number of consistent labelings is less than the number before we started, 
then the neighbors' constraints have had an effect on this vertex, so we propagate the 
new-found constraints on this vertex back to each neighbor. The function returns 
nil and thus immediately stops the propagation if there is an impossible vertex. 
Otherwise, propagation continues until there are no more changes to the labelings. 
The whole propagation algorithm is started by a call to every in pri nt -1 abel i ngs, 
which propagates constraints from each vertex in the diagram. But it is not obvious 
that this is all that is required. After propagating from each vertex once, couldn't 
there be another vertex that needs relabeling? The only vertex that could possibly 
need relabeling would be one that had a neighbor changed since its last update. 
But any such vertex would have been visited by propagate-constraint, since we 
propagate to all neighbors. Thus, a single pass through the Vertexes, compounded 
with recursive calls, will find and apply all possible constraints. 
The next question worth asking is if the algorithm is guaranteed to terminate. 
Clearly, it is, because propagate-constra i nts can only produce recursive calls when 
it removes a labeling. But since there are a finite number of labelings initially (no more 
than six per vertex), there must be a finite number of calls topropagate-constraints. 
(defun propagate-constraints (vertex) 
"Reduce the labelings on vertex by considering neighbors. 
If we can reduce, propagate the constraints to each neighbor." 
Return nil only when the constraints lead to an impossibility 
(let ((old-num (number-of-labelings vertex))) 
(setf (vertex-labelings vertex) (consistent-labelings vertex)) 
(unless (impossible-vertex-p vertex) 
(when (< (number-of-labelings vertex) old-num) 
(every #'propagate-constraints (vertex-neighbors vertex))) 
t))) 


<a id='page-572'></a>

The function consi stent -1 abel i ngs is passed a vertex. It gets all the labels for this 
vertex from the neighboring Vertexes, collecting them in nei ghbor-1 abel s. It then 
checks all the labels on the current vertex, keeping only the ones that are consistent 
with all the neighbors' constraints. The auxiliary function labels-for finds the 
labels for a particular neighbor at a vertex, and reverse -1 abel accounts for the fact 
that L and R labels are interpreted with respect to the vertex they point at. 

(defun consistent-labelings (vertex) 
"Return the set of labelings that are consistent with neighbors." 
(let ((neighbor-labels 

(mapcar #'(lambda (neighbor) (labels-for neighbor vertex)) 
(vertex-neighbors vertex)))) 
Eliminate labelings that don't have all lines consistent 
;; with the corresponding line's label from the neighbor. 
Account for the L-R mismatch with reverse -label, 
(find-all-if 
#'(lambda (labeling) 
(every #'member (mapcar #'reverse-1abel labeling) 
neighbor-labels)) 
(vertex-labelings vertex)))) 

Constraint propagation is often sufficient to yield a unique interpretation. But sometimes 
the diagram is still underconstrained, and we will have to search for solutions. 
The function search-sol utions first checks to see if the diagram is ambiguous, by 
seeing if it has an ambiguous vertex, v. If the diagram is unambiguous, then it is a 
solution, and we return it (in a hst, since sea rch - sol ut i ons is designed to return a 
list of all solutions). Otherwise, for each of the possible labelings for the ambiguous 
vertex, we create a brand new copy of the diagram and set v's labeling in the copy to 
one of the possible labelings. In effect, we are guessing that a labeling is a correct one. 
We call propagate - const ra i nts; if it fails, then we have guessed wrong, so there are 
no solutions with this labeling. But if it succeeds, then we call sea rch-sol utions 
recursively to give us the list of solutions generated by this labeling. 

(defun search-solutions (diagram) 
"Try all labelings for one ambiguous vertex, and propagate." 

If there is no ambiguous vertex, return the diagram. 
;; If there is one, make copies of the diagram trying each of 
;; the possible labelings. Propagate constraints and append 
;; all the solutions together, 
(let ((v (find-if #'ambiguous-vertex-p 

(diagram-Vertexes diagram)))) 

(if (null V) 
(list diagram) 
(mapcan 

#'(lambda (v-labeling) 


<a id='page-573'></a>

(let* ((diagram2 (make-copy-diagram diagram)) 

(v2 (find-vertex (vertex-name v) diagram2))) 
(setf (vertex-labelings v2) (list v-labeling)) 
(if (propagate-constraints v2) 

(search-solutions diagram2) 
nil))) 
(vertex-labelings v))))) 

That's all there is to the algorithm; all that remains are some auxiliary functions. 
Here are three of them: 

(defun labels-for (vertex from) 
"Return all the labels for the line going to vertex." 
(let ((pos (position from (vertex-neighbors vertex)))) 

(mapcar #*(lambda (labeling) (nth pos labeling)) 
(vertex-labelings vertex)))) 

(defun reverse-label (label) 
"Account for the fact that one vertex's right is another's left." 
(case label (L 'R) (R 'D (otherwise label))) 

(defun find-vertex (name diagram) 
"Find the vertex in the given diagram with the given name." 
(find name (diagram-vertexes diagram) :key #'vertex-name)) 

Here are the printing functions, print - vertex prints a vertex in short form. It obeys 
the .r i .t convention of returning the first argument. The functions s how - ve r t ex and 
show-di agramprintmoredetailedforms. They obey theconventionfordescri be-like 
functions of returning no values at all. 

(defun print-vertex (vertex stream depth) 
"Print a vertex in the short form." 
(declare (ignore depth)) 
(format stream "~a/~d" (vertex-name vertex) 

(number-of-labelings vertex)) 
vertex) 

(defun show-vertex (vertex &optional (stream t)) 
"Print a vertex in a long form, on a new line." 
(format stream "~& "a "d:" vertex (vertex-type vertex)) 
(mapc #'(lambda (neighbor labels) 

(format stream " '"a~a=[''{~a''}]" (vertex-name vertex) 

(vertex-name neighbor) labels)) 
(vertex-neighbors vertex) 
(matrix-transpose (vertex-labelings vertex))) 

(values)) 


<a id='page-574'></a>

(defun show-diagram (diagram &optional (title "~2&Diagram:") 

(stream t)) 
"Print a diagram in a long form. Include a title. " 
(format stream title) 
(mapc #*show-vertex (diagram-vertexes diagram)) 
(let ((n (reduce #'* (mapcar #'number-of-labelings 

(diagram-vertexes diagram))))) 
(when (> . 1) 
(format stream "~&For "RD interpretation~:p." n)) 
(values))) 

Note that matri x-transpose is called by show-vertex to turn the matrix of labelings 
on its side. It works like this: 

> (possible-labelings *Y) 

ii+ + +) 

( ) 

(L R -) 
(- L R) 
(R - D) 

> (matrix-transpose (possible-labelings .)) 

((+ - L - R) 
(-^' - R L -) 
(-H -- R D) 

The implementation of matrix-transpose is surprisingly concise. It is an old Lisp 
trick, and well worth understanding: 

(defun matrix-transpose (matrix) 
"Turn a matrix on its side." 
(if matrix (apply #'mapcar #'list matrix))) 

The remaining code has to do with creating diagrams. We need some handy way of 
specifying diagrams. One way would be with a line-recognizing program operating 
on digitized input from a camera or bitmap display. Another possibility is an interactive 
drawing program using a mouse and bitmap display. But since there is not yet a 
Common Lisp standard for interacting with such devices, we will have to settle for a 
textual description. The macro def di agram defines and names a diagram. The name 
is followed by a list of vertex descriptions. Each description is a list consisting of 
the name of a vertex, the vertex type (Y, A, L, or T), and the names of the neighboring 
Vertexes. Here again is the def di agram description for the cube shown in figure 17.6. 


<a id='page-575'></a>

(defdiagram cube 
(a Y b c d) 

(bW e a) 
(c W f a) 
(dW g a) 
(eL b) 
(f L c) 
(gL d)) 

Figure 17.6: A Cube 

Tiie macro def diagram calls construct -diagram to do the real work. It would 
be feasible to have defdi agram expand into a defvar, making the names be special 
variables. But then it would be the user's responsibility to make copies of such a 
variable before passing it to a destructive function. Instead, I use put-di agram and 
di agram to put and get diagrams in a table, di agram retrieves the named diagram 
and makes a copy of it. Thus, the user cannot corrupt the original diagrams stored in 
the table. Another possibility would be to have def di agram expand into a function 
definition for name that returns a copy of the diagram. I chose to keep the diagram 
name space separate from the function name space, since names like cube make 
sense in both spaces. 

(defmacro defdiagram (name &rest vertex-descriptors) 
"Define a diagram. A copy can be gotten by (diagram name)." 
'(put-diagram '.name (construct-diagram '.vertex-descriptors))) 

(let ((diagrams (make-hash-table))) 


<a id='page-576'></a>

(defun diagram (name) 
"Get a fresh copy of the diagram with this name." 
(make-copy-diagram (gethash name diagrams))) 

(defun put-diagram (name diagram) 
"Store a diagram under a name." 
(setf (gethash name diagrams) diagram) 
name)) 

The function construct-di agram translates each vertex description, using 
construct - vertex, and then fills in the neighbors of each vertex. 

(defun construct-diagram (vertex-descriptors) 
"Build a new diagram from a set of vertex descriptor." 
(let ((diagram (make-diagram))) 

Put in the Vertexes 
(setf (diagram-vertexes diagram) 

(mapcar #'construct-vertex vertex-descriptors)) 
;; Put in the neighbors for each vertex 
(dolist (v-d vertex-descriptors) 

(setf (vertex-neighbors (find-vertex (first v-d) diagram)) 
(mapcar #'(lambda (neighbor) 
(find-vertex neighbor diagram)) 
(v-d-neighbors v-d)))) 
diagram)) 

(defun construct-vertex (vertex-descriptor) 
"Build the vertex corresponding to the descriptor." 
;; Descriptors are like: (x L y z) 
(make-vertex 

:name (first vertex-descriptor) 
:type (second vertex-descriptor) 
:labelings (possible-labelings (second vertex-descriptor)))) 

(defun v-d-neighbors (vertex-descriptor) 
"The neighboring vertex names in a vertex descriptor." 
(rest (rest vertex-descriptor))) 

The defstruct for di agram automatically creates the function copy-di agram, but it 
just copies each field, without copying the contents of each field. Thus we need 
make - copy -di ag ram to create a copy that shares no structure with the original. 


<a id='page-577'></a>

(defun make-copy-diagram (diagram) 
"Make a copy of a diagram, preserving connectivity." 
(let* ((new (make-diagram 

'.Vertexes (mapcar #*copy-vertex 
(diagram-vertexes diagram))))) 
Put in the neighbors for each vertex 
(dolist (v (diagram-vertexes new)) 
(setf (vertex-neighbors v) 
(mapcar #*(lambda (neighbor) 
(find-vertex (vertex-name neighbor) new)) 
(vertex-neighbors v)))) 
new)) 

17.3 Labeling Diagrams 
We are now ready to try labeling diagrams. First the cube: 

> (print-labelings (diagram 'cube)) 

The initial diagram is: 
A/5 Y: AB=C+-L-R] AC=[+-RL-] AD=[+--RL] 
B/3 W: BG=[L-+] BE=[R-+] BA=[++-] 
C/3 W: CE=[L-+] CF=[R-+] CA=C++-] 
D/3 W: DF=[L-+] DG=[R-+] DA=C++-] 
E/6 L: EC=[RL+L-R] EB=[LRR+L-] 
F/6 L: FD=[RL+L-R] FC=CLRR+L-] 
G/6 L: GB=[RL+L-R] GD=[LRR+L-] 

For 29,160 interpretations. 

After constraint propagation the diagram is: 
A/1 Y: AB=[+] AC=[+] AD=[+] 
B/2 W: BG=CL-] BE=[R-] BA=C++] 
C/2 W: CE=[L-] CF=[R-] CA=C++] 
D/2 W: DF=[L-] DG=[R-] DA=C++] 
E/3 L: EC=[R-R] EB=[LL-] 
F/3 L: FD=[R-R] FC=CLL-] 
G/3 L: GB=[R-R] GD=[LL-] 

For 216 interpretations. 

There are four solutions: 


<a id='page-578'></a>

Diagram: 
A/1 Y: AB=[+] AC=C+] AD=C+] 
B/1 W: BG=[L] BE=[R] BA=[+] 
C/1 W: CE=CL] CF=CR] CA=[+] 
D/1 W: DF=CL] DG=[R] DA=[+] 
E/1 L: EC=[R] EB=[L] 
F/1 L: FD=CR] FC=CL] 
G/1 L: GB=[R] GD=[L] 

Diagram: 
A/1 Y: AB=[+] AC=[+] AD=[+] 
B/1 W: BG=[L] BE=[R] BA=[+] 
C/1 W: CE=[L] CF=[R] CA=C+] 
D/1 W: DF=C-] DG=[-] DA=C+] 
E/1 L: EC=CR] EB=CL] 
F/1 L: FD=C-] FC=[L] 
G/1 L: GB=CR] GD=[-] 

Diagram: 
A/1 Y: AB=C+] AC=C+] AD=C+] 
B/1 W: BG=[L] BE=[R] BA=C+] 
C/1 W: CE=[-] CF=C-] CA=C+] 
D/1 W: DF=CL] DG=CR] DA=[-H] 
E/1 L: EC=C-] EB=[L] 
F/1 L: FD=[R] FC=[-] 
G/1 L: GB=[R] GD=[L] 

Diagram: 
A/1 Y: AB=[+] AC=[+] AD=C+] 
B/1 W: BGK-] BE=C-] BA=C+] 
C/1 W: CE=[L] CF=[R] CA=[+] 
D/1 W: DF=[L] DG=CR] DA=C+] 
E/1 L: EC=CR] EB=C-] 
F/1 L: FD=[R] FC=CL] 
G/1 L: GB=C-] GD=[L] 

The four interpretations correspond, respectively, to the cases where the cube is free 
floating, attached to the floor (GD and DF = -), attached to a wall on the right (EC 
and CF = -), or attached to a wall on the left (BG and BE = -). These are shown in 
figure 17.7. It would be nice if we could supply information about where the cube is 
attached, and see if we can get a unique interpretation. The function ground takes a 
diagram and modifies it by making one or more lines be grounded lines - lines that 
have a concave (-) label, corresponding to a junction with the ground. 


<a id='page-579'></a>

Figure 17.7: Four Interpretations of the Cube 

(defun ground (diagram vertex-a vertex-b) 
"Attach the line between the two Vertexes to the ground. 
That is. label the line with a -" 
(let* ((A (find-vertex vertex-a diagram)) 

(B (find-vertex vertex-b diagram)) 

(i (position . (vertex-neighbors A)))) 
(assert (not (null i))) 
(setf (vertex-labelings A) 

(find-all-if #'(lambda (1) (eq (nth i 1) '-)) 
(vertex-labelings A))) 
diagram)) 


<a id='page-580'></a>

We can see how this works on the cube: 

Figure 17.8: Cube on a Plate 

> (print-labelings (ground (diagram 'cube) 'g *d)) 

The initial diagram is: 
A/5 Y: AB=[+-L-R] AC=C+-RL-] AD=[+--RL] 
B/3 W: BG=[L-+] BE=[R-+] BA=C++-] 
C/3 W: CE=CL-+] CF=CR-+] CA=C-H-] 
D/3 W: DF=[L-+] DG=[R-+] DA=[-M-] 
E/6 L: EC=[RL+L-R] EB=[LRR+L-] 
F/6 L: FD=[RL+L-R] FCKLRR+L-] 
G/1 L: GB=[R] GD=[-] 
For 4,860 interpretations. 

After constraint propagation the diagram is: 
A/1 Y: AB=C+] AC=C+] AD=C+] 
B/1 W: BG=[L] BE=[R] BA=[+] 
C/1 W: CE=[L] CF=CR] CA=[+] 
D/1 W: DF=[-] DG=C-] DA=C+] 
E/1 L: EC=[R] EB=CL] 
F/1 L: FD=[-] FC=CL] 
G/1 L: GB=CR] GD=C-] 


<a id='page-581'></a>
Note that the user only had to specify one of the two ground lines, GD. The program 
found that DF is also grounded. Similarly, in programming ground-1 ine, we only 
had to update one of the Vertexes. The rest is done by constraint propagation. 

The next example yields the same four interpretations, in the same order (free 
floating, attached at bottom, attached at right, and attached at left) when interpreted 
ungrounded. The grounded version yields the unique solution shown in the following 
output and in figure 17.9. 

Figure 17.9: Labeled Cube on a Plate 

(defdiagram cube-on-plate 
(a Y b c d) 
(b W e a) 
(c W f a) 
(d W g a) 

(eL b) 
(f Y c i) 
(gY d h) 

(h W 1 g j) 
(i W f m j) 
(j Y hi k) 
(k Wm 1 j) 
(1 L h k) 


<a id='page-582'></a>

(m L k i)) 

> (print-labelings (ground (diagram 'cube-on-plate) 'k 'm)) 

The initial diagram is: 
A/5 Y: AB=C+-L-R] AC=C+-RL-] A[)=[+--RL] 
B/3 W: BG=CL-+] BE=[R-+] BA=C++-] 
C/3 W: CE=CL-+] CF=CR-+] CA=C++-] 
D/3 W: DF=CL-+] DG=CR-+] DA=C++-] 
E/6 L: EC=CRL+L-R] EB=CLRR+L-] 
F/5 Y: FD=C+-L-R] FC=C+-RL-] FI=[+--RL] 
G/5 Y: GB=C+-L-R] GD=C+-RL-] GH=[+--RL] 
H/3 W: HL=CL-+] HG=[R-+] HJ=[+-h-] 
1/3 W: IF=CL-+] IM=[R-+] IJ=[++-] 
J/5 Y: JH=C+-L-R] JI=C+-RL-] JK=[+--RL] 
K/1 W: KM=[-] KL=C-] KJ=[+] 
L/6 L: LH=CRL+L-R] LK=CLRR+L-] 
M/6 L: MK=[RL+L-R] MI=CLRR+L-] 

For 32.805.000 interpretations. 

After constraint propagation the diagram is: 
A/1 Y: AB=C+] AC=C+] AD=C+] 
B/1 W: BG=CL] BE=[R] BA=C+] 
C/1 W: CE=[L] CF=CR] CA=[+] 
D/1 W: DF=C-] DG=C-] DA=[+] 
E/1 L: EC=CR] EB=CL] 
F/1 Y: FD=C-] FC=CL] FI=[R] 
G/1 Y: GB=[R] GD=C-] GH=CL] 
H/1 W: HL=[L] HG=CR] HJ=M 
I/l W: IF=CL] IM=[R] IJ=C+] 
J/1 Y: JH=[+] JI=C+] JK=[+] 
K/1 W: KM=[-] KL=[-] KJ=[+] 
L/1 L: LH=CR] LK=C-] 
M/1 L: MK=[-] MI=[L] 

It is interesting to try the algorithm on an "impossible" diagram. It turns out the 
algorithm correctly finds no interpretation for this well-known illusion: 

(defdiagram poiuyt 
(a L b g) 
(b L j a) 
(c L d 1) 
(d L h c) 
(e L f i) 
(f L k e) 
(g L a 1) 
(h L 1 d) 
(i L e k) 
(j L k b) 


<a id='page-583'></a>

Figure 17.10: An Impossible Figure (A Poiuyt) 

(k W j i f) 
(1 W h g c)) 

> (print-labelings (diagram 'poiuyt)) 

The initial diagram is: 
A/6 AB=CRL+L-R] AG=[LRR+L-] 
B/6 BJ=[RL+L-R] BA=[LRR+L-] 
C/6 CD=CRL+L-R] CL=[LRR+L-] 
D/6 DH=[RL+L-R] DC=CLRR+L-] 
E/6 EF=[RL+L-R] EI=[LRR+L-] 
F/6 FK=[RL+L-R] FE=CLRR+L-] 
G/6 GA=[RL+L-R] GL=[LRR+L-] 
H/6 HL=[RL+L-R] HD=CLRR+L-] 
1/6 IE=[RL+L-R] IK=CLRR+L-] 
J/6 JK=[RL+L-R] JB=CLRR+L-] 
K/3 W KJ=[L-+] KI=CR-+] KF=[++-] 
L/3 W LH=[L-+] LG=[R-+] LC=C++-] 

For 544.195.584 interpretations. 

After constraint propagation the diagram is: 
A/5 AB=CRL+-R] AG=[LRRL-] 
B/5 BJ=CRLL-R] BA=[LR+L-] 
C/2 CD=[LR] CL=[+-] 
D/3 DH=[RL-] DC=[LRL3 
E/3 EF=[RLR] EI=[LR-] 
F/2 FK=C+-] FE=[RL] 


<a id='page-584'></a>

G/4 L: GA=[RL-R] GL=[L+L-] 
H/4 L: HL=[R+-R] HD=[LRL-] 
1/4 L: IE=CRL-R] IK=[L+L-] 
J/4 L: JK=[R+-R] JB=CLRL-] 
K/3 W: KJ=[L-+] KI=CR-+] KF=C++-3 
L/3 W: LH=CL-+] LG=[R-+] LC=C++-] 

For 2.073,600 interpretations. 

There are zero solutions: 

Now we try a more complex diagram: 

(defdiagram tower

(a Y b c d) (n L q 0) 
(b W g e a) (0 W y j n) 
(c W e f a) (P L r i) 
(d W f g a) (q W . s w) 
(e L c b) (r W s . .) 
(f Y d c i) (s L r q) 
(g Y b d h) (t W w . .) 
(h W 1 g j) (u W . y .) 
(i W f m p) (V W y w .) 
(j Y h 0 k) (w Y t . q) 
(k W m 1 j) (x Y r u t) 
(1 L h k) (y Y V u o) 
(m L k i) (z Y t U V)) 

> (print-labelings (ground (diagram 'tower) . 'k)) 

The initial diagram is: 
A/5 Y: =[+-L-R3 AC=[+-RL-] AD=C+--RL] 
B/3 W: 
C/3 W: 
D/3 W: 
E/6 L: 
F/5 Y: FD=C+-L-R] FC=C+-RL-] FI=C+--RL] 
G/5 Y: GB=[+-L-R] GD=C+-RL-] GH=C+--RL] 
H/3 W: HL=CL-+] HG=CR-+] HJ=C-h-] 
1/3 W: IF=[L-+] IM=[R-+] IP=C++-] 
J/5 Y: ^--RL] 
K/3 W: KM=[L-+] KL=CR-+] KJ=C++-] 
L/1 L: 
M/6 L: 
N/6 L: 
0/3 W: 
P/6 L: 
0/3 W: QN=[L-+] QS=CR-+] QW=C++-] 
R/3 W: RS=CL-+] RP=[R-+3 RX=C-H-] 
S/
S/S/6
66 L
LL:
:: SR=CRL+L-R] SQ=CLRR+L-] 


<a id='page-585'></a>

T/3 W: TW=CL-+] TX=CR-+] TZ=C++-] 
U/3 W: UX=[L-+] UY=[R-+] UZ=[++-] 
V/3 W: VY=CL-+] VW=CR-+] VZ=[++-] 
W/5 Y: WT=C+-L-R] WV=C+-RL-] WQ=C+--RL] 
X/5 Y: XR=[+-L-R] XU=C+-RL-] XT=[+--RL] 
Y/5 Y: YV=[+-L-R] YU=[+-RL-] YO=C+--RL] 
Z/5 Y: ZT=C+-L-R] ZU=C+-RL-] ZV=[+--RL] 

For 1,614,252,037,500,000 interpretations. 

Figure 17.11: A Tower 

After constraint propagation the diagram is: 
A/1 Y: AB=[+] AC=[+] AI>[+] 
B/1 W: BG=[L] BE=[R] BA=[+] 
C/1W: CE=[L] CF=[R] CA=[+] 
D/1W: DF=[-] DG=[-] DA=[+] 
E/1L: EC=[R]EB=[L] 
F/1YFD=[-] FC=[L] FI=[R] 
G/1Y GB=[R] GD=[-] GH=[L] 
H/1W: HL=[L] HG=[R] HJ=[+] 
I/l W: IF=[L]IM=[R] IP=[+] 
J/lYJH=[+]JO=MJK=[+] 


<a id='page-586'></a>

K/1 W: KM=[-] KL=[-] KJ=[+] 
L/1 L: LH=[R] LK=[-] 
M/1L: MK=[-] MI=[L] 
N/1 L: NQ=[R] NO=[-] 
O/l W:OY=M OJ=M ON=[-] 
P/1 L: PR=[L] PI=[+] 
Q/1 W: QN=[L] QS=[R] QW=[+] 
R/1 W: RS=[L] RP=[R] RX=M 
S/1 L: SR=[R] SQ=[L] 
T/1 W: TW=[+] TX=[+] TZ=[-] 
U/1 W: UX=[+] UY=[+] UZ=[-] 
V/1 W: VY=[+] VW=[+] VZ=[-] 
W/1.: WT=[+] WV=M WQ=[+] 
X/1.: XR=[+] XU=[+] XT=[+] 
./1.: YV=[+] YU=[+] Y0=[+] 
Z/1Y: ZT=[-] ZU=[-] ZV=[-] 

We see that the algorithm was able to arrive at a single interpretation. Moreover, even 
though there were a large number of possibilities - over a quadrillion - the computation 
is quite fast. Most of the time is spent printing, so to get a good measurement, 
we define a function to find solutions without printing anything: 

(defun find-labelings (diagram) 
"Return a list of all consistent labelings of the diagram." 
(every #'propagate-constraints (diagram-vertexes diagram)) 
(search-solutions diagram)) 

When we time the application of find-label ings to the grounded tower and the 
poiuyt, we find the tower takes 0.11 seconds, and the poiuyt 21 seconds. This is over 
180 times longer, even though the poiuyt has only half as many Vertexes and only 
about half a million interpretations, compared to the tower's quadrillion. The poiuyt 
takes a long time to process because there are few local constraints, so violations are 
discovered only by considering several widely separated parts of the figure all at the 
same time. It is interesting that the same fact that makes the processing of the poiuyt 
take longer is also responsible for its interest as an illusion. 

17.4 Checking Diagrams for Errors 
This section considers one more example, and considers what to do when there are 
apparent errors in the input. The example is taken from Charniak and McDermott's 
Introduction to Artificial Intelligence, page 138, and shown in figure 17.12. 


<a id='page-587'></a>
Figure 17.12: Diagram of an arch 

(defdiagram arch 
(a W e b c) (P L 0 q) 
(b L d a) (q . . i .) 
(c Y a d g) (. .3 s q) 
(d Y c b m) (s L . t) 
(e L a f) (t W. s k) 
(f . e g n) (u L t 1) 
(g wh f c) (V L 2 4) 
(h . g i 0) (w W X 1 y) 
(i . h j q) (X Lw z) 
(j . i k r) (y Yw 2 z) 
(k . j 1 t) (z W 3X y) 
(1 . k m v) (1 . . 0 w) 
(m L 1 d) (2 W V3 y) 
(n L f 1) (3 L . 2) 
(0 W . 1 h) (4 . u 1 V)) 

Unfortunately, running this example results in no consistent interpretations after 
constraint propagation. This seems wrong. Worse, when we try to ground the 
diagram on the line XZ and call pri nt -1 abel i ngs on that, we get the following error: 


<a id='page-588'></a>

>>ERROR: The first argument to NTH was of the wrong type. 
The function expected a fixnum >= zero. 
While in the function LABELS-FOR ^ CONSISTENT-LABELINGS 

Debugger entered while in the following function: 

LABELS-FOR (P.C. = 23) 
Arg 0 (VERTEX): U/6 
Arg 1 (FROM): 4/4 

What has gone wrong? A good guess is that the diagram is somehow inconsistent - 
somewhere an error was made in transcribing the diagram. It could be that the 
diagram is in fact impossible, like the poiuyt. But that is unlikely, as it is easy for us 
to provide an intuitive interpretation. We need to debug the diagram, and it would 
also be a good idea to handle the error more gracefully. 

One property of the diagram that is easy to check for is that every line should be 
mentioned twice. If there is a line between Vertexes A and B, there should be two 
entries in the vertex descriptors of the following form: 

(A ? ... . ...) 
(. ? ... A ...) 

Here the symbolmeans we aren't concerned about the type of the Vertexes, only 
with the presence of the line in two places. The following code makes this check 
when a diagram is defined. It also checks that each vertex is one of the four legal 
types, and has the right number of neighbors. 

(defmacro defdiagram (name &rest vertex-descriptors) 
"Define a diagram. A copy can be gotten by (diagram name)." 
'(put-diagram '.name (construct-diagram 

(check-diagram '.vertex-descriptors)))) 

(defun check-diagram (vertex-descriptors) 
"Check if the diagram description appears consistent." 
(let ((errors 0)) 

(dolist (v-d vertex-descriptors) 
v-d is like: (a Y b c d) 
(let ((A (first v-d)) 

(v-type (second v-d))) 
Check that the number of neighbors is right for 
the vertex type (and that the vertex type is legal) 

(when (/= (length (v-d-neighbors v-d)) 

(case v-type ((W Y .) 3) ((L) 2) (t -1))) 
(warn "Illegal type/neighbor combo: '^a" v-d) 
(incf errors)) 

;; Check that each neighbor . is connected to 


<a id='page-589'></a>

this vertex. A. exactly once 
(dolist (B (v-d-neighbors v-d)) 
(when (/= 1 (count-if 
#'(lambda (v-d2) 
(and (eql (first v-d2) B) 
(member A (v-d-neighbors v-d2)))) 

vertex-descriptors)) 
(warn "Inconsistent vertex: "a-^a" A B) 
(incf errors))))) 

(when (> errors 0) 
(error "Inconsistent diagram. ~d total error~:p." 
errors))) 
vertex-descriptors) 

Now let's try the arch again: 

(defdiagram arch 
(a W eb c) (PL 0 q) 
(b L d a) (q. . i r) 
(c Y a d g) (r . j s q) 
(d Y c b m) (s L r t) 
(e L a f) (t W V s k) 
(f . e g n) (u L t 1) 
(g W hf c) (V L 2 4) 
(h . g i 0) (wW X 1 y) 
(i . h j q) (X L w z) 
(j . i k r) (yY w 2 z) 
(k . j 1 t) (.W 3 X y) 
(1 . k m v) (1. . 0 w) 
(m L 1 d) (2W V 3 y) 
(n L f 1) (3 L . 2) 
(0 W .1 h) (4. u 1 V)) 

Warning: Inconsistent vertex: T-'V 
Warning: Inconsistent vertex: U-'T 
Warning: Inconsistent vertex: U--L 
Warning: Inconsistent vertex: L--V 
Warning: Inconsistent vertex: 4

--u 

Warning: Inconsistent vertex: 4-'L 

>ERROR: Inconsistent diagram, 6 total errors. 

The def d i a g ram was transcribed from a hand-labeled diagram, and it appears that the 
transcription has fallen prey to one of the oldest problems in mathematical notation: 
confusing a "u" with a "v." The other problem was in seeing the line U-L as a single 
line, when in fact it is broken up into two segments, U-4 and 4-L. Repairing these 
bugs gives the diagram: 


<a id='page-590'></a>

(defdiagram arch 

(a W e b c) (P L 0 q ) 
( b L d a) (q . . i . ) 
(c Y a d g ) ( . . J s q ) 
(d Y c b m) (s L . t ) 
(e L a f) ( t W u s k) ; t-u not t-v 
(f . e g n) (u L t 4) ; w4 
not u-l 
( g W h f c) (V L 2 4) 
(h . g i 0 ) (w W X 1 y ) 
( i 
(j 
(k 
. h 
. 
. i 
J q ) 
k r) 
1 t ) 
( X L w z) 
( y Y w 2 z) 
(z W 3 X y ) 
(1 . k m 4) (1 . . 0 w) ;1-4 not l-v 
(m L 1 d ) 
(n L f 1) 
( 0 W . 1 h ) 
(2 
(3 
(4 
wV 3 y ) 
L . 2) 
. u 1 V) ) 

This time there are noerrors detected by check-di agram, butrunningprint-label ings 

again still does not give a solution. To get more information about which constraints 

are applied, I modified propagate-constrai nts to print out some information: 

(defun propagate-constraints (vertex) 
"Reduce the number of labelings on vertex by considering neighbors. 
If we can reduce, propagate the new constraint to each neighbor." 

Return nil only when the constraints lead to an impossibility 

(let ((old-num (number-of-labelings vertex))) 
(setf (vertex-labelings vertex) (consistent-labelings vertex)) 
(unless (impossible-vertex-. vertex) 

(when (< (number-of-labelings vertex) old-num) 

(format t "-&; ~a: "Ua ~a" vertex 
(vertex-neighbors vertex) 
(vertex-labelings vertex)) 
(every #'propagate-constraints (vertex-neighbors vertex))) 

vertex))) 

Running the problem again gives the following trace: 

> (print-labelings (ground (diagram 'arch) *x '.)) 

The initial diagram is: 
A/3 W: AE=[L-+] AB=[R-+] AC=[++-] 
P/6 L: PO=[RL+L-R] PQ=CLRR+L-] 
B/6 L: BD=[RL-HL-R] BA=CLRR+L-] 
Q/4 T: QP=[RRRR] QI=[LLLL] QR=[+-LR] 
C/5 Y: CA=C+-L-R] CD=C+-RL-] CG=[+--RL] 
R/4 T: RJ=[RRRR] RS=[LLLL] RQ=[+-LR] 
D/5 Y: DC=C+-L-R] DB=C+-RL-] DM=C+--RL] 


<a id='page-591'></a>
S/6 L: SR=[RL+L-R] ST=CLRR+L-] 
E/6 L: EA=CRL+L-R] EF=[LRR+L-] 
T/3 W: TU=[L-+] TS=[R-+] TK=C++-] 
F/4 T: FE=[RRRR] FG=CLLLL] FN=C+-LR] 
U/6 L: UT=CRL+L-R] U4=CLRR+L-] 
G/3 W: GH=[L-+] GF=CR-+] GCK-H-] 
V/6 L: V2=[RL+L-R] V4=[LRR+L-] 
H/4 T: HG=CRRRR] HI=CLLLL] HO=[+-LR] 
W/3 W: WX=CL-+] W1=[R-+] WY=C++-] 
1/4 T: IH=[RRRR] IJ=CLLLL] IQ=C+-LR] 
X/1 L: XW=[R] XZ=[-] 
J/4 T: JI=CRRRR] JK=CLLLL] JR=[-h-LR] 
Y/5 Y: YW=[+-L-R] Y2=[+-RL-] YZ=C+--RL] 
K/4 T: KJ=[RRRR] KL=[LLLL] KT=C+-LR] 
Z/3 W: Z3=CL-+] ZX=CR-+] ZY=C++-] 
L/4 T: LK=[RRRR] LM=[LLLL] L4=C+-LR] 
1/4 T: 1N=[RRRR] 10=CLLLL] 1W=[+-LR] 
M/6 L: ML=CRL+L-R] MD=[LRR+L-] 
2/3 W: 2V=[L-+] 23=[R-+] 2Y=[++-] 
N/6 L: NF=CRL+L-R] N1=CLRR+L-] 
3/6 L: 3Z=[RL+L-R] 32=CLRR+L-] 
0/3 W: OP=[L-+] 01=CR-+] OH=C++-] 
4/4 T: 4U=[RRRR] 4L=[LLLL] 4V=C+-LR] 

For 2,888 ,816,545,234,944,000 i nterpretati ons. 
P/2 (0/3 0/4) ((R L) (- D) 
0/1 (P/2 1/4 H/4) ((L R +)) 
P/1 (0/1 Q/4) ((R D) 
1/3 (N/6 0/1 W/3) ((R L +) (R L -) (R L D) 
N/2 (F/4 1/3) ((R L) (- D) 
F/2 (E/6 G/3 N/2) ((R L -) (R L D) 
E/2 (A/3 F/2) ((R L) (- D) 
A/2 (E/2 B/6 C/5) ((L R +) (-- +)) 
B/3 (D/5 A/2) ((R L) (- L) (R -)) 
D/3 (C/5 B/3 M/6) ((---) (- L R) (R - D) 
W/1 (X/1 1/3 Y/5) ((L R +)) 
1/1 (N/2 0/1 W/1) ((R L D ) 
Y/1 (W/1 2/3 Z/3) ((+ + +)) 
2/2 (V/6 3/6 Y/1) ((L R +) (-- +)) 
V/3 (2/2 4/4) ((R L) (- L) (R -)) 
4/2 (U/6 L/4 V/3) ((R L -) (R L R)) 
U/2 : (T/3 4/2) ((R L) (- D) 
T/2 (U/2 S/6 K/4) ((L R +) (-- +)) 
S/2 (R/4 T/2) ((R L) (R -)) 
K/1 (J/4 L/4 T/2) ((R L +)) 
J/1 (1/4 K/1 R/4) ((R L D ) 
I/l (H/4 J/1 0/4) ((R L R)) 
L/1 (K/1 M/6 4/2) ((R L R)) 
M/2 (L/1 D/3) ((R L) (R -)) 


<a id='page-592'></a>

3/3: (Z/3 2/2) ((R L) (- L) (R -)) 
1/1: (3/3 X/1 Y/1) (( - - +)) 
3/1: (Z/1 2/2) (( - D) 
2/1: (V/3 3/1 Y/1) ((L R +)) 
V/2: (2/1 4/2) ((R L) (R -)) 

After constraint propagation the diagram is: 
A/0 W: 
P/1 L: PO=[R] PQ=CL] 
B/0 L: 
Q/4 T: QP=[RRRR] QI=[LLLL] QR=[+-LR] 
C/0 Y: 
R/4 T: RJ=[RRRR] RS=[LLLL] RQ=C+-LR] 
D/0 Y: 
S/2 L: SR=CRR] ST=[L-] 
E/2 L: EA=[R-] EF=CLL] 
T/2 W: TU=[L-] TS=CR-] TK=[++] 
F/2 T: FE=CRR] FG=[LL] FN=C-L] 
U/2 L: UT=[R-] U4=[LL] 
G/0 W: 
V/2 L: V2=[RR] V4=CL-] 
H/0 T: 
W/1 W: WX=[L] W1=[R] WY=C+] 

I/l T: IH=[R3 IJ=[L] IQ=[R] 
X/1 L: XW=[R] XZ=[-] 
J/1 T: JI=[R] JK=[L] JR=[L] 
Y/1 Y: YW=C+] Y2=[+] YZ=[+] 
K/1 T: KJ=CR] KL=[L] KT=[+] 
Z/1 W: Z3=C-] ZX=[-] ZY=[H-] 
L/1 T: LK=[R] LM=[L] L4=[R] 
1/1 T: 1N=[R] 10=[L] 1W=[L] 
M/2 L: ML=[RR] MD=CL-] 
2/1 W: 2V=CL] 23=CR] 2Y=[+] 
N/2 L: NF=[R-] N1=[LL] 
3/1 L: 3Z=[-] 32=[L] 
0/1 W: OP=[L] 01=CR] OH=C+] 
4/2 T: 4U=[RR] 4L=[LL] 4V=[-R] 

From the diagram after constraint propagation we can see that the Vertexes A,B,C,D,G, 
and . have no interpretations, so they are a good place to look first for an error. From 
the trace generated by propagate-constraints (the lines beginning with a semicolon), 
we see that constraint propagation started at . and after seven propagations 
reached some of the suspect Vertexes: 


<a id='page-593'></a>

A/2: (E/2 B/6 C/5) ((L R +) (-- +)) 
8/3: (D/5 A/2) ((R L) (- L) (R -)) 
D/3: (C/5 B/3 M/6) (( ) (- L R) (R - D) 

A and . look acceptable, but look at the entry for vertex D. It shows three interpretations, 
and it shows that the neighbors are C, B, and M. Note that line DC, the first 
entry in each of the interpretations, must be either -, - or R. But this is an error, 
because the "correct" interpretation has DC as a + line. Looking more closely, we 
notice that D is in fact a W-type vertex, not a Y vertex as written in the definition. We 
should have: 

(defdiagram arch 
(a W e b c) (p L 0 q) 
(b L d a) (q . . i r) 
(cY a d g) (r . j s q) 
(d W b m c) (s L r t) ;disaW,notY 

(e L a f) (t W u s k) 
(f . e g n) (u L t 4) 
(gW h f c) (V L 2 4) 
(h . g i 0) (w W . 1 y) 
(i . h j q) (x L w z) 
(j . i k r) (y Y w 2 z) 
(k . j 1 t) (z W 3 X y) 
(1 . k m 4) (1 . . 0 w) 
(m L 1 d) (2 WV 3 y) 
(. L f 1) (3 L . 2) 
(0 W . 1 h) (4 . u 1 V)) 

By running the problem again and inspecting the trace output, we soon discover the 
real root of the problem: the most natural interpretation of the diagram is beyond the 
scope of the program! There are many interpretations that involve blocks floating in 
air, but if we ground lines OP, TU and XZ, we run into trouble. Remember, we said 
that we were considering trihedral Vertexes only. But vertex 1 would be a quad-hedral 
vertex, formed by the intersection of four planes: the top and back of the base, and 
the bottom and left-hand side of the left pillar. The intuitively correct labeling for the 
diagram would have Ol be a concave (-) line and Al be an occluding line, but our 
repertoire of labelings for . Vertexes does not allow this. Hence, the diagram cannot 
be labeled consistently. 

Let's go back and consider the error that came up in the first version of the 
diagram. Even though the error no longer occurs on this diagram, we want to make 
sure that it won't show up in another case. Here's the error: 


<a id='page-594'></a>

>>ERROR: The first argument to NTH was of the wrong type. 
The function expected a fixnum >= zero. 
While in the function LABELS-FOR <i= CONSISTENT-LABELINGS 

Debugger entered while in the following function: 

LABELS-FOR (P.C. = 23) 
Arg 0 (VERTEX): U/6 
Arg 1 (FROM): 4/4 

Looking at the definition of 1 abel s - for, we see that it is looking for the from vertex, 
which in this case is 4, among the neighbors of U. It was not found, so pos became nil, 
and the function nth complained that it was not given an integer as an argument. So 
this error, if we had pursued it earlier, would have pointed out that 4 was not listed 
as a neighbor of U, when it should have been. Of course, we found that out by other 
means. In any case, there is no bug here to fix - as long as a diagram is guaranteed to 
be consistent, the 1 abel s - for bug will not appear again. 

This section has made two points: First, write code that checks the input as 
thoroughly as possible. Second, even when input checking is done, it is still up to 
the user to understand the limitations of the program. 

17.5 History and References 
Guzman (1968) was one of the first to consider the problem of interpreting line 
diagrams. He classified Vertexes, and defined some heuristics for combining information 
from adjacent Vertexes. Huffman (1971) and Clowes (1971) independently 
came up with more formal and complete analyses, and David Waltz (1975) extended 
the analysis to handle shadows, and introduced the constraint propagation algorithm 
to cut down on the need for search. The algorithm is sometimes called "Waltz 
filtering" in his honor. With shadows and nontrihedral angles, there are thousands 
of vertex labelings instead of 18, but there are also more constraints, so the constraint 
propagation actually does better than it does in our limited world. Waltz's approach 
and the Huffman-Clowes labels are covered in most introductory AI books, including 
Rich and Knight 1990, Charniak and McDermott 1985, and Winston 1984, Waltz's 
original paper appears in The Psychology of Computer Vision (Winston 1975), an influential 
volume collecting early work done at MIT. He also contributed a summary 
article on Waltz filtering (Waltz 1990). 

Many introductory AI texts give vision short coverage, but Charniak and McDermott 
(1985) and Tanimoto (1990) provide good overviews of the field. Zucker (1990) 
provides an overview of low-level vision. 

Ramsey and Barrett (1987) give an implementation of a line-recognition program. 
It would make a good project to connect their program to the one presented in this 
chapter, and thereby go all the way from pixels to 3-D descriptions. 


<a id='page-595'></a>

17.6 Exercises 
This chapter has solved the problem of line-labeling for polyhedra made of trihedral 
Vertexes. The following exercises extend this solution. 

&#9635; Exercise 17.1 [h] Use the line-labeling to produce a face labeling. Write a function 
that takes a labeled diagram as input and produces a list of the faces (planes) that 
comprise the diagram. 

&#9635; Exercise 17.2 [h] Use the face labeling to produce a polyhedron labeling. Write 
a function that takes a hst of faces and a diagram and produces a list of polyhedra 
(blocks) that comprise the diagram. 

&#9635; Exercise 17.3 [d] Extend the system to include quad-hedral Vertexes and/or shadows. 
There is no conceptual difficulty in this, but it is a very demanding task to find 
all the possible vertex types and labelings for them. Consult Waltz 1975. 

&#9635; Exercise 17.4 [d] Implement a program to recognize lines from pixels. 

&#9635; Exercise 17.5 [d] If you have access to a workstation with a graphical interface, 
implement a program to allow a user to draw diagrams with a mouse. Have the 
program generate output in the form expected by construct-di agram. 


## Chapter 18
<a id='page-596'></a>

Search and the 
Game of Othello 

In the beginner's mind there are 
endless possibilities; 
in the expert's there are few. 

-Suzuki Roshi, Zen Master 

G
G
ame playing has been the target of much early work in AI for three reasons. First, 
the rules of most games are formalized, and they can be implemented in a computer 
program rather easily. Second, in many games the interface requirements are trivial. 
The computer need only print out its moves and read in the opponent's moves. This is true for 
games like chess and checkers, but not for ping-pong and basketball, where vision and motor 
skills are crucial. Third, playing a good game of chess is considered by many an intellectual 
achievement. Newell, Shaw, and Simon say, "Chess is the intellectual game par excellence " and 
Donald Michie called chess the "Drosophila melanogaster of machine intelligence," meaning that 
chess is a relatively simple yet interesting domain that can lead to advances in AI, just as study 
of the fruit fly served to advance biology. 


<a id='page-597'></a>
Today there is less emphasis on game playing in AI. It has been realized that 
techniques that work well in the limited domain of a board game do not necessarily 
lead to intelligent behavior in other domains. Also, as it turns out, the techniques 
that allow computers to play well are not the same as the techniques that good 
human players use. Humans are capable of recognizing abstract patterns learned 
from previous games, and formulating plans of attack and defense. While some 
computer programs try to emulate this approach, the more succesful programs 
work by rapidly searching thousands of possible sequences of moves, making fairly 
superficial evaluations of the worth of each sequence. 

While much previous work on game playing has concentrated on chess and 
checkers, this chapter demonstrates a program to play the game of Othello.^ Othello 
is a variation on the nineteenth-century game Reversi. It is an easy game to program 
because the rules are simpler than chess. Othello is also a rewarding game to 
program, because a simple search technique can yield an excellent player. There 
are two reasons for this. First, the number of legal moves per turn is low, so the 
search is not too explosive. Second, a single Othello move can flip a dozen or more 
opponent pieces. This makes it difficult for human players to visualize the long-range 
consequences of a move. Search-based programs are not confused, and thus do well 
relative to humans. 

The very name "Othello" derives from the fact that the game is so unpredictable, 
like the Moor of Venice. The name may also be an allusion to the line, "Your daughter 
and the Moor are now making the beast with two backs,"^ since the game pieces 
do indeed have two backs, one white and one black. In any case, the association 
between the game and the play carries over to the name of several programs: Cassio, 
lago, and Bill. The last two will be discussed in this chapter. They are equal to or 
better than even champion human players. We will be able to develop a simplified 
version that is not quite a champion but is much better than beginning players. 

18.1 The Rules of the Game 
Othello is played on a 8-by-8 board, which is initially set up with four pieces in the 
center, as shown in figure 18.1. The two players, black and white, alternate turns, 
with black playing first. On each turn, a player places a single piece of his own color 
on the board. No piece can be moved once it is placed, but subsequent moves may 
flip a piece from one color to another. Each piece must be placed so that it brackets 
one or more opponent pieces. That is, when black plays a piece there must be a 
line (horizontal, vertical, or diagonal) that goes through the piece just played, then 
through one or more white pieces, and then to another black piece. The intervening 

^Othello is a registered trademark of CBS Inc. Gameboard design © 1974 CBS Inc. 

^Othelh [I. i. 117] WiUiam Shakespeare. 


<a id='page-598'></a>

white pieces are flipped over to black. If there are bracketed white pieces in more 
than one direction, they are all flipped. Figure 18.2 (a) indicates the legal moves for 
black with small dots. Figure 18.2 (b) shows the position after black moves to square 
b4. Players alternate turns, except that a player who has no legal moves must pass. 
When neither player has any moves, the game is over, and the player with the most 
pieces on the board wins. This usually happens because there are no empty squares 
left, but it occasionally happens earlier in the game. 

f g h 

O'o ' 

Figure 18.1: The Othello Board 

. f g . f g h 

o o 

o o '' ' ' ' 
'' ' 

O o o o o o o

'o ' ' ' 

o 

(b) 

Figure 18.2: Legal Othello Moves 


<a id='page-599'></a>
18.2 Representation Choices 
In developing an Othello program, we will want to test out various strategies, playing 
those strategies against each other and against human players. We may also want 
our program to allow two humans to play a game. Therefore, our main function, 
othel 10, will be a monitoring function that takes as arguments two strategies. It 
uses these strategies to get each player's moves, and then applies these moves to a 
representation of the game board, perhaps printing out the board as it goes. 

The first choice to make is how to represent the board and the pieces on it. The 
board is an 8-by-8 square, and each square can be filled by a black or white piece or 
can be empty. Thus, an obvious representation choice is to make the board an 8-by-8 
array, where each element of the array is the symbol bl ack, whi te, or ni 1. 

Notice what is happening here: we are following the usual Lisp convention of 
implementing an enumerated type (the type of pieces that can fill a square) as a set 
of symbols. This is an appropriate representation because it supports the primary 
operation on elements of an enumerated type: test for equality using eq. It also 
supports input and output quite handily. 

In many other languages (such as C or Pascal), enumerated types are implemented 
as integers. In Pascal one could declare: 

type piece = (black, white, empty); 

to define pi ece as a set of three elements that is treated as a subtype of the integers. 
The language does not allow for direct input and output of such types, but equality 
can be checked. An advantage of this approach is that an element can be packed into 
a small space. In the Othello domain, we anticipate that efficiency will be important, 
because one way to pick a good move is to look at a large number of possible sequences 
of moves, and choose a sequence that leads toward a favorable result. Thus, we are 
willing to look hard at alternative representations to find an efficient one. It takes 
only two bits to represent one of the three possible types, while it takes many more 
(perhaps 32) to represent a symbol. Thus, we may save space by representing pieces 
as small integers rather than symbols. 

Next, we consider the board. The two-dimensional array seems like such an 
obvious choice that it is hard to imagine a better representation. We could consider 
an 8-element list of 8-element lists, but this would just waste space (for the cons 
cells) and time (in accessing the later elements of the lists). However, we will have to 
implement two other abstract data types that we have not yet considered: the square 
and the direction. We will need, for example, to represent the square that a player 
chooses to move into. This will be a pair of integers, such as 4,5. We could represent 
this as a two-element list, or more compactly as a cons cell, but this still means that 
we may have to generate garbage (create a cons cell) every time we want to refer 
to a new square. Similarly, we need to be able to scan in a given direction from a 


<a id='page-600'></a>

square, looking for pieces to flip. Directions will be represented as a pair of integers, 
such as +1,-1. One clever possibility is to use complex numbers for both squares and 
directions, with the real component mapped to the horizontal axis and the imaginary 
component mapped to the vertical axis. Then moving in a given direction from a 
square is accomplished by simply adding the direction to the square. But in most 
implementations, creating new complex numbers will also generate garbage. 

Another possibiUty is to represent squares (and directions) as two distinct integers, 
and have the routines that manipulate them accept two arguments instead of 
one. This would be efficient, but it is losing an important abstraction: that squares 
(and directions) are conceptually single objects. 

A way out of this dilemma is to represent the board as a one-dimensional vector. 
Squares are represented as integers in the range 0 to 63. In most implementations, 
small integers (fixnums) are represented as immediate data that can be manipulated 
without generating garbage. Directions can also be implemented as integers, representing 
the numerical difference between adjacent squares along that direction. To 
get a feel for this, take a look at the board: 

0 1 2 3 4 5 6 7 
8 9 10 11 12 13 14 15 
16 17 18 19 20 21 22 23 
24 25 26 27 28 29 30 31 
32 33 34 35 36 37 38 39 
40 41 42 43 44 45 46 47 
48 49 50 51 52 53 54 55 
56 57 58 59 60 61 62 63 

You can see that the direction +1 corresponds to movement to the right, +7 corresponds 
to diagonal movement downward and to the left, +8 is downward, and +9 is 
diagonally downward and to the right. The negations of these numbers (-1, -7, -8, -9) 
represent the opposite directions. 

There is one complication with this scheme: we need to know when we hit the 
edge of the board. Starting at square 0, we can move in direction +1 seven times to 
arrive at the right edge of the board, but we aren't allowed to move in that direction 
yet again to arrive at square 8. It is possible to check for the edge of the board by 
considering quotients and remainders modulo 8, but it is somewhat complicated and 
expensive to do so. 

A simpler solution is to represent the edge of the board explicitly, by using a 100element 
vector instead of a 64-element vector. The outlying elements are filled with a 
marker indicating that they are outside the board proper. This representation wastes 
some space but makes edge detection much simpler. It also has the minor advantage 
that legal squares are represented by numbers in the range 11-88, which makes them 
easier to understand while debugging. Here's the new 100-element board: 


<a id='page-601'></a>
0 1 2 3 4 5 6 7 8 9 
10 11 12 13 14 15 16 17 18 19 
20 21 22 23 24 25 26 27 28 29 
30 31 32 33 34 35 36 37 38 39 
40 41 42 43 44 45 46 47 48 49 
50 51 52 53 54 55 56 57 58 59 
60 61 62 63 64 65 66 67 68 69 
70 71 72 73 74 75 76 77 78 79 
80 81 82 83 84 85 86 87 88 89 
90 91 92 93 94 95 96 97 98 99 

The horizontal direction is now &plusmn;1, vertical is &plusmn;10, and the diagonals are &plusmn;9 and 
&plusmn;11. We'll tentatively adopt this latest representation, but leave open the possibility 
of changing to another format. With this much decided, we are ready to begin. 
Figure 18.3 is the glossary for the complete program. A glossary for a second version 
of the program is on [page 623](chapter18.md#page-623). 

What follows is the code for directions and pieces. We explicitly define the type 
piece to be a number from empty to outer (0 to 3), and define the function name-of 
to map from a piece number to a character: a dot for empty, @ for black, 0 for white, 
and a question mark (which should never be printed) for outer. 

(defconstant all-directions '(-11 -10 -9-119 10 ID) 

(defconstant empty 0 "An empty square") 
(defconstant black 1 "A black piece") 
(defconstant white 2 "A white piece") 
(defconstant outer 3 "Marks squares outside the 8x8 board") 

(deftype piece () '(integer .empty .outer)) 

(defun name-of (piece) (char ".@0?" piece)) 

(defun opponent (player) (if (eql player black) white black)) 

And here is the code for the board. Note that we introduce the function bref, 
for "board reference" rather than using the built-in function aref. This facilitates 
possible changes to the representation of boards. Also, even though there is no 
contiguous range of numbers that represents the legal squares, we can define the 
constant a 11 - squa res to be a list of the 64 legal squares, computed as those numbers 
from 11 to 88 whose value mod 10 is between 1 and 8. 

(deftype board () '(simple-array piece (100))) 

(defun bref (board square) (aref board square)) 
(defsetf bref (board square) (val) 
'(setf (aref .board .square) .val)) 


<a id='page-602'></a>

Othello 

empty 
black 
white 
outer 
all-directions 
all-squares 
winning-value 
losing-value 

piece 

board 

get-move 
make-move 
human 
random-strategy 
maximi ze-di fference 

maximizer 
weighted-squares 
modified-weighted-squares 
mi .imax 
minimax-searcher 
alpha-beta 
alpha-beta-searcher 

bref 
copy-board 
initial-board 
print-board 
count-difference 
name-of 
opponent 
valid-p 
legal-p 

make-flips 
would-flip? 
find-bracketing-piece 
any-legal-move? 
next-to-play 
legal-moves 
final-value 
neighbors 
switch-strategies 

random-elt 

Top-Level Function 

Play a game of Othello. Return the score. 

Constants 

0 represents an empty square. 
1 represents a black piece. 
2 represents a white piece. 
3 represents a piece outside the 8x8 board. 
A list of integers representing the eight directions. 
A list of all legal squares. 
The best possible evaluation. 
The worst possible evaluation. 

Data Types 

An integer from empty to outer. 
A vector of 100 pieces. 

Major Functions 

Call the player's strategy function to get a move. 

Update board to reflect move by player. 
A strategy that prompts a human player. 
Make any legal move. 

A strategy that maximizes the difference in pieces. 
Return a strategy that maximizes some measure. 
Sum of the weights of player's squares minus opponent's. 
Like above, but treating corners better. 
Find the best move according to EVAL.FN, searching PLY levels. 
Return a strategy that uses mi.i max to search. 
Find the best move according to EVAL-FN, searching PLY levels. 
Return a strategy that uses al pha- beta to search. 

Auxiliary Functions 

Reference to a position on the board. 
Make a new board. 
Return a board, empty except for four pieces in the middle. 
Print a board, along with some statistics. 
Count player's pieces minus opponent's pieces. 
A character used to print a piece. 
The opponent of black is white, and vice-versa. 
A syntactically vahd square. 
A legal move on the board. 
Make any flips in the given direction. 
Would this move result in any flips in this direction? 
Return the square number of the bracketing piece. 
Does player have any legal moves in this position? 
Compute the player to move next, or NIL if nobody can move. 
Returns a list of legal moves for player. 
Is this a win, loss, or draw for player? 
Return a list of all squares adjacent to a square. 
Play one strategy for a while, then switch. 

Previously Defined Functions 

Choose a random element from a sequence, (pg. 36) 

Figure 18.3: Glossary for the Othello Program 


<a id='page-603'></a>

(defun copy-board (board) 
(copy-seq board)) 

(defconstant all-squares 
(loop for i from 11 to 88 when (<= 1 (mod i 10) 8) collect i)) 

(defun initial-board () 

"Return a board, empty except for four pieces in the middle." 
Boards are 100-element vectors, with elements 11-88 used, 
and the others marked with the sentinel OUTER. Initially 
the 4 center squares are taken, the others empty, 

(let ((board (make-array 100 :element-type 'piece 
:initial-element outer))) 
(dolist (square all-squares) 
(setf (bref board square) empty)) 
(setf (bref board 44) white (bref board 45) black 
(bref board 54) black (bref board 55) white) 
board)) 

(defun print-board (board) 
"Print a board, along with some statistics." 
(format t "~2& 1 2 3 4 5 6 7 8 [~c=~2a ~c=''2a (~@d)]" 

(name-of black) (count black board) 
(name-of white) (count white board) 
(count-difference black board)) 

(loop for row from 1 to 8 do 
(format t "-& ~d " (* 10 row)) 
(loop for col from 1 to 8 

for piece = (bref board (+ col (* 10 row))) 
do (format t ""c " (name-of piece)))) 
(format t "~2&")) 

(defun count-difference (player board) 
"Count player's pieces minus opponent's pieces." 
(- (count player board) 

(count (opponent player) board))) 

Now let's take a look at the initial board, as it is printed by pri nt - boa rd, and by a raw 
wri te (I added the line breaks to make it easier to read): 


<a id='page-604'></a>

> (write (initial-board ) > (print-board (initial-board) ) 
rarray t) 
#(3 33333333 3 1234567 8 C@=2 0=2 (-^0)1 
300000000 3 10 
300000000 3 20 
300000000 3 30 
300021000 3 40...0@.. . 
300012000 3 50 . ..@0.. . 
300000000 3 60 
300000000 3 70 
300000000 3 80 
33333333 3 3) 
#<ART-2B-100 -72570734> NIL 

Notice that pri nt - boa rd provides some additional information: the number of pieces 
that each player controls, and the difference between these two counts. 

The next step is to handle moves properly: given a board and a square to move 
to, update the board to reflect the effects of the player moving to that square. This 
means flipping some of the opponent's pieces. One design decision is whether the 
procedure that makes moves, make-move, will be responsible for checking for error 
conditions. My choice is that make - move assumes it will be passed a legal move. That 
way, a strategy can use the function to explore sequences of moves that are known to 
be valid without slowing make - move down. Of course, separate procedures will have 
to insure that a move is legal. Here we introduce two terms: a valid move is one that 
is syntactically correct: an integer from 11 to 88 that is not off the board. A legal move 
is a valid move into an empty square that will flip at least one opponent. Here's the 
code: 

(defun valid-p (move) 

"Valid moves are numbers in the range 11-88 that end in 1-8." 

(and (integerp move) (<= 11 move 88) (<= 1 (mod move 10) 8))) 

(defun legal-p (move player board) 

"A Legal move must be into an empty square, and it must 

flip at least one opponent piece." 

(and (eql (bref board move) empty) 
(some #'(lambda (dir) (would-flip? move player board dir)) 

all-directions))) 

(defun make-move (move player board) 

"Update board to reflect move by player" 

First make the move, then make any flips 
(setf (bref board move) player) 
(dolist (dir all-directions) 

(make-flips move player board dir)) 
board) 


<a id='page-605'></a>
Now all we need is to make-fl ips. To do that, we search in all directions for a 
bracketing piece: a piece belonging to the player who is making the move, which 
sandwiches a string of opponent pieces. If there are no opponent pieces in that 
direction, or if an empty or outer piece is hit before the player's piece, then no flips 
are made. Note that would-f 1 ip? is a semipredicate that returns false if no flips 
would be made in the given direction, and returns the square of the bracketing piece 
if there is one. 

(defun make-flips (move player board dir) 
"Make any flips in the given direction." 
(let ((bracketer (would-flip? move player board dir))) 

(when bracketer 
(loop for c from (+ move dir) by dir until (eql c bracketer) 
do (setf (bref board c) player))))) 

(defun would-flip? (move player board dir) 
"Would this move result in any flips in this direction? 
If so. return the square number of the bracketing piece." 

A flip occurs if, starting at the adjacent square, c. there 
is a string of at least one opponent pieces, bracketed by 
one of player's pieces 

(let ((c (+ move dir))) 
(and (eql (bref board c) (opponent player)) 
(find-bracketing-piece (+ c dir) player board dir)))) 

(defun find-bracketing-piece (square player board dir) 
"Return the square number of the bracketing piece." 
(cond ((eql (bref board square) player) square) 

((eql (bref board square) (opponent player)) 
(find-bracketing-piece (+ square dir) player board dir)) 
(t nil))) 

Finally we can write the function that actually monitors a game. But first we are 
faced with one more important choice: how will we represent a player? We have 
already distinguished between black and white's pieces, but we have not decided 
how to ask black or white for their moves. I choose to represent player's strategies 
as functions. Each function takes two arguments: the color to move (black or white) 
and the current board. The function should return a legal move number. 

(defun Othello (bl-strategy wh-strategy &optional (print t)) 
"Play a game of Othello. Return the score, where a positive 
difference means black (the first player) wins." 
(let ((board (initial-board))) 

(loop for player = black 
then (next-to-play board player print) 
for strategy = (if (eql player black) 


<a id='page-606'></a>

bl-strategy 

wh-strategy) 
until (null player) 
do (get-move strategy player board print)) 

(when print 
(format t "~&The game is over. Final result:") 
(print-board board)) 

(count-difference black board))) 

We need to be able to determine who plays next at any point. The rules say that 
players alternate turns, but if one player has no legal moves, the other can move 
again. When neither has a legal move, the game is over. This usually happens 
because there are no empty squares left, but it sometimes happens earlier in the 
game. The player with more pieces at the end of the game wins. If neither player has 
more, the game is a draw. 

(defun next-to-play (board previous-player print) 
"Compute the player to move next, or NIL if nobody can move." 
(let ((opp (opponent previous-player))) 

(cond ((any-legal-move? opp board) opp) 
((any-legal-move? previous-player board) 
(when print 
(format t ""&^C has no moves and must pass." 
(name-of opp))) 
previous-player) 
(t nil)))) 

(defun any-legal-move? (player board) 
"Does player have any legal moves in this position?" 
(some #'(lambda (move) (legal-p move player board)) 

all-squares)) 

Note that the argument print (of Othello, next-to-play, and below, get-move) 
determines if information about the progress of the game will be printed. For an 
interactive game, pri nt should be true, but it is also possible to play a "batch" game 
with pri nt set to false. 

In get - move below, the player's strategy function is called to determine his move. 
Illegal moves are detected, and proper moves are reported when pri nt is true. The 
strategy function is passed a number representing the player to move (black or white) 
and a copy of the board. If we passed the real game board, the function could cheat 
by changing the pieces on the board! 


<a id='page-607'></a>

(defun get-move (strategy player board print) 
"Call the player's strategy function to get a move. 
Keep calling until a legal move is made." 
(when print (print-board board)) 
(let ((move (funcall strategy player (copy-board board)))) 

(cond 
((and (valid-p move) (legal-p move player board)) 
(when print 
(format t "'^&'. moves to ~d." (name-of player) move)) 
(make-move move player board)) 
(t (warn "Illegal move: ~d" move) 
(get-move strategy player board print))))) 

Here we define two simple strategies: 

(defun human (player board) 
"A human player for the game of Othello" 
(declare (ignore board)) 
(format t "~&~c to move: " (name-of player)) 
(read)) 

(defun random-strategy (player board) 
"Make any legal move." 
(random-elt (legal-moves player board))) 

(defun legal-moves (player board) 
"Returns a list of legal moves for player" 
(loop for move in all-squares 

when (legal-p move player board) collect move)) 

We are now in a position to play the game. The expression 
(othel 1 0 # * human #'human) will let two people play against each other. Alternately, 
(othel lo #'random-strategy #'human) will allow us to match our wits against a 
particularly poor strategy. The rest of this chapter shows how to develop a better 
strategy. 

18.3 Evaluating Positions 
The random-move strategy is, of course, a poor one. We would like to make a good 
move rather than a random move, but so far we don't know what makes a good 
move. The only positions we are able to evaluate for sure are final positions: when 
the game is over, we know that the player with the most pieces wins. This suggests a 
strategy: choose the move that maximizes count-di f f erence, the piece differential. 


<a id='page-608'></a>

The function maxi mize-di ff erence does just that. It calls maxi mi zer, a higher-order 

function that chooses the best move according to an arbitrary evaluation function. 

(defun maximize-difference (player board) 
"A strategy that maximizes the difference in pieces." 
(funcall (maximizer #'count-difference) player board)) 

(defun maximizer (eval-fn) 
"Return a strategy that will consider every legal move, 
apply EVAL-FN to each resulting board, and choose 
the move for which EVAL-FN returns the best score. 
FN takes two arguments: the player-to-move and board" 
#*(lambda (player board) 

(let* ((moves (legal-moves player board)) 
(scores (mapcar #'(lambda (move) 

(funcall 
eval-fn 
player 
(make-move move player 

(copy-board board)))) 
moves)) 
(best (apply #*max scores))) 
(elt moves (position best scores))))) 

&#9635; Exercise 18.1 Playsomegameswithmaximize -differenceagainst random-strategy 
and human. How good is maximize-difference? 
Those who complete the exercise will quickly see that the maximi ze-di ff erence 
player does better than random, and may even beat human players in their first game 
or two. But most humans are able to improve, learning to take advantage of the 
overly greedy play of maximi ze-di ff erence. Humans learn that the edge squares, 
for example, are valuable because the player dominating the edges can surround the 
opponent, while it is difficult to recapture an edge. This is especially true of corner 
squares, which can never be recaptured. 
Using this knowledge, a clever player can temporarily sacrifice pieces to obtain 
edge and corner squares in the short run, and win back pieces in the long run. 
We can approximate some of this reasoning with the weighted-squa res evaluation 
function. Like count-difference, it adds up all the player's pieces and subtracts 
the opponents, but each piece is weighted according to the square it occupies. Edge 
squares are weighted highly, corner squares higher still, and squares adjacent to the 
corners and edges have negative weights, because occupying these squares often 
gives the opponent a means of capturing the desirable square. Figure 18.4 shows 
the standard nomenclature for edge squares: X, A, B, and C. In general, X and C 


<a id='page-609'></a>
squares are to be avoided, because taking them gives the opponent a chance to take 
the corner. The wei ghted-squares evaluation function reflects this. 

a b c d e f g h 

1 c A . . A C 

2 X X

c c 

3 A A 
4 . . 
5 .

. 
6 A A 

7 C X X C 

8 c A . . A c 

Figure 18.4: Names for Edge Squares 

(defparameter ^weights* 

'#(0 0 0 0 0 0 0 0 0 0 
0 120 -20 20 5 5 20 -20 120 0 
0 -20 -40 -5 -5 -5 -5 -40 -20 0 
0 20 -5 15 3 3 15 -5 20 0 
5-5333 3 -5 5 0 
5-5333 3 -5 5 0 
20 -5 15 3 3 15 -5 20 0 
-20 -40 -5 -5 -5 -5 -40 -20 0 
120 -20 20 5 5 20 -20 120 0 

0 00000 0 0 0)) 

(defun weighted-squares (player board) 
"Sum of the weights of player's squares minus opponent's." 
(let ((opp (opponent player))) 

(loop for i in all-squares 
when (eql (bref board i) player) 
sum (aref *weights* i) 
when (eql (bref board i) opp) 
sum (- (aref ^weights* i))))) 

&#9635; Exercise 18.2 Compare strategies by evaluating the two forms below. What happens? 
Is this a good test to determine which strategy is better? 


<a id='page-610'></a>

(Othello (maximizer #'weighted-squares) 
(maximizer #*count-difference) nil) 

(Othello (maximizer #'count-difference) 
(maximizer #'weighted-squares) nil) 

18.4 Searching Ahead: Minimax 
Even the weighted-squares strategy is no match for an experienced player. There 
are two ways we could improve the strategy. First, we could modify the evaluation 
function to take more information into account. But even without changing the 
evaluation function, we can improve the strategy by searching ahead. Instead of 
choosing the move that leads immediately to the highest score, we can also consider 
the opponent's possible replies, our replies to those replies, and so on. By searching 
through several levels of moves, we can steer away from potential disaster and find 
good moves that were not immediately apparent. 

Another way to look at the maxi mi zer function is as a search function that searches 
only one level, or ply, deep: 

The top of the tree is the current board position, and the squares below that indicate 
possible moves. The maxi mi zer function evaluates each of these and picks the best 
move, which is underlined in the diagram. 

Now let's see how a 3-ply search might go. The first step is to apply maxi mi zer to 
the positions just above the bottom of the tree. Suppose we get the following values: 


<a id='page-611'></a>

Each position is shown as having two possible legal moves, which is unreahstic 
but makes the diagram fit on the page. In a real game, five to ten legal moves per 
position is typical. The values at the leaves of the tree were computed by applying 
the evaluation function, while the values one level up were computed by maxi mi zer. 
The result is that we know what our best move is for any of the four positions just 
above the bottom of the tree. 

Going up a level, it is the opponent's turn to move. We can assume the opponent 
will choose the move that results in the minimal value to us, which would be the 
maximal value to the opponent. Thus, the opponent's choices would be the 10- and 
9-valued positions, avoiding the 20- and 23-valued positions. 


<a id='page-612'></a>

Now it is our turn to move again, so we apply maxi mi zer once again to get the final 
value of the top-level position: 

If the opponent plays as expected, we will always follow the left branch of the tree 
and end up at the position with value 10. If the opponent plays otherwise, we will 
end up at a position with a better value. 

This kind of search is traditionally called a minimax search, because of the alternate 
application of the maxi mi zer and a hypothetical mi ni mi zer function. Notice that only 
the leaf positions in the tree are looked at by the evaluation function. The value of all 
other positions is determined by minimizing and maximizing. 

We are almost ready to code the minimax algorithm, but first we have to make 
a few design decisions. First, we could write two functions, mi nimax and maxi mi n, 
which correspond to the two players' analyses. However, it is easier to write a single 
function that maximizes the value of a position for a particular player. In other words, 
by adding the player as a parameter, we avoid having to write two otherwise identical 
functions. 

Second, we have to decide if we are going to write a general minimax searcher 
or an Othello-specific searcher. I decided on the latter for efficiency reasons, and 
because there are some Othello-specific complications that need to be accounted for. 
First, it is possible that a player will not have any legal moves. In that case, we want 
to continue the search with the opponent to move. If the opponent has no moves 
either, then the game is over, and the value of the position can be determined with 
finality by counting the pieces. 

Third, we need to decide the interaction between the normal evaluation function 
and this final evaluation that occurs when the game is over. We could insist that 


<a id='page-613'></a>
each evaluation function determine when the game is over and do the proper computation. 
But that overburdens the evaluation functions and may lead to wasteful 
checking for the end of game. Instead, I implemented a separate f i nal - val ue evaluation 
function, which returns 0 for a draw, a large positive number for a win, and 
a large negative number for a loss. Because fixnum arithmetic is most efficient, the 
constants most-positive-fixnum and most-negative-fixnum are used. The evaluation 
functions must be careful to return numbers that are within this range. All 
the evaluation functions in this chapter will be within range if fixnums are 20 bits 
or more. 

In a tournament, it is not only important who wins and loses, but also by how 
much. If we were trying to maximize the margin of victory, then f i na1 - va1 ue would 
be changed to include a small factor for the final difference. 

(defconstant winning-value most-positive-fixnum) 
(defconstant losing-value most-negative-fixnum) 

(defun final-value (player board) 
"Is this a win. loss, or draw for player?" 
(case (Signum (count-difference player board)) 

(-1 losing-value) 
( 0 0) 
(+1 winning-value))) 

Fourth, and finally, we need to decide on the parameters for the minimax function. 
Like the other evaluation functions, it needs the player to move and the current board 
as parameters. It also needs an indication of how many ply to search, and the static 
evaluation function to apply to the leaf positions. Thus, minimax will be a function 
of four arguments. What will it return? It needs to return the best move, but it also 
needs to return the value of that move, according to the static evaluation function. 
We use multiple values for this. 

(defun minimax (player board ply eval-fn) 
"Find the best move, for PLAYER, according to EVAL-FN. 
searching PLY levels deep and backing up values." 
(if (= ply 0) 

(funcall eval-fn player board) 
(let ((moves (legal-moves player board))) 
(if (null moves) 
(if (any-legal-move? (opponent player) board) 
(- (minimax (opponent player) board 
(- ply 1) eval-fn)) 
(final-value player board)) 
(let ((best-move nil) 
(best-val nil)) 
(dolist (move moves) 


<a id='page-614'></a>

(let* ((board2 (make-move move player 
(copy-board board))) 

(val (- (minimax 
(opponent player) board2 
(- ply 1) eval-fn)))) 

(when (or (null best-val) 

(> val best-val)) 
(setf best-val val) 
(setf best-move move)))) 

(values best-val best-move)))))) 

The mi . i max function cannot be used as a strategy function as is, because it takes too 
many arguments and returns too many values. The functional minimax-searcher 
returns an appropriate strategy. Remember that a strategy is a fimction of two 
arguments: the player and the board, get-move is responsible for passing the right 
arguments to the function, so the strategy need not worry about where the arguments 
come from. 

(defun minimax-searcher (ply eval-fn) 
"A strategy that searches PLY levels and then uses EVAL-FN." 
#*(lambda (player board) 

(multiple-value-bind (value move) 

(minimax player board ply eval-fn) 
(declare (ignore value)) 
move))) 

We can test the minimax strategy, and see that searching ahead 3 ply is indeed better 
than looking at only 1 ply. I show only the final result, which demonstrates that it is 
indeed an advantage to be able to look ahead: 

> (Othello (minimax-searcher 3 #*count-difference) 
(maximizer #'count-difference)) 

The game is over. Final result: 

12 3 4 5 6 7 8 [@=53 0=0 (+53)] 

20@@@@@@@@ 

30@@@@@@@@ 
40@@@@@@@@ 
50@@@@@@@@ 

60 . . @@ @@ @ @ 
70 . . . @ @ @ @ @ 
80 . . . . @ @ . . 

<a id='page-615'></a>

18.5 Smarter Searching: Alpha-Beta Search 
The problem with a full minimax search is that it considers too many positions. It 
looks at every line of play, including many improbable ones. Fortunately, there is a 
way to find the optimal line of play without looking at every possible position. Let's 
go back to our familiar search tree: 

Here we have marked certain positions with question marks. The idea is that the 
whole search tree evaluates to 10 regardless of the value of the positions labeled ?i. 
Consider the position labeled ?i. It does not matter what this position evaluates to, 
because the opponent will always choose to play toward the 10-position, to avoid the 
possibility of the 15. Thus, we can cut off the search at this point and not consider 
the ?-position. This kind of cutoff has historically been called a beta cutoff. 

Now consider the position labeled ?4. It does not matter what this position 
evaluates to, because we will always prefer to choose the 10 position at the left 
branch, rather than giving the opponent a chance to play to the 9-position. This is an 
alpha cutoff. Notice that it cuts off a whole subtree of positions below it (labeled ?2 
and ?3). 

In general, we keep track of two parameters that bound the true value of the 
current position. The lower bound is a value we know we can achieve by choosing a 
certain line of play. The idea is that we need not even consider moves that will lead 
to a value lower than this. The lower bound has traditionally been called alpha, but 
we will name it achi evabl e. The upper bound represents a value the opponent can 
achieve by choosing a certain line of play. It has been called beta, but we will call it 
cutoff. Again, the idea is that we need not consider moves with a higher value than 
this (because then the opponent would avoid the move that is so good for us). The 


<a id='page-616'></a>

alpha-beta algorithm is just minimax, but with some needless evaluations pruned by 
these two parameters. 

In deeper trees with higher branching factors, many more evaluations can be 
pruned. In general, a tree of depth d and branching factor b requires b^ evaluations 
for full minimax, and as few as 6^/^ evaluations with alpha-beta minimax. 

To implement alpha-beta search, we add two more parameters to the function 
minimax and rename it alpha-beta, achievable is the best score the player can 
achieve; it is what we want to maximize. The cutoff is a value that, when exceeded, 
will make the opponent choose another branch of the tree, thus making the rest of 
the current level of the tree irrelevant. The test unti 1 (>= achi evabl e cutoff) in 
the penultimate line of minimax does the cutoff; all the other changes just involve 
passing the parameters around properly. 

(defun alpha-beta (player board achievable cutoff ply eval-fn) 
"Find the best move, for PLAYER, according to EVAL-FN, 
searching PLY levels deep and backing up values, 
using cutoffs whenever possible." 
(if (= ply 0) 

(funcall eval-fn player board) 
(let ((moves (legal-moves player board))) 
(if (null moves) 
(if (any-legal-move? (opponent player) board) 

(- (alpha-beta (opponent player) board 
(- cutoff) (- achievable) 
(- ply 1) eval-fn)) 

(final-value player board)) 
(let ((best-move (first moves))) 
(loop for move in moves do 
(let* ((boardZ (make-move move player 
(copy-board board))) 

(val (- (alpha-beta 
(opponent player) board2 
(- cutoff) (- achievable) 
(- ply 1) eval-fn)))) 

(when (> val achievable) 
(setf achievable val) 
(setf best-move move))) 

until (>= achievable cutoff)) 
(values achievable best-move)))))) 

(defun alpha-beta-searcher (depth eval-fn) 
"A strategy that searches to DEPTH and then uses EVAL-FN." 
#.(lambda (player board) 

(multiple-value-bind (value move) 
(alpha-beta player board losing-value winning-value 
depth eval-fn) 


<a id='page-617'></a>
(declare (ignore value)) 
move))) 

It must be stressed that a 1 pha- beta computes the exact same result as the full-search 
version of mi . i max. The only advantage of the cutoffs is making the search go faster 
by considering fewer positions. 

18.6 An Analysis of Some Games 
Now is a good time to stop and analyze where we have gone. We've demonstrated a 
program that can play a legal game of Othello, and some strategies that may or may 
not play a good game. First, we'll look at some individual games to see the mistakes 
made by some strategies, and then we'll generate some statistics for series of games. 

Is the weighted-squares measure a good one? We can compare it to a strategy of 
maximizing the number of pieces. Such a strategy would of course be perfect if it 
could look ahead to the end of the game, but the speed of our computers limits us 
to searching only a few ply, even with cutoffs. Consider the following game, where 
black is maximizing the difference in the number of pieces, and white is maximizing 
the weighted sum of squares. Both search to a depth of 4 ply: 

> (Othello (alpha-beta-searcher 4 #'count-difference) 
(alpha-beta-searcher 4 #*weighted-squares)) 

Black is able to increase the piece difference dramatically as the game progresses. 
After 17 moves, white is down to only one piece: 

12 3 4 5 6 7 8 [@=20 0=1 (+19)] 
10 0 @ 

20 . @ . . . @ @ . 
30 @ @ @ @ @ @ . . 
40 . @ . @ @ . . . 
50 @ @ @@ @ @ . . 
60 . @ 
70 
80 
Although behind by 19 points, white is actually in a good position, because the piece 
in the corner is safe and threatens many of black's pieces. White is able to maintain 
good position while being numerically far behind black, as shown in these positions 
later in the game: 


<a id='page-618'></a>

12 3 4 5 6 7 8 [e=32 0=15 (+17)] 
10 0 0 0 0 @ @ 0 0 
20 @ @0 @ @ @ @ @ 
30 @ @ 0 0 @ 0 @ @ 
40 0 0 @ @ @ @ @ @ 
50 @0 @ @ @ @ 
60 @ @0 @ @ 0 
70 @ . . @ @ . 
80 
1 2 3 4 5 6 7 8 [@=34 0=19 (+15)] 
10 0 0 0 0 @ @ 0 0 
20 @ @0 @ @ @ @ @ 
30 @ @ 0 0 @ 0 @ @ 
40 0 @ 0 @ @ @ @ @ 
50 0 @ 0 @ @ @ @ . 
60 0 @ 0 @ @ @ 
70 0 @ @ @ @ . 
80 0 @ 0 . 

After some give-and-take, white gains the advantage for good by capturing eight 
pieces on a move to square 85 on the third-to-last move of the game: 

1 2 3 4 5 6 7 8 [@=31 0=30 (+1)] 
10 0 0 0 0 @ @ 0 0 
20 @ @ 0 0 @ @ @ 0 
30 @ @ 0 0 0@ @ 0 
40 0 @ 0 0 0@ @ 0 
50 0 @ 0 @ 0 @ @ 0 

60 0 @ 0 @ @ @@ 0 

70 0 @ @ @ @ @0 0 

80 0 @ @ @ . . ' 0 

0 moves to 85. 

1 2 3 4 5 6 7 8 [@=23 0=39 (-16)] 
10 0 0 0 0 @ @ 0 0 
20 @ @0 0 @ @ @ 0 
30 @ @0 0 0@ @ 0 
40 0 @ 0 0 0@ @ 0 
50 0 @ 0 @0 @ @ 0 
60 0 @ 0 @ 0 @ 0 0 
70 0 @ @ 0 0 0 0 0 
80 0 0 0 0 0 . ' 0 

@ moves to 86. 


<a id='page-619'></a>
12 3 4 5 6 7 8 [@=26 0=37 (-11)] 
10 0000@@00 
20@@00@@@0 
30@@000@@0 
40 0@000@@0 
50 0@0@0@@0 
60 0@0@0@00 
70 0@@0@@00 
80 00000@.0 

0 moves to 87. 
The game is over. Final result: 

1 2 3 4 5 6 7 8 [@=24 0=40 (-16)] 
10 0000@@00 
20@@00@@@0 
30@@000@@0 
40 0@000@@0 
50 0@0@0@@0 
60 0@0@0@00 
70 0@@0@000 
80 00000000 

-16 

White ends up winning by 16 pieces. Black's strategy was too greedy: black was 
willing to give up position (all four corners and all but four of the edge squares) for 
temporary gains in material. 

Increasing the depth of search does not compensate for a faulty evaluation function. 
In the following game, black's search depth is increased to 6 ply, while white's 
is kept at 4. The same things happen, although black's doom takes a bit longer to 
unfold. 

> (Othello (alpha-beta-searcher 6 #'count-difference) 
(alpha-beta-searcher 4 #'weighted-squares)) 

Black slowly builds up an advantage: 

12 3 4 5 6 7 8 [@=21 0=8 (+13)] 

10 . . @ @ @ @ @ 
20 . @ . @ 0 @ . 
30 0@@0@00 
40 . @. @ 0 @ 0 
50. @ @ @ @ @ . 
60 . @ . @ . 0 . 
70 
80 

<a id='page-620'></a>

But at this point white has clear access to the upper left corner, and through that 
corner threatens to take the whole top edge. Still, black maintains a material edge as 
the game goes on: 

12 3 4 5 6 7 8 [@=34 0=11 (+23)] 

10 0 . @ @ @ @@. 
20 . 0 0 @ @ @ . . 
30 0@00@@@@ 
40@@@@0@@ . 
50@@@@@0@. 
60@@@@@@00 
70 @ . . @ . . @ 0 
80 
But eventually white's weighted-squares strategy takes the lead: 

12 3 4 5 6 7 8 [@=23 0=27 (-4)] 
10 00 0 00000 
20 @ @ 0 @ @ @ . . 
30 0@00@@@@ 
40 0@0@0@@ . 
50 0@0@@0@ . 
60 000@@@00 

70 0 . 0 @ . . @ 0 
800 
and is able to hold on to win: 

12 3 4 5 6 7 8 [@=24 0=40 (-16)] 
10 00000000 
20@@0@00@@ 
30 0@00@@@@ 
40 0@00@@@0 
50 00@@0@00 
60 000@0@@0 
70 0000@@00 
80 00000@@0 

-16 

This shows that brute-force searching is not a panacea. While it is helpful to be able 
to search deeper, greater gains can be made by making the evaluation function more 
accurate. There are many problems with the weighted-squares evaluation function. 
Consider again this position from the first game above: 


<a id='page-621'></a>

12 3 4 5 6 7 8 [@=20 0=1 (+19)] 
10 0 @ 

20 . @ . . . @@ . 
30 @ @ @ @@ @ . . 
40 . @ . @ @ . . . 
50 @ @@ @ @@ . . 
60 . @ 
70 
80 
Here white, playing the weighted-squares strategy, chose to play 66. This is probably 
a mistake, as 13 would extend white's dominance of the top edge, and allow white to 
play again (since black would have no legal moves). Unfortunately, white rejects this 
move, primarily because square 12 is weighted as -20. Thus, there is a disincentive 
to taking this square. But 12 is weighted -20 because it is a bad idea to take such a 
square when the corner is empty - the opponent will then have a chance to capture 
the corner, regaining the 12 square as well. Thus, we want squares like 12 to have a 
negative score when the corner is empty, but not when it is already occupied. The 
modi f i ed - wei ghted - squa res evaluation function does just that. 

(defun modified-weighted-squares (player board) 
"Like WEIGHTED-SQUARES, but don't take off for moving 
near an occupied corner." 
(let ((w (weighted-squares player board))) 

(dolist (corner '(11 18 81 88)) 
(when (not (eql (bref board corner) empty)) 
(dolist (c (neighbors corner)) 
(when (not (eql (bref board c) empty)) 
(incf w (* (-5 (aref *weights* c)) 
(if (eql (bref board c) player) 
+1 -1))))))) 
w)) 

(let ((neighbor-table (make-array 100 linitial-element nil))) 
;; Initialize the neighbor table 
(dolist (square all-squares) 

(dolist (dir all-directions) 
(if (valid-p (+ square dir)) 
(push (+ square dir) 
(aref neighbor-table square))))) 

(defun neighbors (square) 
"Return a list of all squares adjacent to a square." 
(aref neighbor-table square))) 


<a id='page-622'></a>

18.7 The Tournament Version of Othello 
While the othel 1 o function serves as a perfectly good moderator for casual play, 
there are two points that need to be fixed for tournament-level play. First, tournament 
games are played under a strict time limit: a player who takes over 30 minutes total 
to make all the moves forfeits the game. Second, the standard notation for Othello 
games uses square names in the range al to h8, rather than in the 11 to 88 range that 
we have used so far. al is the upper left corner, a8 is the lower left corner, and h8 is 
the lower right corner. We can write routines to translate between this notation and 
the one we were using by creating a table of square names. 

(let ((square-names 

(cross-product #'symbol 
'(? a b c d e f g h ?) 
'(712345678 ?)))) 

(defun h8->88 (str) 
"Convert from alphanumeric to numeric square notation." 
(or (position (string str) square-names rtest #'string-equal) 

str)) 

(defun 88->h8 (num) 
"Convert from numeric to alphanumeric square notation." 
(if (valid-p num) 

(elt square-names num) 
num))) 

(defun cross-product (fn xlist ylist) 
"Return a list of all (fn . y) values." 
(mappend #*(lambda (y) 

(mapcar #'(lambda (x) (funcall fn . y)) 
xlist)) 
ylist)) 

Note that these routines return their input unchanged when it is not one of the 
expected values. This is to allow commands other than moving to a particular 
square. For example, we will add a feature that recognizes res i gn as a move. 

The h uma. player needs to be changed slightly to read moves in this format. While 
we're at it, we'll also print the list of possible moves: 

(defun human (player board) 
"A human player for the game of Othello" 
(format t "~&~c to move "a: " (name-of player) 

(mapcar #*88->h8 (legal-moves player board))) 
(h8->88 (read))) 


<a id='page-623'></a>
Top-Level Functions 

Othello-series Play a series of . games. 
random-Othello-series Play a series of games, starting from a random position. 
round-robin Play a tournament among strategies. 

Special Variables 

*clock* A copy of the game clock (tournament version only). 
*board* A copy of the game board (tournament version only). 
*move-number* Number of moves made (tournament version only). 
*ply-boards* A vector of boards; used as a resource to avoid consing. 

Data Structures 

node Holds a board and its evaluation. 

Main Functions 

alpha-beta2 Sorts moves by static evaluation. 
alpha-beta-searcher2 Strategy using a1 pha- beta2. 
alpha-beta3 Uses the killer heuristic. 
alpha-beta-searcher3 Strategy using a1 pha- beta3. 
lago-eval Evaluation function based on Rosenbloom's program. 
lago Strategy using lago-eval. 

Auxiliary Functions 

h8->88 Convert from alphanumeric to numeric square notation. 
88->h8 Convert from numeric to alphanumeric square notation. 
time-string Convert internal time units to a mm.ss string. 
switch-strategies Play one strategy for a while, then another. 
mobil ity A strategy that counts the number of legal moves. 
legal-nodes A list of legal moves sorted by their evaluation. 
negate-node Set the value of a node to its negative. 
put-first Put the killer move first, if it is legal. 

Previously Defined Fimctions 

cross-product Apply fn to all pairs of arguments, (pg. 47) 
symbol Build a symbol by concatenating components. 

Figure 18.5: Glossary for the Tournament Version of Othello 

The othel 10 function needn't worry about notation, but it does need to monitor the 
time. We make up a new data structure, the clock, which is an array of integers 
saying how much time (in internal units) each player has left. For example, (aref 
cl ock bl ack) is the amount of time black has left to make all his moves. In Pascal, 
we would declare the clock array as arrayCbl ack. .white], but in Common Lisp all 
arrays are zero-based, so we need an array of three elements to allow the subscript 
black, which is 2. 

The clock is passed to get - move and print - boa rd but is otherwise unused. I could 
have complicated the main game loop by adding tests for forfeits because of expired 
time and, as we shall see later, resignation by either player. However, I felt that would 
add a great deal of complexity for rarely used options. Instead, I wrap the whole game 
loop, along with the computation of the final score, in a catch special form. Then, if 


<a id='page-624'></a>

get-move encounters a forfeit or resignation, it can throw an appropriate final score: 
64 or -64, depending on which player forfeits. 

(defvar *move-number* 1 "The number of the move to be played") 

(defun Othello (bl-strategy wh-strategy 

&optional (print t) (minutes 30)) 
"Play a game of Othello. Return the score, where a positive 
difference means black, the first player, wins." 
(let ((board (initial-board)) 

(clock (make-array (+ 1 (max black white)) 
:initial-element 
(* minutes 60 

internal-time-units-per-second)))) 
(catch 'game-over 

(loop for *move-number* from 1 
for player = black then (next-to-play board player print) 
for strategy = (if (eql player black) 

bl-strategy 

wh-strategy) 
until (null player) 
do (get-move strategy player board print clock)) 

(when print 
(format t "~&The game is over. Final result:") 
(print-board board clock)) 

(count-difference black board)))) 

Strategies now have to comply with the time-limit rule, so they may want to look at 
the time remaining. Rather than passing the clock in as an argument to the strategy,I 
decided to store the clock in the special variable *cl ock*. The new version of othel 10 
also keeps track of the *move-number*. This also could have been passed to the 
strategy functions as a parameter. But adding these extra arguments would require 
changes to all the strategies we have developed so far. By storing the information in 
special variables, strategies that want to can look at the clock or the move number, 
but other strategies don't have to know about them. 

We still have the security problem-we don't want a strategy to be able to set the 
opponent's remaining time to zero and thereby win the game. Thus, we use *cl ock* 
only as a copy of the "real" game clock. The function repl ace copies the real clock 
into *cl ock*, and also copies the real board into *board*. 

(defvar *clock* (make-array 3) "A copy of the game clock") 
(defvar *board* (initial-board) "A copy of the game board") 


<a id='page-625'></a>

(defun get-move (strategy player board print clock) 
"Call the player's strategy function to get a move. 
Keep calling until a legal move is made." 

Note we don't pass the strategy function the REAL board. 
;; If we did, it could cheat by changing the pieces on the board, 
(when print (print-board board clock)) 
(replace *clock* clock) 
(let* ((to (get-internal-real-time)) 

(move (funcall strategy player (replace *board* board))) 

(tl (get-internal-real-time))) 
(decf (elt clock player) (- tl tO)) 
(cond 

((< (elt clock player) 0) 
(format t ""&^c has no time left and forfeits." 
(name-of player)) 
(THROW 'game-over (if (eql player black) -64 64))) 
((eq move 'resign) 
(THROW 'game-over (if (eql player black) -64 64))) 
((and (valid-p move) (legal-p move player board)) 
(when print 
(format t "^&'O moves to ~a. " 
(name-of player) (88->h8 move))) 
(make-move move player board)) 
(t (warn "Illegal move: ~a" (88->h8 move)) 
(get-move strategy player board print clock))))) 

Finally, the function print - boa rd needs to print the time remaining for each player; 
this requires an auxiliary function to get the number of minutes and seconds from an 
internal-format time interval. Note that we make the arguments optional, so that in 
debugging one can say just (print- board) to see the current situation. Also note the 
esoteric format option: " ~2 / Od" prints a decimal number using at least two places, 
padding on the left with zeros. 

(defun print-board (&optional (board *board*) clock) 
"Print a board, along with some statistics." 
First print the header and the current score 

(format t "~2& a b c d e f g h [~c=~2a ~c=~2a ("d)]" 
(name-of black) (count black board) 
(name-of white) (count white board) 
(count-difference black board)) 

Print the board itself 

(loop for row from 1 to 8 do 
(format t "~& ~d " row) 
(loop for col from 1 to 8 

for piece = (bref board (+ col (* 10 row))) 

do (format t "~c " (name-of piece)))) 


<a id='page-626'></a>

;: Finally print the time remaining for each player 
(when clock 

(format t " ["'c='"a ~c=~a]~2&" 
(name-of black) (time-string (elt clock black)) 
(name-of white) (time-string (elt clock white))))) 

(defun time-string (time) 
"Return a string representing this internal time in minisecs. 
(multiple-value-bind (min sec) 

(floor (round time internal-time-units-per-second) 60) 
(format nil ""Zdrz/Od" min sec))) 

18.8 Playing a Series of Games 
A single game is not enough to establish that one strategy is better than another. The 
following function allows two strategies to compete in a series of games: 

(defun Othello -series (strategyl strategy2 n-pairs) 
"Play a series of 2*n-pairs games, swapping sides." 
(let ((scores (loop repeat n-pairs 

collect (Othello strategyl strategy2 nil) 

collect (- (Othello strategy2 strategyl nil))))) 
Return the number of wins, (1/2 for a tie), 
the total of thepoint differences, and the 
scores themselves, all from strategyl's point of view, 

(values (+ (count-if #'plusp scores) 

(/ (count-if #*zerop scores) 2)) 
(apply #'+ scores) 
scores))) 

Let's see what happens when we use it to pit the two weighted-squares functions 
against each other in a series of ten games: 

> (othello-series 
(alpha-beta-searcher 2 #*modified-weighted-squares) 
(alpha-beta-searcher 2 #'weighted-squares) 5) 

0 
60 
(-28 40 -28 40 -28 40 -28 40 -28 40) 

Something is suspicious here - the same scores are being repeated. A little thought 
reveals why: neither strategy has a random component, so the exact same game 
was played five times with one strategy going first, and another game was played 


<a id='page-627'></a>

five times when the other strategy goes first! A more accurate appraisal of the two 

strategies' relative worth would be gained by starting each game from some random 

position and playing from there. 

Think for a minute how you would design to run a series of games starting from a 
random position. One possibility would be to change the function othel 1 o to accept 
an optional argument indicating the initial state of the board. Then othel 1 o- seri es 
could be changed to somehow generate a random board and pass it to othel 1 o. While 
this approach is feasible, it means changing two existing working functions, as well 
as writing another function, generate - random-board. But we couldn't generate just 
any random board: it would have to be a legal board, so it would have to call othel 1 o 
and somehow get it to stop before the game was over. 

An alternative is to leave both Othello and othello-series alone and build 
another function on top of it, one that works by passing in two new strategies: 
strategies that make a random move for the first few moves and then revert to 
the normal specified behavior. This is a better solution because it uses existing 
functions rather than modifying them, and because it requires no new functions 
besides switch-strategies, which could prove useful for other purposes, and 
random-othel lo-seri es, which does nothing more than call othel lo-seri es with 
the proper arguments. 

(defun random-Othello-series (strategyl strategy2 

n-pairs &optional (n-random 10)) 
"Play a series of 2*n games, starting from a random position." 
(othello-series 

(switch-strategies #'random-strategy n-random strategyl) 
(switch-strategies #*random-strategy n-random strategy2) 
n-pairs)) 

(defun switch-strategies (strategyl m strategy2) 
"Make a new strategy that plays strategyl for m moves, 
then plays according to strategy2." 
#'(lambda (player board) 

(funcall (if (<= *move-number* m) strategyl strategy2) 
player board))) 

There is a problem with this kind of series: it may be that one of the strategies just 

happens to get better random positions. A fairer test would be to play two games 

from each random position, one with the each strategy playing first. One way to 

do that is to alter othel 1 o-seri es so that it saves the random state before playing 

the first game of a pair, and then restores the saved random state before playing the 

second game. That way the same random position will be duplicated. 


<a id='page-628'></a>

(defun Othello-series (strategyl strategy2 n-pairs) 
"Play a series of 2*n-pairs games, swapping sides." 
(let ((scores 

(loop repeat n-pairs 
for random-state = (make-random-state) 
collect (Othello strategyl strategy2 nil) 
do (setf *random-state* random-state) 
collect (- (Othello strategy2 strategyl nil))))) 

Return the number of wins (1/2 for a tie), 
the total of the point differences, and the 
scores themselves, all from strategyl's point of view, 

(values (+ (count-if #*plusp scores) 

(/ (count-if #*zerop scores) 2)) 
(apply #'+ scores) 
scores))) 

Now we are in a position to do a more meaningful test. In the following, the weighted-
squares strategy wins 4 out of 10 games against the modified strategy, losing by a 
total of 76 pieces, with the actual scores indicated. 

> (random-Othello-series 
(alpha-beta-searcher 2 #'weighted-squares) 
(alpha-beta-searcher 2#'modified-weighted-squares) 
5) 

4 
-76 
(-8 -40 22 -30 10 -10 12 -18 4 -18) 

The random- othel lo-series function is useful for comparing two strategies. When 
there are more than two strategies to be compared at the same time, the following 
function can be useful: 

(defun round-robin (strategies n-pairs &optional 

(n-random 10) (names strategies)) 
"Play a tournament among the strategies. 
N-PAIRS = games each strategy plays as each color against 
each opponent. So with . strategies, a total of 
N*(N-1)*N-PAIRS games are played." 
(let* ((N (length strategies)) 

(totals (make-array . .-initial-element 0)) 
(scores (make-array (list . .) 
:i ni ti al-element 0))) 
Play the games 
(dotimes (IN) 
(loop for j from (+ i 1) to (- . 1) do 
(let* ((wins (random-Othello-series 


<a id='page-629'></a>
(elt strategies i) 
(elt strategies j) 
n-pairs n-random)) 

(losses (- (* 2 n-pairs) wins))) 
(incf (aref scores i j) wins) 
(incf (aref scores j i) losses) 
(incf (aref totals i) wins) 
(incf (aref totals j) losses)))) 

Print the results 

(dotimes (i N) 
(format t "~ra~20T ~4f: " (elt names i) (elt totals i)) 
(dotimes (j N) 

(format t "~4f " (if (= i j) 
(aref scores i j))))))) 

Here is a comparison of five strategies that search only 1 ply: 

(defun mobility (player board) 
"The number of moves a player has." 
(length (legal-moves player board))) 

> (round-robin 

(list (maximizer #'count-difference) 
(maximizer #'mobility) 
(maximizer #*weighted-squares) 
(maximizer #'modified-weighted-squares) 
#'random-strategy) 

5 10 
'(count-difference mobility weighted modified-weighted random)) 

COUNT-DIFFERENCE 12.5: --3.0 
2.5 0.0 7.0 
MOBILITY 20.5: 7.0 --1.5 
5.0 7.0 
WEIGHTED 28.0: 7.5 8.5 --3.0 
9.0 
MODIFIED-WEIGHTED 31.5: 10.0 5.0 7.0 --9.5 
RANDOM 7.5: 3.0 3.0 1.0 0.5 --


The parameter .-pai rs is 5, meaning that each strategy plays five games as black 
and five as white against each of the other four strategies, for a total of 40 games 
for each strategy and 100 games overall. The first line of output says that the count-
difference strategy won 12.5 of its 40 games, including 3 against the mobility strategy, 

2.5 against the weighted strategy, none against the modified weighted, and 7 against 
the random strategy. The fact that the random strategy manages to win 7.5 out of 40 
games indicates that the other strategies are not amazingly strong. Now we see what 
happens when the search depth is increased to 4 ply (this will take a while to run): 

<a id='page-630'></a>

> (round-robi. 

(list (alpha-beta-searcher 4 #*count-difference) 
(alpha-beta-searcher 4 #'weighted-squares) 
(alpha-beta-searcher 4 #'modified-weighted-squares) 
#'random-strategy) 

5 10 

'(count-difference weighted modified-weighted random)) 

COUNT-DIFFERENCE 12.0: --
2.0 0.0 10.0 
WEIGHTED 23.5: 8.0 .. . 5.5 10.0 
MODIFIED-WEIGHTED 24.5: 10.0 4.5 .. . 10.0 
RANDOM 0.0: 0.0 0.0 0.0 

Here the random strategy does not win any games - an indication that the other 
strategies are doing something right. Notice that the modified weighted-squares 
has only a slight advantage over the weighted-squares, and in fact it lost their head-
to-head series, four games to five, with one draw. So it is not clear which strategy 
is better. 

The output does not break down wins by black or white, nor does it report the 
numerical scores. I felt that that would clutter up the output too much, but you're 
welcome to add this information. It turns out that white wins 23 (and draws 1) of 
the 40 games played between 4-ply searching strategies. Usually, Othello is a fairly 
balanced game, because black has the advantage of moving first but white usually 
gets to play last. It is clear that these strategies do not play well in the operung game, 
but for the last four ply they play perfectly. This may explain white's slight edge, or 
it may be a statistical aberration. 

18.9 More Efficient Searching 
The alpha-beta cutoffs work when we have established a good move and another 
move proves to be not as good. Thus, we will be able to make cutoffs earlier if we 
ensure that good moves are considered first. Our current algorithm loops through 
the list of 1 egal -moves, but 1 egal -moves makes no attempt to order the moves in any 
way. We will call this the random-ordering strategy (even though the ordering is not 
random at all-square 11 is always considered first, then 12, etc.). 

One way to try to generate good moves first is to search highly weighted squares 
first. Since 1 egal -moves considers squares in the order defined by all -squares, all 
we have to do is redefine the list al 1 -squares^: 

^Remember, when a constant is redefined, it may be necessary to recompile any functions 
that use the constant. 


<a id='page-631'></a>

(defconstant all-squares 
(sort (loop for i from 11 to 88 
when (<= 1 (mod i 10) 8) collect i) 
#*> :key #'(lambda (sq) (elt *weights* sq)))) 

Now the corner squares will automatically be considered first, followed by the other 
highly weighted squares. We call this the static-ordering strategy, because the ordering 
is not random, but it does not change depending on the situation. 

A more informed way to try to generate good moves first is to sort the moves 
according to the evaluation function. This means making more evaluations. Previously, 
only the boards at the leaves of the search tree were evaluated. Now we need 
to evaluate every board. In order to avoid evaluating a board more than once, we 
make up a structure called a node, which holds a board, the square that was taken to 
result in that board, and the evaluation value of that board. The search is the same 
except that nodes are passed around instead of boards, and the nodes are sorted by 
their value. 

(defstruct (node) square board value) 

(defun alpha-beta-searcher2 (depth eval-fn) 
"Return a strategy that does A-B search with sorted moves." 
#'(lambda (player board) 

(multiple-value-bind (value node) 
(alpha-beta2 
player (make-node :board board 
.-value (funcall eval-fn player board)) 

losing-value winning-value depth eval-fn) 
(declare (ignore value)) 
(node-square node)))) 

(defun alpha-beta2 (player node achievable cutoff ply eval-fn) 
"A-B search, sorting moves by eval-fn" 
;; Returns two values: achievable-value and move-to-make 
(if (= ply 0) 

(values (node-value node) node) 
(let* ((board (node-board node)) 
(nodes (legal-nodes player board eval-fn))) 
(if (null nodes) 
(if (any-legal-move? (opponent player) board) 

(values (- (alpha-beta2 (opponent player) 
(negate-value node) 
(- cutoff) (- achievable) 
(- ply 1) eval-fn)) 

nil) 
(values (final-value player board) nil)) 
(let ((best-node (first nodes))) 
(loop for move in nodes 


<a id='page-632'></a>

for val = (- (alpha-betaZ 
(opponent player) 
(negate-value move) 
(- cutoff) (- achievable) 
(- ply 1) eval-fn)) 

do (when (> val achievable) 
(setf achievable val) 
(setf best-node move)) 

until (>= achievable cutoff)) 
(values achievable best-node)))))) 

(defun negate-value (node) 
"Set the value of a node to its negative." 
(setf (node-value node) (- (node-value node))) 
node) 

(defun legal-nodes (player board eval-fn) 
"Return a list of legal moves, each one packed into a node." 
(let ((moves (legal-moves player board))) 

(sort (map-into 
moves 
#*(lambda (move) 

(let ((new-board (make-move move player 
(copy-board board)))) 

(make-node 
: squa re move .-board new-board 
:value (funcall eval-fn player new-board)))) 

moves) 
#'> :key #'node-value))) 

(Note the use of the function map -i nto. This is part of ANSI Common Lisp, but if it 
is not a part of your implementation, a definition is provided on [page 857](chapter24.md#page-857).) 

The following table compares the performance of the random-ordering strategy, 
the sorted-ordering strategy and the static-ordering strategy in the course of a single 
game. All strategies search 6 ply deep. The table measures the number of boards 
investigated, the number of those boards that were evaluated (in all cases the evaluation 
function was modi f i ed - wei ghted - squa res) and the time in seconds to compute 
a move. 


<a id='page-633'></a>
random order sorted order static order 
boards evals sees boards evals sees boards evals sees 
13912 10269 69 5556 5557 22 2365 1599 19 
9015 6751 56 6571 6572 25 3081 2188 18 
9820 7191 46 11556 11557 45 5797 3990 31 
4195 3213 20 5302 5303 17 2708 2019 15 
10890 7336 60 10709 10710 38 3743 2401 23 
13325 9679 63 6431 6432 24 4222 2802 24 
13163 9968 58 9014 9015 32 6657 4922 31 
16642 12588 70 9742 9743 33 10421 7488 51 
18016 13366 80 11002 11003 37 9508 7136 41 
23295 17908 104 15290 15291 48 26435 20282 111 
34120 25895 143 22994 22995 75 20775 16280 78 
56117 43230 224 46883 46884 150 48415 36229 203 
53573 41266 209 62252 62253 191 37803 28902 148 
43943 33184 175 31039 31040 97 33180 24753 133 
51124 39806 193 45709 45710 135 19297 15064 69 
24743 18777 105 20003 20004 65 15627 11737 66 
1.0 1.0 1.0 .81 1.07 .62 .63 .63 .63 

The last two lines of the table give the averages and the averages normalized to the 
random-ordering strategy's performance. The sorted-ordering strategy takes only 
62% of the time of the random-ordering strategy, and the static-ordering takes 63 %. 
These times are not to be trusted too much, because a large-scale garbage collection 
was taking place during the latter part of the game, and it may have thrown off the 
times. The board and evaluation count may be better indicators, and they both show 
the static-ordering strategy doing the best. 

We have to be careful how we evaluate these results. Earlier I said that alpha-beta 
search makes more cutoffs when it is presented first with better moves. The actual 
truth is that it makes more cutoffs when presented first with moves that the evaluation 
function thinks are better. In this case the evaluation function and the static-ordering 
strategy are in strong agreement on what are the best moves, so it is not surprising 
that static ordering does so well. As we develop evaluation functions that vary from 
the weighted-squares approach, we will have to run experiments again to see if the 
static-ordering is still the best. 

18.10 It Pays to Precycle 
The progressive city of Berkeley, California, has a strong recycling program to reclaim 
glass, paper, and aluminum that would otherwise be discarded as garbage. In 1989, 


<a id='page-634'></a>

Berkeley instituted a novel program of precycling: consumers are encouraged to avoid 
buying products that come in environmentally wasteful packages. 

Your Lisp system also has a recycling program: the Lisp garbage collector automatically 
recycles any unused storage. However, there is a cost to this program, and 
you the consumer can get better performance by precycling your data. Don't buy 
wasteful data structures when simpler ones can be used or reused. You, the Lisp 
programmer, may not be able to save the rain forests or the ozone layer, but you can 
save valuable processor time. 

We saw before that the search routines look at tens of thousands of boards per 
move. Currently, each board position is created anew by copy-board and discarded 
soon thereafter. We could avoid generating all this garbage by reusing the same board 
at each ply. We'd still need to keep the board from the previous ply for use when 
the search backs up. Thus, a vector of boards is needed. In the following we assume 
that we will never search deeper than 40 ply. This is a safe assumption, as even the 
fastest Othello programs can only search about 15 ply before running out of time. 

(defvar *ply-boards* 
(apply #*vector (loop repeat 40 collect (initial-board)))) 

Now that we have sharply limited the number of boards needed, we may want to 
reevaluate the implementation of boards. Instead of having the board as a vector of 
pieces (to save space), we may want to implement boards as vectors of bytes or full 
words. In some implementations, accessing elements of such vectors is faster. (In 
other implementations, there is no difference.) 

An implementation using the vector of boards will be done in the next section. 
Note that there is another alternative: use only one board, and update it by making 
and retracting moves. This is a good alternative in a game like chess, where a move 
only alters two squares. In Othello, many squares can be altered by a move, so 
copying the whole board over and making the move is not so bad. 

It should be mentioned that it is worth looking into the problem of copying a 
position from one board to another. The function repl ace copies one sequence (or 
part of it) into another, but it is a generic function that may be slow. In particular, if 
each element of a board is only 2 bits, then it may be much faster to use displaced 
arrays to copy 32 bits at a time. The advisability of this approach depends on the 
implementation, and so it is not explored further here. 

18.11 Killer Moves 
In section 18.9, we considered the possibility of searching moves in a different 
order, in an attempt to search the better moves first, thereby getting more alpha-beta 
pruning. In this section, we consider the killer heunstic, which states that a move that 


<a id='page-635'></a>
has proven to be a good one in one line of play is also likely to be a good one in another 
line of play. To use chess as perhaps a more familiar example, suppose I consider 
one move, and it leads to the opponent replying by capturing my queen. This is a 
killer move, one that I would like to avoid. Therefore, when I consider other possible 
moves, I want to immediately consider the possibility of the opponent making that 
queen-capturing move. 

The function alpha-beta3 adds the parameter ki 11 er, which is the best move 
found so far at the current level. After we determine the legal -moves, we use 
put-first to put the killer move first, if it is in fact a legal move. When it comes 
time to search the next level, we keep track of the best move in kiHer2. This 
requires keeping track of the value of the best move in ki 11 er 2- va1. Everything else 
is unchanged, except that we get a new board by recycling the *pl y-boards* vector 
rather than by allocating fresh ones. 

(defun alpha-betaS (player board achievable cutoff ply eval-fn 

killer) 
"A-. search, putting killer move first." 
(if (= ply 0) 

(funcall eval-fn player board) 
(let ((moves (put-first killer (legal-moves player board)))) 
(if (null moves) 
(if (any-legal-move? (opponent player) board) 

(- (alpha-betaS (opponent player) board 
(- cutoff) (- achievable) 
(- ply 1) eval-fn nil)) 

(final-value player board)) 

(let ((best-move (first moves)) 
(new-board (aref *ply-boards* ply)) 
(killer2 nil) 
(killer2-val winning-value)) 

(loop for move in moves 
do (multiple-value-bind (val reply) 

(alpha-betaS 
(opponent player) 
(make-move move player 

(replace new-board board)) 
(- cutoff) (- achievable) 
(- ply 1) eval-fn killer2) 

(setf val (- val)) 

(when (> val achievable) 
(setf achievable val) 
(setf best-move move)) 

(when (and reply (< val killer2-val)) 
(setf killer2 reply) 
(setf killer2-val val))) 

until (>= achievable cutoff)) 


<a id='page-636'></a>

(values achievable best-move)))))) 

(defun alpha-beta-searcher3 (depth eval-fn) 
"Return a strategy that does A-B search with killer moves." 
#'(lambda (player board) 

(multiple-value-bind (value move) 
(alpha-betaS player board losing-value winning-value 

depth eval-fn nil) 
(declare (ignore value)) 
move))) 

(defun put-first (killer moves) 
"Move the killer move to the front of moves, 
if the killer move is in fact a legal move." 
(if (member killer moves) 

(cons killer (delete killer moves)) 
moves)) 

Another experiment on a single game reveals that adding the killer heuristic to static-
ordering search (again at 6-ply) cuts the number of boards and evaluations, and the 
total time, all by about 20%. To summarize, alpha-beta search at 6 ply with random 
ordering takes 105 seconds per move (in our experiment), adding static-ordering cuts 
it to 66 seconds, and adding killer moves to that cuts it again to 52 seconds. This 
doesn't include the savings that alpha-beta cutoffs give over full minimax search. At 
6 ply with a branching factor of 7, full minimax would take about nine times longer 
than static ordering with killers. The savings increase with increased depth. At 
7 ply and a branching factor of 10, a small experiment shows that static-ordering 
with killers looks at only 28,000 boards in about 150 seconds. Full minimax would 
evaluate 10 million boards and take 350 times longer. The times for full minimax are 
estimates based on the number of boards per second, not on an actual experiment. 

The algorithm in this section just keeps track of one killer move. It is of course 
possible to keep track of more than one. The Othello program Bill (Lee and Mahajan 
1990b) merges the idea of killer moves with legal move generation: it keeps a list of 
possible moves at each level, sorted by their value. The legal move generator then 
goes down this list in sorted order. 

It should be stressed once again that all this work on alpha-beta cutoffs, ordering, 
and killer moves has not made any change at all in the moves that are selected. We 
still end up choosing the same move that would be made by a full minimax search to 
the given depth, we are just doing it faster, without looking at possibilities that we 
can prove are not as good. 


<a id='page-637'></a>
18.12 Championship Programs: lago and Bill 
As mentioned in the introduction, the unpredictability of Othello makes it a difficult 
game for humans to master, and thus programs that search deeply can do comparatively 
well. In fact, in 1981 the reigning champion, Jonathan Cerf, proclaimed "In 
my opinion the top programs ... are now equal (if not superior) to the best human 
players." In discussing Rosenbloom's lago program (1982), Cerf went on to say "I 
understand Paul Rosenbloom is interested in arranging a match against me. Unfortunately 
my schedule is very full, and I'm going to see that it remains that way for the 
foreseeable future." 

In 1989, another program. Bill (Lee and Mahajan 1990) beat the highest rated 
American Othello player, Brian Rose, by a score of 56-8. Bill's evaluation function is 
fast enough to search 6-8 ply under tournament conditions, yet it is so accurate that 
it beats its creator, Kai-Fu Lee, searching only 1 ply. (However, Lee is only a novice 
Othello player; his real interest is in speech recognition; see Waibel and Lee 1991.) 
There are other programs that also play at a high level, but they have not been written 
up in the AI literature as lago and Bill have. 

In this section we present an evaluation function based on lago's, although it also 
contains elements of Bill, and of an evaluation function written by Eric Wef aid in 1989. 
The evaluation function makes use of two main features: mobility and edge stability. 

Mobility 

Both lago and Bill make heavy use of the concept of mobility. Mobility is a measure of 
the ability to make moves; basically, the more moves one can make, the better. This 
is not quite true, because there is no advantage in being able to make bad moves, 
but it is a useful heuristic. We define current mobility as the number of legal moves 
available to a player, and potential mobility as the number of blank squares that are 
adjacent to opponent's pieces. These include the legal moves. A better measure of 
mobility would try to count only good moves. The following function computes both 
current and potential mobility for a player: 

(defun mobility (player board) 
"Current mobility is the number of legal moves. 
Potential mobility is the number of blank squares 
adjacent to an opponent that are not legal moves. 
Returns current and potential mobility for player." 
(let ((opp (opponent player)) 

(current 0) ; player's current mobility 
(potential 0)) ; player's potential mobility 
(dolist (square all-squares) 
(when (eql (bref board square) empty) 
(cond ((legal-p square player board) 


<a id='page-638'></a>

(incf current)) 
((some #.(lambda (sq) (eql (bref board sq) opp)) 
(neighbors square)) 
(incf potential))))) 
(values current (+ current potential)))) 

Edge Stability 

Success at Othello often hinges around edge play, and both lago and Bill evaluate 
the edges carefully. Edge analysis is made easier by the fact that the edges are fairly 
independent of the interior of the board: once a piece is placed on the edge, no 
interior moves can flip it. This independence allows a simplifying assumption: to 
evaluate a position's edge strength, evaluate each of the four edges independently, 
without consideration of the interior of the board. The evaluation can be made more 
accurate by considering the X-squares to be part of the edge. 

Even evaluating a single edge is a time-consuming task, so Bill and lago compile 
away the evaluation by building a table of all possible edge positions. An "edge" 
according to Bill is ten squares: the eight actual edge squares and the two X-squares. 
Since each square can be black, white, or empty, there are 3^^ or 59,049 possible edge 
positions - a large but manageable number. 

The value of each edge position is determined by a process of succesive approximation. 
Just as in a minimax search, we will need a static edge evaluation function 
to determine the value of a edge position without search. This static edge evaluation 
function is appHed to every possible edge position, and the results are stored in a 
59,049 element vector. The static evaluation is just a weighted sum of the occupied 
squares, with different weights given depending on if the piece is stable or unstable. 

Each edge position's evaluation can be improved by a process of search. lago 
uses a single ply search: given a position, consider all moves that could be made 
(including no move at all). Some moves will be clearly legal, because they flip pieces 
on the edge, but other moves will only be legal if there are pieces in the interior of 
the board to flip. Since we are only considering the edge, we don't know for sure if 
these moves are legal. They will be assigned probabilities of legality. The updated 
evaluation of a position is determined by the values and probabilities of each move. 
This is done by sorting the moves by value and then summing the product of the 
value times the probability that the move can be made. This process of iterative 
approximation is repeated five times for each position. At that point, Rosenbloom 
reports, the values have nearly converged. 

In effect, this extends the depth of the normal alpha-beta search by including an 
edge-only search in the evaluation function. Since each edge position with . pieces 
is evaluated as a function of the positions with . -h 1 pieces, the search is complete-it 
is an implicit 10-ply search. 


<a id='page-639'></a>
Calculating edge stability is a bit more complicated than the other features. The 
first step is to define a variable, *eclge - table*, which will hold the evaluation of each 
edge position, and a constant, edge-and-x-1 i sts, which is a list of the squares on 
each of the four edges. Each edge has ten squares because the X-squares are included. 

(defvar *edge-table* (make-array (expt 3 10)) 
"Array of values to player-to-move for edge positions.") 

(defconstant edge-and-x-1ists 

'((22 11 12 13 14 15 16 17 18 27) 
(72 81 82 83 84 85 86 87 88 77) 
(22 11 21 31 41 51 61 71 81 72) 
(27 18 28 38 48 58 68 78 88 77)) 

"The four edges (with their X-squares).") 

Now for each edge we can compute an index into the edge table by building a 10-digit 
base-3 number, where each digit is 1 if the corresponding edge square is occupied by 
the player, 2 if by the opponent, and 0 if empty. The function edge- i ndex computes 
this, and edge - stabi 1 i ty sums the values of the four edge indexes. 

(defun edge-index (player board squares) 
"The index counts 1 for player; 2 for opponent, 
on each square---summed as a base 3 number." 
(let ((index 0)) 

(dolist (sq squares) 
(setq index (+ (* index 3) 

(cond ((eql (bref board sq) empty) 0) 
((eql (bref board sq) player) 1) 
(t 2))))) 

index)) 

(defun edge-stability (player board) 
"Total edge evaluation for player to move on board." 
(loop for edge-list in edge-and-x-1ists 

sum (aref *edge-table* 
(edge-index player board edge-list)))) 

The function edge - stabi 1 i ty is all we will need in lago's evaluation function, but we 

still need to generate the edge table. Since this needs to be done only once, we don't 

have to worry about efficiency. In particular, rather than invent a new data structure 

to represent edges, we will continue to use complete boards, even though they will 

be mostly empty. The computations for the edge table will be made on the top edge, 

from the point of view of black, with black to play. But the same table can be used for 

white, or for one of the other edges, because of the way the edge index is computed. 

Each position in the table is first initialized to a static value computed by a kind 
of weighted-squares metric, but with different weights depending on if a piece is in 


<a id='page-640'></a>

danger of being captured. After that, each position is updated by considering the 
possible moves that can be made from the position, and the values of each of these 
moves. 

(defconstant top-edge (first edge-and-x-lists)) 

(defun init-edge-table () 
"Initialize *edge-table*. starting from the empty board." 
Initialize the static values 
(loop for n-pieces from 0 to 10 do 
(map-edge-n-pieces 
#*(lambda (board index) 
(setf (aref *edge-table* index) 
(static-edge-stability black board))) 
black (initial-board) n-pieces top-edge 0)) 
Now iterate five times trying to improve: 

(dotimes (i 5) 
;; Do the indexes with most pieces first 
(loop for n-pieces from 9 downto 1 do 

(map-edge-n-pieces 
#'(lambda (board index) 
(setf (aref *edge-table* index) 
(possible-edge-moves-value 
black board index))) 
black (initial-board) n-pieces top-edge 0)))) 

The function map-edge-n-pieces iterates through all edge positions with a total of 
. pieces (of either color), applying a function to each such position. It also keeps a 
running count of the edge index as it goes. The function should accept two arguments: 
the board and the index. Note that a single board can be used for all the positions 
because squares are reset after they are used. The function has three cases: if the 
number of squares remaining is less than n, then it will be impossible to place . pieces 
on those squares, so we give up. If there are no more squares then . must also be 
zero, so this is a valid position, and the function f . is called. Otherwise we first try 
leaving the current square blank, then try filling it with player's piece, and then with 
the opponent's piece, in each case calling map-edge-.-pi eces recursively. 

(defun map-edge-n-pieces (fn player board . squares index) 
"Call fn on all edges with . pieces." 
;; Index counts 1 for player; 2 for opponent 
(cond 

((< (length squares) n) nil) 
((null squares) (funcall fn board index)) 
(t (let ((index3 (* 3 index)) 

(sq (first squares))) 
(map-edge-n-pieces fn player board . (rest squares) indexS) 


<a id='page-641'></a>
(when (and (> . 0) (eql (bref board sq) empty)) 
(setf (bref board sq) player) 
(map-edge-n-pieces fn player board (- . 1) (rest squares) 

(+ 1 index3)) 
(setf (bref board sq) (opponent player)) 
(map-edge-n-pieces fn player board (- . 1) (rest squares) 

(+ 2 indexS)) 
(setf (bref board sq) empty)))))) 

The function possible-edge-moves-value searches through all possible moves to 
determine an edge value that is more accurate than a static evaluation. It loops 
through every empty square on the edge, calling possible-edge-move to return a 
(probability value) pair. Since it is also possible for a player not to make any move at 
all on an edge, the pair (1.0 current-value) is also included. 

(defun possible-edge-moves-value (player board index) 
"Consider all possible edge moves. 
Combine their values into a single number." 
(combine-edge-moves 

(cons 
(list 1.0 (aref *edge-table* index)) ;; no move 
(loop for sq in top-edge ;; possible moves 

when (eql (bref board sq) empty) 
collect (possible-edge-move player board sq))) 
player)) 

The value of each position is determined by making the move on the board, then 
looking up in the table the value of the resulting position for the opponent, and 
negating it (since we are interested in the value to us, not to our opponent). 

(defun possible-edge-move (player board sq) 
"Return a (prob val) pair for a possible edge move." 
(let ((new-board (replace (aref *ply-boards* player) board))) 

(make-move sq player new-board) 
(list (edge-move-probability player board sq) 
(- (aref *edge-table* 
(edge-index (opponent player) 
new-board top-edge)))))) 

The possible moves are combined with combi ne-edge-moves, which sorts the moves 
best-first. (Since ini t-edge-tabl e started from black's perspective, black tries to 
maximize and white tries to minimize scores.) We then go down the moves, increasing 
the total value by the value of each move times the probability of the move, and 
decreasing the remaining probability by the probability of the move. Since there will 


<a id='page-642'></a>

always be a least one move (pass) with probability 1.0, this is guaranteed to converge. 
In the end we round off the total value, so that we can do the run-time calculations 
with fixnums. 

(defun combine-edge-moves (possibilities player) 
"Combine the best moves." 
(let ((prob 1.0) 

(val 0.0) 
(fn (if (eql player black) #'> #'<))) 

(loop for pair in (sort possibilities fn :key #'second) 
while (>= prob 0.0) 
do (incf val (* prob (first pair) (second pair))) 

(decf prob (* prob (first pair)))) 
(round val))) 

We still need to compute the probability that each possible edge move is legal. These 
probabiUties should reflect things such as the fact that it is easy to capture a corner 
if the opponent is in the adjacent X-square, and very difficult otherwise. First we 
define some functions to recognize corner and X-squares and relate them to their 
neighbors: 

(let ((corner/xsqs '((11 . 22) (18 . 27) (81. 72) (88 . 77)))) 
(defun corner-p (sq) (assoc sq corner/xsqs)) 
(defun x-square-p (sq) (rassoc sq corner/xsqs)) 
(defun x-square-for (corner) (cdr (assoc corner corner/xsqs))) 
(defun corner-for (xsq) (car (rassoc xsq corner/xsqs)))) 

Now we consider the probabilities. There are four cases. First, since we don't 
know anything about the interior of the board, we assume each player has a 50% 
chance of being able to play in an X-square. Second, if we can show that a move 
is legal (because it flips opponent pieces on the edge) then it has 100% probability. 
Third, for the corner squares, we assign a 90% chance if the opponent occupies the 
X-square, 10% if it is empty, and only .1 % if we occupy it. Otherwise, the probability 
is determined by the two neighboring squares: if a square is next to one or more 
opponents it is more likely we can move there; if it is next to our pieces it is less likely. 
If it is legal for the opponent to move into the square, then the chances are cut in half 
(although we may still be able to move there, since we move first). 

(defun edge-move-probability (player board square) 
"What's the probability that player can move to this square?" 
(cond 

((x-square-p square) .5) ;; X-squares 
((legal-p square player board) 1.0) immediate capture 
((corner-p square) :; move to corner depends on X-square 


<a id='page-643'></a>
(let ((x-sq (x-square-for square))) 

(cond 
((eql (bref board x-sq) empty) .1) 
((eql (bref board x-sq) player) 0.001) 
(t .9)))) 

(t (/ (aref 

'#2A((.l .4 .7) 
(.05 .3 *) 
(.01 * *)) 

(count-edge-neighbors player board square) 
(count-edge-neighbors (opponent player) board square)) 
(if (legal-p square (opponent player) board) 2 1))))) 

(defun count-edge-neighbors (player board square) 
"Count the neighbors of this square occupied by player." 
(count-if #'(lambda (inc) 

(eql (bref board (+ square inc)) player)) 
'(+1 -1))) 

Now we return to the problem of determining the static value of an edge position. 
This is computed by a weighted-squares metric, but the weights depend on the 
stability of each piece. A piece is called stable if it cannot be captured, unstable if 
it is in immediate danger of being captured, and semistable otherwise. A table of 
weights follows for each edge square and stability. Note that corner squares are 
always stable, and X-squares we will call semistable if the adjacent corner is taken, 
and unstable otherwise. 

(defparameter *static-edge-table* 

'#2A(;stab semi un 
( * 0 -2000) X 
( 700 ' *) corner 
(1200 200 -25) C 
(1000 200 75) A 
(1000 200 50) . 
(1000 200 50) . 
(1000 200 75) A 
(1200 200 -25) C 
( 700 ' *) corner 
( * 0 -2000) . X 

)) 


<a id='page-644'></a>

The static evaluation then just sums each piece's value according to this table: 

(defun static-edge-stability (player board) 
"Compute this edge's static stability" 
(loop for sq in top-edge 

for i from 0 

sum (cond 
((eql (bref board sq) empty) 0) 
((eql (bref board sq) player) 

(aref *static-edge-table* i 
(piece-stability board sq))) 
(t (- (aref *static-edge-table* i 
(piece-stability board sq))))))) 

The computation of stability is fairly complex. It centers around finding the two 
"pieces," pi and p2, which lay on either side of the piece in question and which are 
not of the same color as the piece. These "pieces" may be empty, or they may be off 
the board. A piece is unstable if one of the two is empty and the other is the opponent; 
it is semistable if there are opponents on both sides and at least one empty square to 
play on, or if it is surrounded by empty pieces. Finally, if either pi or p2 is nil then 
the piece is stable, since it must be connected by a solid wall of pieces to the corner. 

(let ((stable 0) (semi-stable 1) (unstable 2)) 

(defun piece-stability (board sq) 

(cond 
((corner-p sq) stable) 
((x-square-p sq) 

(if (eql (bref board (corner-for sq)) empty) 
unstable semi-stable)) 

(t (let* ((player (bref board sq)) 
(opp (opponent player)) 
(pi (find player board :test-not #*eql 

istart sq :end 19)) 

(p2 (find player board :test-not #'eql 
:start 11 :end sq 
:from-end t))) 

(cond 
unstable pieces can be captured immediately 
by playing in the empty square 

((or (and (eql pi empty) (eql p2 opp)) 
(and (eql p2 empty) (eql pi opp))) 

unstable) 
;; semi-stable pieces might be captured 
((and (eql pi opp) (eql p2 opp) 


<a id='page-645'></a>
(find empty board :start 11 :end 19)) 
semi-stable) 
((and (eql pi empty) (eql p2 empty)) 
semi-stable) 
Stable pieces can never be captured 
(t stable))))))) 

The edge table can now be built by a call to i ni t-edge-tabl e. After the table is built 
once, it is a good idea to save it so that we won't need to repeat the initialization. We 
could write simple routines to dump the table into a file and read it back in, but it is 
faster and easier to use existing tools that already do this job quite well: comp i 1 e-f i 1 e 
and 1 oad. All we have to do is create and compile a file containing the single line: 

(setf *edge-table* *#.*edge-table*) 

The #. read macro evaluates the following expression at read time. Thus, the 
compiler will see and compile the current edge table. It will be able to store this more 
compactly and 1 oad it back in more quickly than if we printed the contents of the 
vector in decimal (or any other base). 

Combining the Factors 

Now we have a measure of the three factors: current mobility, potential mobility, and 
edge stability. All that remains is to find a good way to combine them into a single 
evaluation metric. The combination function used by Rosenbloom (1982) is a linear 
combination of the three factors, but each factor's coefficient is dependent on the 
move number. Rosenbloom's features are normalized to the range [-1000,1000]; we 
normalize to the range [-1,1] by doing a division after multiplying by the coefficient. 
That allows us to use fixnuums for the coefficients. Since our three factors are 
not calculated in quite the same way as Rosenbloom's, it is not surprising that his 
coefficients are not the best for our program. The edge coefficient was doubled and 
the potential coefficient cut by a factor of five. 

(defun lago-eval (player board) 
"Combine edge-stability, current mobility and 
potential mobility to arrive at an evaluation." 

The three factors are multiplied by coefficients 
that vary by move number: 
(let ((c-edg(+ 312000 (* 6240 *move-number*))) 

(c-cur (if (< *move-number* 25) 
(+ 50000 (* 2000 *move-number*)) 
(+ 75000 (* 1000 *move-number*)))) 

(c-pot 20000)) 


<a id='page-646'></a>

(multiple-value-bind (p-cur p-pot) 
(mobility player board) 
(multiple-value-bind (o-cur o-pot) 

(mobility (opponent player) board) 
;; Combine the three factors into one sum: 
(+ (round (* c-edg (edge-stability player board)) 32000) 

(round (* c-cur (- p-cur o-cur)) (+ p-cur o-cur 2)) 
(round (* c-pot (- p-pot o-pot)) (+ p-pot o-pot 2))))))) 

Finally, we are ready to code the lago function. Given a search depth, lago returns a 
strategy that will do alpha-beta search to that depth using the lago-eval evaluation 
function. This version of lago was able to defeat the modified weighted-squares 
strategy in 8 of 10 games at 3 ply, and 9 of 10 at 4 ply. On an Explorer II, 4-ply search 
takes about 20 seconds per move. At 5 ply, many moves take over a minute, so the 
program runs the risk of forfeiting. At 3 ply, the program takes only a few seconds 
per move, but it still was able to defeat the author in five straight games, by scores 
of 50-14, 64-0, 51-13, 49-15 and 36-28. Despite these successes, it is likely that the 
evaluation function could be improved greatly with a little tuning of the parameters. 

(defun lago (depth) 
"Use an approximation of lago's evaluation function." 
(alpha-beta-searcher3 depth #'iago-eval)) 

18.13 Other Techniques 
There are many other variations that can be tried to speed up the search and improve 
play. Unfortunately, choosing among the techniques is a bit of a black art. You will 
have to experiment to find the combination that is best for each domain and each 
evaluation function. Most of the following techniques were incorporated, or at least 
considered and rejected, in Bill. 

Iterative Deepening 

We have seen that the average branching factor for Othello is about 10. This means 
that searching to depth . -f 1 takes roughly 10 times longer than search to depth 

n. Thus, we should be willing to go to a lot of overhead before we search one level 
deeper, to assure two things: that search will be done efficiently, and that we won't 
forfeit due to running out of time. A by-now familiar technique, iterative deepening 
(see chapters 6 and 14), serves both these goals. 

<a id='page-647'></a>
Iterative deepening is used as follov/s. The strategy determines how much of the 
remaining time to allocate to each move. A simple strategy could allocate a constant 
amount of time for each move, and a more sophisticated strategy could allocate more 
time for moves at crucial points in the game. Once the time allocation is determined 
for a move, the strategy starts an iterative deepening alpha-beta search. There are 
two complications: First, the search at . ply keeps track of the best moves, so that 
the search at . -h 1 ply will have better ordering information. In many cases it will be 
faster to do both the . and n + 1 ply searches with the ordering information than to 
do only the . -i-1 ply search without it. Second, we can monitor how much time has 
been taken searching each ply, and cut off the search when searching one more ply 
would exceed the allocated time limit. Thus, iterative-deepening search degrades 
gracefully as time limits are imposed. It will give a reasonable answer even with a 
short time allotment, and it will rarely exceed the allotted time. 

Forward Pruning 

One way to cut the number of positions searched is to replace the legal move generator 
with a plausible move generator: in other words, only consider good moves, and never 
even look at moves that seem clearly bad. This technique is called forward pruning. 
It has fallen on disfavor because of the difficulty in determining which moves are 
plausible. For most games, the factors that would go into a plausible move generator 
would be duplicated in the static evaluation function anyway, so forward pruning 
would require more effort without much gain. Worse, forward pruning could rule 
out a brilliant sacrifice - a move that looks bad initially but eventually leads to a gain. 

For some games, forward pruning is a necessity. The game of Go, for example, is 
played on a 19 by 19 board, so the first player has 361 legal moves, and a 6-ply search 
would involve over 2 quadrillion positions. However, many good Go programs can 
be viewed as not doing forward pruning but doing abstraction. There might be 30 
empty squares in one portion of the board, and the program would treat a move to 
any of these squares equivalently. 

Bill uses forward pruning in a limited way to rule out certain moves adjacent to 
the corners. It does this not to save time but because the evaluation function might 
lead to such a move being selected, even though it is in fact a poor move. In other 
words, forward pruning is used to correct a bug in the evaluation function cheaply. 

Nonspeculative Forward Pruning 

This technique makes use of the observation that there are limits in the amount the 
evaluation function can change from one position to the next. For example, if we 
are using the count difference as the evaluation function, then the most a move can 
change the evaluation is +37 (one for placing a piece in the corner, and six captures 
in each of the three directions). The smallest change is 0 (if the player is forced to 


<a id='page-648'></a>

pass). Thus, if there are 2 ply left in the search, and the backed-up value of position 
A has been established as 38 points better than the static value of position B, then it 
is useless to expand position B. This assumes that we are evaluating every position, 
perhaps to do sorted ordering or iterative deepening. It also assumes that no position 
in the search tree is a final position, because then the evaluation could change by 
more than 37 points. In conclusion, it seems that nonspeculative forward pruning is 
not very useful for Othello, although it may play a role in other games. 

Aspiration Search 

Alpha-beta search is initated with the achievable and cutoff boundaries set to 
. os i ng-val ue and wi nni ng-val ue, respectively. In other words, the search assumes 
nothing: the final position may be anything from a loss to a win. But suppose we are 
in a situation somewhere in the mid-game where we are winning by a small margin 
(say the static evaluation for the current position is 50). In most cases, a single move 
will not change the evaluation by very much. Therefore, if we invoked the alpha-
beta search with a window defined by boundaries of, say, 0 and 100, two things can 
happen: if the actual backed-up evaluation for this position is in fact in the range 0 
to 100, then the search will find it, and it will be found quickly, because the reduced 
window will cause more pruning. If the actual value is not in the range, then the 
value returned will reflect that, and we can search again using a larger window. This 
is called aspiration search, because we aspire to find a value within a given window. 
If the window is chosen well, then often we will succeed and will have saved some 
search time. 

Pearl (1984) suggests an alternative called zero-window search. At each level, the 
first possible move, which we'll call m, is searched using a reasonably wide window 
to determine its exact value, which we'll call v. Then the remaining possible moves 
are searched using . as both the lower and upper bounds of the window. Thus, the 
result of the search will tell if each subsequent move is better or worse than m, but 
won't tell how much better or worse. There are three outcomes for zero-window 
search. If no move turns out to be better than m, then stick with m. If a single move is 
better, then use it. If several moves are better than m, then they have to be searched 
again using a wider window to determine which is best. 

There is always a trade-off between time spent searching and information gained. 
Zero-window search makes an attractive trade-off: we gain some search time by 
losing information about the value of the best move. We are still guaranteed of 
finding the best move, we just don't know its exact value. 

Bill's zero-window search takes only 63% of the time taken by full alpha-beta 
search. It is effective because Bill's move-ordering techniques ensure that the first 
move is often best. With random move ordering, zero-window search would not be 
effective. 


<a id='page-649'></a>
Think-Ahead 

A program that makes its move and then waits for the opponent's reply is wasting 
half the time available to it. A better use of time is to compute, or think-ahead while 
the opponent is moving. Think-ahead is one factor that helps Bill defeat lago. While 
many programs have done think-ahead by choosing the most likely move by the 
opponent and then starting an iterative-deepening search assuming that move. Bill's 
algorithm is somewhat more complex. It can consider more than one move by the 
opponent, depending on how much time is available. 

Hashing and Opening Book Moves 

We have been treating the search space as a tree, but in general it is a directed acyclic 
graph (dag): there may be more than one way to reach a particular position, but there 
won't be any loops, because every move adds a new piece. This raises the question 
we explored briefly in section 6.4: should we treat the search space as a tree or a 
graph? By treating it as a graph we eliminate duplicate evaluations, but we have the 
overhead of storing all the previous positions, and of checking to see if a new position 
has been seen before. The decision must be based on the proportion of duplicate 
positions that are actually encountered in play. One compromise solution is to store 
in a hash table a partial encoding of each position, encoded as, say, a single fixnum 
(one word) instead of the seven or so words needed to represent a full board. Along 
with the encoding of each position, store the move to try first. Then, for each new 
position, look in the hash table, and if there is a hit, try the corresponding move first. 
The move may not even be legal, if there is an accidental hash collision, but there is 
a good chance that the move will be the right one, and the overhead is low. 

One place where it is clearly worthwhile to store information about previous 
positions is in the opening game. Since there are fewer choices in the opening, it is a 
good idea to compile an opening "book" of moves and to play by it as long as possible, 
until the opponent makes a move that departs from the book. Book moves can be 
gleaned from the literature, although not very much has been written about Othello 
(as compared to openings in chess). However, there is a danger in following expert 
advice: the positions that an expert thinks are advantageous may not be the same as 
the positions from which our program can play well. It may be better to compile the 
book by playing the program against itself and determining which positions work 
out best. 

The End Game 

It is also a good idea to try to save up time in the midgame and then make an all-out 
effort to search the complete game tree to completion as soon as feasible. Bill can 
search to completion from about 14 ply out. Once the search is done, of course, the 


<a id='page-650'></a>

most promising lines of play should be saved so that it won't be necessary to solve 
the game tree again. 

Metareasontng 

If it weren't for the clock, Othello would be a trivial game: just search the complete 
game tree all the way to the end, and then choose the best move. The clock imposes 
a complication: we have to make all our moves before we run out of time. The 
algorithms we have seen so far manage the clock by allocating a certain amount of 
time to each move, such that the total time is guaranteed (or at least very likely) to 
be less than the allotted time. This is a very crude policy. A finer-grained way of 
managing time is to consider computation itself as a possible move. That is, at every 
tick of the clock, we need to decide if it is better to stop and play the best move we 
have computed so far or to continue and try to compute a better move. It will be 
better to compute more only in the case where we eventually choose a better move; 
it will be better to stop and play only in the case where we would otherwise forfeit 
due to time constraints, or be forced to make poor choices later in the game. An 
algorithm that includes computation as a possible move is called a metareasoning 
system, because it reasons about how much to reason. 

Russell and Wefald (1989) present an approach based on this view. In addition to 
an evaluation function, they assume a variance function, which gives an estimate of 
how much a given position's true value is likely to vary from its static value. At each 
step, their algorithm compares the value and variance of the best move computed so 
far and the second best move. If the best move is clearly better than the second best 
(taking variance into account), then there is no point computing any more. Also, if the 
top two moves have similar values but both have very low variance, then computing 
will not help much; we can just choose one of the two at random. 

For example, if the board is in a symmetric position, then there may be two 
symmetric moves that will have identical value. By searching each move's subtree 
more carefully, we soon arrive at a low variance for both moves, and then we can 
choose either one, without searching further. Of course, we could also add special-
case code to check for symmetry, but the metareasoning approach will work for 
nonsymmetric cases as well as symmetric ones. If there is a situation where two 
moves both lead to a clear win, it won't waste time choosing between them. 

The only situation where it makes sense to continue computing is when there 
are two moves with high variance, so that it is uncertain if the true value of one 
exceeds the other. The metareasoning algorithm is predicated on devoting time to 
just this case. 


<a id='page-651'></a>
Learning 

From the earhest days of computer game playing, it was realized that a championship 
program would need to learn to improve itself. Samuel (1959) describes a program 
that plays checkers and learns to improve its evaluation function. The evaluation 
function is a linear combination of features, such as the number of pieces for each 
player, the number of kings, the number of possible forks, and so on. Learning is 
done by a hill-climbing search procedure: change one of the coefficients for one of 
the features at random, and then see if the changed evaluation function is better than 
the original one. 

Without some guidance, this hill-climbing search would be very slow. First, the 
space is very large - Samuel used 38 different features, and although he restricted 
the coefficients to be a power of two between 0 and 20, that still leaves 21^^ possible 
evaluation functions. Second, the obvious way of determining the relative worth of 
two evaluation functions - playing a series of games between them and seeing which 
wins more of ten - is quite time-consuming. 

Fortunately, there is a faster way of evaluating an evaluation function. We can 
apply the evaluation function to a position and compare this static value with the 
backed-up value determined by an alpha-beta search. If the evaluation function is 
accurate, the static value should correlate well with the backed-up value. If it does not 
correlate well, the evaluation function should be changed in such a way that it does. 
This approach still requires the trial-and-error of hill-climbing, but it will converge 
much faster if we can gain information from every position, rather than just from 
every game. 

In the past few years there has been increased interest in learning by a process 
of guided search. Neural nets are one example of this. They have been discussed 
elsewhere. Another example is genetic learning algorithms. These algorithms start 
with several candidate solutions. In our case, each candidate would consist of a set 
of coefficients for an evaluation function. On each generation, the genetic algorithm 
sees how well each candidate does. The worst candidates are eliminated, and the 
best ones "mate" and "reproduce" - two candidates are combined in some way to 
yield a new one. If the new offspring has inherited both its parents' good points, then 
it will prosper; if it has inherited both its parents' bad points, then it will quickly die 
out. Either way, the idea is that natural selection will eventually yield a high-quality 
solution. To increase the chances of this, it is a good idea to allow for mutations: 
random changes in the genetic makeup of one of the candidates. 

18.14 History and References 
Lee and Mahajan (1986,1990) present the current top Othello program. Bill. Their 
description outlines all the techniques used but does not go into enough detail to allow 


<a id='page-652'></a>

the reader to reconstruct the program. Bill is based in large part on Rosenbloom's 
lago program. Rosenbloom's article (1982) is more thorough. The presentation in 
this chapter is based largely on this article, although it also contains some ideas from 
Bill and from other sources. 

The journal Othello Quarterly is the definitive source for reports on both human 
and computer Othello games and strategies. 

The most popular game for computer implementation is chess. Shannon (1950a,b) 
speculated that a computer might play chess. In a way, this was one of the boldest 
steps in the history of AI. Today, writing a chess program is a challenging but feasible 
project for an undergraduate. But in 1950, even suggesting that such a program 
might be possible was a revolutionary step that changed the way people viewed 
these arithmetic calculating devices. Shannon introduced the ideas of a game tree 
search, minimaxing, and evaluation functions - ideas that remain intact to this day. 
Marsland (1990) provides a good short introduction to computer chess, and David 
Levy has two books on the subject (1976,1988). It was Levy, an international chess 
master, who in 1968 accepted a bet from John McCarthy, Donald Michie, and others 
that a computer chess program would not beat him in the next ten years. Levy won 
the bet. Levy's Heuristic Programming (1990) and Computer Games (1988) cover a variety 
of computer game playing programs. The studies by DeGroot (1965,1966) give a 
fascinating insight into the psychology of chess masters. 

Knuth and Moore (1975) analyze the alpha-beta algorithm, and Pearl's book 
Heuristics (1984) covers all kinds of heuristic search, games included. 

Samuel (1959) is the classic work on learning evaluation function parameters. It 
is based on the game of checkers. Lee and Mahajan (1990) present an alternative 
learning mechanism, using Bayesian classification to learn an evaluation function 
that optimally distinguishes winning positions from losing positions. Genetic algorithms 
are discussed by L. Davis (1987,1991) and Goldberg (1989). 

18-15 Exercises 

&#9635; Exercise 18.3 [s] How many different Othello positions are there? Would it be 
feasible to store the complete game tree and thus have a perfect player? 

&#9635; Exercise 18.4 [m] At the beginning of this chapter, we implemented pieces as an 
enumerated type. There is no built-in facility in Common Lisp for doing this, so 
we had to introduce a series of defconstant forms. Define a macro for defining 
enumerated types. What else should be provided besides the constants? 

&#9635; Exercise 18.5 [h] Add fixnum and speed declarations to the lago evaluation func



<a id='page-653'></a>
tion and the alpha-beta code. How much does this speed up lago? What other 
efficiency measures can you take? 

&#9635; Exercise 18.6 [h] Implement an iterative deepening search that allocates time for 
each move and checks between each iteration if the time is exceeded. 

&#9635; Exercise 18.7 [h] Implement zero-window search, as described in section 18.13. 

&#9635; Exercise 18.8 [d] Read the references on Bill (Lee and Mahajan 1990, and 1986 if 
you can get it), and reimplement Bill's evaluation function as best you can, using the 
table-based approach. It will also be helpful to read Rosenbloom 1982. 

&#9635; Exercise 18.9 [d] Improve the evaluation function by tuning the parameters, using 
one of the techniques described in section 18.13. 

&#9635; Exercise 18.10 [h] Write move-generation and evaluation functions for another 
game, such as chess or checkers. 

18.16 Answers 
Answer 18.2 The wei ghted-squa res strategy wins the first game by 20 pieces, 
but when count-di ff erence plays first, it captures all the pieces on its fifth move. 
These two games alone are not enough to determine the best strategy; the function 
othel 1 o-seri es on [page 626](chapter18.md#page-626) shows a better comparison. 

Answer 18.3 3^ = 3,433,683,820,292,512,484,657,849,089,281. No. 


<a id='page-654'></a>

Answer 18.4 Besides the constants, we provide a def type for the type itself, and 
conversion routines between integers and symbols: 

(defmacro define-enumerated-type (type &rest elements) 
"Represent an enumerated type with integers 0-n. " 
'(progn 

(deftype .type () '(integer 0 ,( - (length elements) 1))) 
(defun .(symbol type *->symbol) (.type) 
(elt '.elements .type)) 
(defun .(symbol 'symbol-> type) (symbol) 
(position symbol '.elements)) 

.(loop for element in elements 
for i from 0 
collect '(defconstant .element .i)))) 

Here's how the macro would be used to define the piece data type, and the code 
produced: 

> (macroexpand 
'(define-enumerated-type piece 
empty black white outer)) 

(PROGN 
(DEFTYPE PIECE () '(INTEGER 0 3)) 
(DEFUN PIECE->SYMBOL (PIECE) 

(ELT '(EMPTY BLACK WHITE OUTER) PIECE)) 
(DEFUN SYMBOL->PIECE (SYMBOL) 

(POSITION SYMBOL '(EMPTY BLACK WHITE OUTER))) 
(DEFCONSTANT EMPTY 0) 
(DEFCONSTANT BLACK 1) 
(DEFCONSTANT WHITE 2) 
(DEFCONSTANT OUTER 3)) 

A more general facility would, like defstruct, provide for several options. For 
example, it might allow for a documentation string for the type and each constant, 
and for a : cone-name, so the constants could have names like pi ece-empty instead 
of empty. This would avoid conflicts with other types that wanted to use the same 
names. The user might also want the ability to start the values at some number other 
than zero, or to assign specific values to some of the symbols. 


## Chapter 19
<a id='page-655'></a>

Introduction to 
Natural Language 

Language is everywhere. It permeates our thoughts, 
mediates our relations with others, and even creeps 
into our dreams. The overwhelming hulk of human 
knowledge is stored and transmitted in language. 
Language is so ubiquitous that we take itfor granted, 
but without it, society as we know it would 
be impossible. 

- Ronand Langacker 

Language and its Structure (1967) 

A
A
natural language is a language spoken by people, such as English, German, or Tagalog. 
This is in opposition to artificial languages like Lisp, FORTRAN, or Morse code. 
Natural language processing is an important part of AI because language is intimately 
connected to thought. One measure of this is the number of important books that mention 
language and thought in the title: in AI, Schank and Colby's Computer Models of Thought 
and Language; in linguistics, Whorf's Language, Thought, and Reality (and Chomsky's Language 
and Mind;) in philosophy, Fodor's The Language of Thought; and in psychology, Vygotsky's 
Thought and Language and John Anderson's Language, Memory, and Thought. Indeed, language is 


<a id='page-656'></a>

the trait many think of as being the most characteristic of humans. Much controversy 
has been generated over the question of whether animals, especially primates and 
dolphins, can use and "understand" language. Similar controversy surrounds the 
same question asked of computers. 

The study of language has been traditionally separated into two broad classes: 
syntax, or grammar, and semantics, or meaning. Historically, syntax has achieved 
the most attention, largely because on the surface it is more amenable to formal and 
semiformal methods. Although there is evidence that the boundary between the two 
is at best fuzzy, we still maintain the distinction for the purposes of these notes. We 
will cover the "easier" part, syntax, first, and then move on to semantics. 

A good artificial language, like Lisp or C, is unambiguous. There is only one 
interpretation for a valid Lisp expression. Of course, the interpretation may depend 
on the state of the current state of the Lisp world, such as the value of global variables. 
But these dependencies can be explicitly enumerated, and once they are spelled out, 
then there can only be one meaning for the expression,^ 

Natural language does not work like this. Natural expressions are inherently 
ambiguous, depending on any number of factors that can never be quite spelled out 
completely. It is perfectly reasonable for two people to disagree on what some other 
person meant by a natural language expression. (Lawyers and judges make their 
living largely by interpreting natural language expressions - laws - that are meant to 
be unambiguous but are not.) 

This chapter is a brief introduction to natural language processing. The next 
chapter gives a more thorough treatment from the point of view of logic grammars, 
and the chapter after that puts it all together into a full-fledged system. 

19-1 Parsing with a Phrase-Structure Grammar 

To parse a sentence means to recover the constituent structure of the sentence - to 
discover what sequence of generation rules could have been applied to come up with 
the sentence. In general, there may be several possible derivations, in which case 
we say the sentence is grammatically ambiguous. In certain circles, the term "parse" 
means to arrive at an understanding of a sentence's meaning, not just its grammatical 
form. We will attack that more difficult question later. 

^Some erroneous expressions are underspecified and may return different results in different 
implementations, but we will ignore that problem. 


<a id='page-657'></a>

We start with the grammar defined on [page 39](chapter2.md#page-39) for the generate program: 

(defvar ^grammar* "The grammar used by GENERATE.") 

(defparameter *grammarl* 

'((Sentence -> (NP VP)) 
(NP -> (Art Noun)) 
(VP -> (Verb NP)) 
(Art -> the a) 
(Noun -> man ball woman table) 
(Verb -> hit took saw liked))) 
Our parser takes as input a list of words and returns a structure containing the parse 
tree and the unparsed words, if any. That way, we can parse the remaining words 
under the next category to get compound rules. For example, in parsing "the man 
saw the table," we would first parse "the man," returning a structure representing 
the noun phrase, with the remaining words "saw the table." This remainder would 
then be parsed as a verb phrase, returning no remainder, and the two phrases could 
then be joined to form a parse that is a complete sentence with no remainder. 

Before proceeding, I want to make a change in the representation of grammar 
rules. Currently, rules have a left-hand side and a list of alternative right-hand sides. 
But each of these alternatives is really a separate rule, so it would be more modular 
to write them separately. For the generate program it was fine to have them all together, 
because that made processing choices easier, but now I want a more flexible 
representation. Later on we will want to add more information to each rule, like the 
semantics of the assembled left-hand side, and constraints between constituents on 
the right-hand side, so the rules would become quite large indeed if we didn't split up 
the alternatives. I also take this opportunity to clear up the confusion between words 
and category symbols. The convention is that a right-hand side can be either an 
atom, in which case it is a word, or a list of symbols, which are then all interpreted as 
categories. To emphasize this, I include "noun" and "verb" as nouns in the grammar 
*grammar3*, which is otherwise equivalent to the previous *grammarl*. 

(defparameter *grammar3* 

'((Sentence -> (NP VP)) 

(NP -> (Art Noun)) 

(VP -> (Verb NP)) 

(Art -> the) (Art -> a) 

(Noun -> man) (Noun -> ball) (Noun -> woman) (Noun -> table) 

(Noun -> noun) (Noun -> verb) 

(Verb -> hit) (Verb -> took) (Verb -> saw) (Verb -> liked))) 

(setf *grammar* *grammar3*) 

I also define the data types rul e, parse, and tree, and some functions for getting 


<a id='page-658'></a>

at the rules. Rules are defined as structures of type list with three slots: the left-
hand side, the arrow (which should always be represented as the literal ->) and the 
right-hand side. Compare this to the treatment on [page 40](chapter2.md#page-40). 

(defstruct (rule (:type list)) Ihs -> rhs) 

(defstruct (parse) "A parse tree and a remainder." tree rem) 

;; Trees are of the form: (Ihs . rhs) 
(defun new-tree (cat rhs) (cons cat rhs)) 
(defun tree-lhs (tree) (first tree)) 
(defun tree-rhs (tree) (rest tree)) 

(defun parse-lhs (parse) (tree-lhs (parse-tree parse))) 

(defun lexical-rules (word) 
"Return a list of rules with word on the right-hand side." 
(find-all word ^grammar* :key #'rule-rhs :test #'equal)) 

(defun rules-starting-with (cat) 
"Return a list of rules where cat starts the rhs." 
(find-all cat *grammar* 

:key #'(lambda (rule) (first-or-nil (rule-rhs rule))))) 

(defun first-or-nil (x) 
"The first element of . if it is a list; else nil." 
(if (consp X) (first x) nil)) 

Now we're ready to define the parser. The main function parser takes a list of 
words to parse. It calls parse, which returns a Ust of all parses that parse some 
subsequence of the words, starting at the beginning, parser keeps only the parses 
with no remainder - that is, the parses that span all the words. 

(defun parser (words) 
"Return all complete parses of a list of words." 
(mapcar #'parse-tree (complete-parses (parse words)))) 

(defun complete-parses (parses) 
"Those parses that are complete (have no remainder)." 
(find-all-if #*null parses :key #*parse-rem)) 

The function parse looks at the first word and considers each category it could be. It 
makes a parse of the first word under each category, and calls extend - pa rse to try to 
continue to a complete parse, pa rse uses mapcan to append together all the resulting 
parses. As an example, suppose we are trying to parse "the man took the ball." pa rse 
would find the single lexical rule for "the" and call extend-pa rse with a parse with 
tree (Art t he) and remainder "man took the ball," with no more categories needed. 


<a id='page-659'></a>
extend-parse has two cases. If the partial parse needs no more categories to be 
complete, then it returns the parse itself, along with any parses that can be formed 
by extending parses starting with the partial parse. In our example, there is one rule 
startingwith Art, namely (NP -> (Art Noun)), so the function would try to extend 
theparse tree (NP (Art the)) with remainder "man took the ball," with the category 
Noun needed. That call to extend-parse represents the second case. We first parse 
"man took the ball," and for every parse that is of category Noun (there will be only 
one), we combine with the partial parse. In this case we get (NP (Art the) (Noun 
man)). This gets extended as a sentence with a VP needed, and eventually we get a 
parse of the complete hst of words. 

(defun parse (words) 
"Bottom-up parse, returning all parses of any prefix of words." 
(unless (null words) 

(mapcan #'(lambda (rule) 
(extend-parse (rule-lhs rule) (list (first words)) 
(rest words) nil)) 
(lexical-rules (first words))))) 

(defun extend-parse (Ihs rhs rem needed) 
"Look for the categories needed to complete the parse." 
(if (null needed) 

If nothing needed, return parse and upward extensions 
(let ((parse (make-parse :tree (new-tree Ihs rhs) :rem rem))) 
(cons parse 
(mapcan 
#.(lambda (rule) 

(extend-parse (rule-lhs rule) 
(list (parse-tree parse)) 
rem (rest (rule-rhs rule)))) 

(rules-starting-with Ihs)))) 
otherwise try to extend rightward 
(mapcan 
#'(lambda (p) 
(if (eq (parse-lhs p) (first needed)) 
(extend-parse Ihs (appendl rhs (parse-tree p)) 
(parse-rem p) (rest needed)))) 

(parse rem)))) 

This makes use of the auxiliary function appendl: 
(defun appendl (items item) 
"Add item to end of list of items." 
(append items (list item))) 


<a id='page-660'></a>

Some examples of the parser in action are shown here: 

> (parser '(the table)) 
((NP (ART THE) (NOUN TABLE))) 

> (parser '(the ball hit the table)) 

((SENTENCE (NP (ART THE) (NOUN BALD) 

(VP (VERB HIT) 

(NP (ARTTHE) (NOUN TABLE))))) 

> (parser '(the noun took the verb)) 
((SENTENCE (NP (ART THE) (NOUN NOUN)) 
(VP (VERB TOOK) 
(NP (ARTTHE) (NOUN VERB))))) 

19.2 Extending the Grammar and 
Recognizing Ambiguity 
Overall, the parser seems to work fine, but the range of sentences we can parse is 
quite limited with the current grammar. The following grammar includes a wider 
variety of linguistic phenomena: adjectives, prepositional phrases, pronouns, and 
proper names. It also uses the usual linguistic conventions for category names, 
summarized in the table below: 

Category Examples 
Sentence John likes Mary

s 

NP Noun Phrase John; a blue table 
VP Verb Phrase likes Mary; hit the ball 
PP Prepositional Phrase to Mary; with the man 

A Adjective little; blue 

A+ A list of one or more adjectives little blue 
D Determiner the; a 
. Noun ball; table 
Name Proper Name John; Mary 
. Preposition to; with 
Pro Pronoun you; me 
V Verb liked; hit 


<a id='page-661'></a>

Here is the grammar: 

(defparameter *grammar4* 

'((S -> (NP VP)) 
(NP -> (D N)) 
(NP -> (D A+ N)) 
(NP -> (NP PP)) 
(NP -> (Pro)) 
(NP -> (Name)) 
(VP -> (V NP)) 
(VP -> (V)) 
(VP -> (VP PP)) 
(PP -> (P NP)) 
(A+ -> (A)) 
(A+ -> (A A+)) 
(Pro -> I) (Pro -> you) (Pro -> he) (Pro -> she) 
(Pro -> it) (Pro -> me) (Pro -> him) (Pro -> her) 
(Name -> John) (Name -> Mary) 
(A -> big) (A -> little) (A -> old) (A -> young) 
(A -> blue) (A -> green) (A -> orange) (A -> perspicuous) 
(D -> the) (D -> a) (D -> an) 
(N -> man) (N -> ball) (N -> woman) (N -> table) (N -> orange) 
(N -> saw) (N -> saws) (N -> noun) (N -> verb) 
(P -> with) (P -> for) (P -> at) (P -> on) (P -> by) (P -> of) (P -> in) 
(V -> hit) (V -> took) (V -> saw) (V -> liked) (V -> saws))) 
(setf ^grammar* *grammar4*) 

Now we can parse more interesting sentences, and we can see a phenomenon that 
was not present in the previous examples: ambiguous sentences. The sentence "The 
man hit the table with the ball" has two parses, one where the ball is the thing that 
hits the table, and the other where the ball is on or near the table, parser finds both 
of these parses (although of course it assigns no meaning to either parse): 

> (parser '(The man hit the table with the ball)) 
((S (NP (D THE) (N MAN)) 
(VP (VP (V HIT) (NP (D THE) (N TABLE))) 
(PP (P WITH) (NP (DTHE) (N BALL))))) 
(S (NP (D THE) (N MAN)) 
(VP (V HIT) 
(NP (NP (D THE) (N TABLE)) 
(PP (P WITH) (NP (DTHE) (N BALL))))))) 

Sentences are not the only category that can be ambiguous, and not all ambiguities 
have to be between parses in the same category. Here we see a phrase that is 
ambiguous between a sentence and a noun phrase: 


<a id='page-662'></a>

> (parser '(the orange saw)) 
((S (NP (D THE) (N ORANGE)) (VP (V SAW))) 
(NP (D THE) (A+ (A ORANGE)) (N SAW))) 

19.3 More Efficient Parsing 
With more complex grammars and longer sentences, the parser starts to slow down. 
The main problem is that it keeps repeating work. For example, in parsing "The 
man hit the table with the ball," it has to reparse "with the ball" for both of the 
resulting parses, even though in both cases it receives the same analysis, a PP. We 
have seen this problem before and have already produced an answer: memoization 
(see section 9.6). To see how much memoization will help, we need a benchmark: 

> (setf s (generate 's)) 
(THE PERSPICUOUS BIG GREEN BALL BY A BLUE WOMAN WITH A BIG MAN 
HIT A TABLE BY THE SAW BY THE GREEN ORANGE) 

> (time (length (parser s))) 
Evaluation of (LENGTH (PARSER S)) took 33.11 Seconds of elapsed time. 
10 

The sentence S has 10 parses, since there are two ways to parse the subject NP and 
five ways to parse the VP. It took 33 seconds to discover these 10 parses with the 
pa rse function as it was written. 

We can improve this dramatically by memoizing parse (along with the table-
lookup functions). Besides memoizing, the only change is to clear the memoization 
table within parser. 

(memoize 'lexical-rules) 
(memoize *rules-starting-with) 
(memoize 'parse -.test #*eq) 

(defun parser (words) 
"Return all complete parses of a list of words." 
(clear-memoize 'parse) 
(mapcar #'parse-tree (complete-parses (parse words)))) 

In normal human language use, memoization would not work very well, since the 
interpretation of a phrase depends on the context in which the phrase was uttered. 
But with context-free grammars we have a guarantee that the context cannot affect the 
interpretation. The call (parse words) must return all possible parses for the words. 
We are free to choose between the possibilities based on contextual information, but 


<a id='page-663'></a>

context can never supply a new interpretation that is not in the context-free list of 

parses. 

The function use is introduced to tell the table-lookup functions that they are out 
of date whenever the grammar changes: 

(defun use (grammar) 

"Switch to a new grammar." 

(clear-memoize 'rules-starting-with) 

(clear-memoize 'lexical-rules) 

(length (setf *grammar* grammar))) 

Now we run the benchmark again with the memoized version of pa rse: 

> (time (length (parser s))) 

Evaluation of (LENGTH (PARSER S 'S)) took .13 Seconds of elapsed time. 

10 

By memoizing pa rs e we reduce the parse time from 33 to .13 seconds, a 250-fold speedup. 
We can get a more systematic comparison by looking at a range of examples. 
For example, consider sentences of the form "The man hit the table [with the ball]*" 
for zero or more repetitions of the PP "with the ball." In the following table we 
record N, the number of repetitions of the PP, along with the number of resulting 
parses^, and for both memoized and unmemoized versions of parse, the number 
of seconds to produce the parse, the number of parses per second (PPS), and the 
number of recursive calls to parse. The performance of the memoized version is 
quite acceptable; for N=5, a 20-word sentence is parsed into 132 possibilities in .68 
seconds, as opposed to the 20 seconds it takes in the unmemoized version. 

^The number of parses of sentences of this kind is the same as the number of bracketings 
of a arithmetic expression, or the number of binary trees with a given number of leaves. The 
resulting sequence (1,2,5,14,42,...) is known as the Catalan Numbers. This kind of ambiguity 
is discussed by Church and Patil (1982) in their articleCoping with Syntactic Ambiguity, or How 
to Put the Block in the Box on the Table. 


<a id='page-664'></a>

Memoized Unmemoized 
. Parses Sees PPS CaUs Sees PPS CaUs 
0 1 0.02 60 4 0.02 60 17 
1 2 0.02 120 11 0.07 30 96 
2 5 0.05 100 21 0.23 21 381 
3 14 0.10 140 34 0.85 16 1388 
4 42 0.23 180 50 3.17 13 4999 
5 132 0.68 193 69 20.77 6 18174 
6 429 1.92 224 91 -
7 1430 5.80 247 116 -
8 4862 20.47 238 144 -

&#9635; Exercise 19.1 Pi] It seems that we could be more efficient still by memoizing with 
a table consisting of a vector whose length is the number of words in the input (plus 
one). Implement this approach and see if it entails less overhead than the more 
general hash table approach. 

19.4 The Unknown-Word Problem 
As it stands, the parser cannot deal with unknown words. Any sentence containing 
a word that is not in the grammar will be rejected, even if the program can parse all 
the rest of the words perfectly. One way of treating unknown words is to allow them 
to be any of the "open-class" categories - nouns, verbs, adjectives, and names, in our 
grammar. An unknown word will not be considered as one of the "closed-class" 
categories - prepositions, determiners, or pronouns. This can be programmed very 
simply by having 1 exi ca 1 - rul es return a list of these open-class rules for every word 
that is not already known. 

(defparameter *open-categories* '(NVA Name) 
"Categories to consider for unknown words") 

(defun lexical-rules (word) 
"Return a list of rules with word on the right-hand side." 
(or (find-all word *grammar* :key #'rule-rhs :test #'equal) 

(mapcar #'(lambda (cat) '(.cat -> .word)) *open-categories*))) 

With memoization of 1 exi cal - rul es, this means that the lexicon is expanded every 
time an unknown word is encountered. Let's try this out: 

> (parser '(John liked Mary)) 
((S (NP (NAME JOHN)) 
(VP (V LIKED) (NP (NAME MARY))))) 


<a id='page-665'></a>

> (parser '(Dana liked Dale)) 
((S (NP (NAME DANA)) 
(VP (V LIKED) (NP (NAME DALE))))) 

> (parser '(the rab zaggled the woogly quax)) 
((S (NP (D THE) (N RAB)) 
(VP (V ZAGGLED) (NP (D THE) (A+ (A WOOGLY)) (N QUAX))))) 

We see the parser works as well with words it knows (John and Mary) as with new 
words (Dana and Dale), which it can recognize as names because of their position 
in the sentence. In the last sentence in the example, it recognizes each unknown 
word unambiguously. Things are not always so straightforward, unfortunately, as 
the following examples show: 

> (parser '(the slithy toves gymbled)) 

((S (NP (D THE) (N SLITHY)) (VP (V TOVES) (NP (NAME GYMBLED)))) 
(S (NP (D THE) (A+ (A SLITHY)) (N TOVES)) (VP (V GYMBLED))) 
(NP (D THE) (A+ (A SLITHY) (A+ (A TOVES))) (N GYMBLED))) 

> (parser '(the slithy toves gymbled on the wabe)) 
((S (NP (D THE) (N SLITHY)) 
(VP (VP (V TOVES) (NP (NAME GYMBLED))) 
(PP (P ON) (NP (D THE) (N WABE))))) 
(S (NP (D THE) (N SLITHY)) 
(VP (V TOVES) (NP (NP (NAME GYMBLED)) 
(PP (P ON) (NP (D THE) (N WABE)))))) 
(S (NP (D THE) (A+ (A SLITHY)) (N TOVES)) 
(VP (VP (V GYMBLED)) (PP (P ON) (NP (D THE) (N WABE))))) 
(NP (NP (D THE) (A+ (A SLITHY) (A+ (A TOVES))) (N GYMBLED)) 
(PP (P ON) (NP (D THE) (N WABE))))) 

If the program knew morphology - that a y at the end of a word often signals an 
adjective, an s a plural noun, and an ed a past-tense verb - then it could do much 
better. 

19.5 Parsing into a Semantic Representation 
Syntactic parse trees of a sentence may be interesting, but by themselves they're not 
very useful. We use sentences to communicate ideas, not to display grammatical 
structures. To explore the idea of the semantics, or meaning, of a phrase, we need 
a domain to talk about. Imagine the scenario of a compact disc player capable of 
playing back selected songs based on their track number. Imagine further that this 
machine has buttons on the front panel indicating numbers, as well as words such as 
"play," "to," "and," and "without." If you then punch in the sequence of buttons "play 


<a id='page-666'></a>

1 to 5 without 3/' you could reasonably expect the machine to respond by playing 
tracks 1,2,4, and 5. After a few such successful interactions, you might say that the 
machine "understands" a limited language. The important point is that the utility of 
this machine would not be enhanced much if it happened to display a parse tree of 
the input. On the other hand, you would be justifiably annoyed if it responded to 
"play 1 to 5 without 3" by playing 3 or skipping 4. 

Now let's stretch the imagination one more time by assuming that this CD player 
comes equipped with a full Common Lisp compiler, and that we are now in charge 
of writing the parser for its input language. Let's first consider the relevant data 
structures. We need to add a component for the semantics to both the rule and tree 
structures. Once we've done that, it is clear that trees are nothing more than instances 
of rules, so their definitions should reflect that. Thus, I use an : 1nc1 ude defstruct 
to define trees, and I specify no copier function, because copy-tree is already a 
Common Lisp function, and I don't want to redefine it. To maintain consistency 
with the old new-tree function (and to avoid having to put in all those keywords) I 
definetheconstructor new-tree. Thisoptiontodefstructmakes (new-tree a b c) 
equivalent to (make-tree :lhs a :sem b :rhsc). 

(defstruct (rule (itype list)) 
Ihs -> rhs sem) 

(defstruct (tree (:type list) (:include rule) (rcopiernil) 
(:constructor new-tree (Ihs sem rhs)))) 

We will adopt the convention that the semantics of a word can be any Lisp object. For 
example, the semantics of the word "1" could be the object 1, and the semantics of 
"without" could be the function set-di ff erence. The semantics of a tree is formed 
by taking the semantics of the rule that generated the tree and applying it (as a 
function) to the semantics of the constituents of the tree. Thus, the grammar writer 
must insure that the semantic component of rules are functions that expect the right 
number of arguments. For example, given the rule 

(NP -> (NP CONJ NP) infix-funcall) 

then the semantics of the phrase "1 to 5 without 3" could be determined by first deter-
miningthesemanticsof"lto5"tobe(l 2 3 4 5),of"without"tobeset -difference, 
and of "3" to be (3). After these sub-constituents are determined, the rule is applied 
by calling the function infix-funcall with the three arguments (1 2 3 4 5), 
set-difference, and (3). Assuming that infix-funcall is defined to apply its 
second argument to the other two arguments, the result will be (1 2 4 5). 

This may make more sense if we look at a complete grammar for the CD player 
problem: 


<a id='page-667'></a>

(use 

'((NP -> (NP CONJ NP) infix-funcall) 
(NP -> (N) list) 
(NP -> (N . .) infix-funcall) 
(. -> (DIGIT) identity) 
(P -> to integers) 
(CONJ -> and union) 
(CONJ -> without set-difference) 
(N -> 1 1) (N -> 2 2) (N -> 3 3) (N -> 4 4) (N -> 5 5) 
(N -> 6 6) (N -> 7 7) (N -> 8 8) (N -> 9 9) (N -> 0 0))) 

(defun integers (start end) 
"A list of all the integers in the range [start...end] inclusive." 
(if (> start end) nil 

(cons start (integers (+ start 1) end)))) 

(defun infix-funcall (argl function arg2) 
"Apply the function to the two arguments" 
(funcall function argl arg2)) 

Consider the first three grammar rules, which are the only nonlexical rules. The first 
says that when two NPs are joined by a conjunction, we assume the translation of 
the conjunction will be a function, and the translation of the phrase as a whole is 
derived by calling that function with the translations of the two NPs as arguments. 
The second rule says that a single noun (whose translation should be a number) 
translates into the singleton list consisting of that number. The third rule is similar 
to the first, but concerns joining Ns rather than NPs. The overall intent is that the 
translation of an NP will always be a list of integers, representing the songs to play. 

As for the lexical rules, the conjunction "and" translates to the union function, 
"without" translates to the function that subtracts one set from another, and "to" 
translates to the function that generates a list of integers between two end points. 
The numbers "0" to "9" translate to themselves. Note that both lexical rules like 
"CONJ -> and" and nonlexical rules like "NP -> (N . .)" can have functions as 
their semantic translations; in the first case, the function will just be returned as the 
semantic translation, whereas in the second case the function will be applied to the 
list of constituents. 

Only minor changes are needed to par s e to support this kind of semantic processing. 
As we see in the following, we add a sem argument to extend - pa r se and arrange 
to pass the semantic components around properly. When we have gathered all the 
right-hand-side components, we actually do the function application. All changes 
are marked with We adopt the convention that the semantic value .i 1 indicates 
failure, and we discard all such parses. 


<a id='page-668'></a>

(defun parse (words) 
"Bottom-up parse, returning all parses of any prefix of words. 
This version has semantics." 
(unless (null words) 

(mapcan #'(lambda (rule) 
(extend-parse (rule-lhs rule) (rule-sem rule) 
(list (first words)) (rest words) nil)) 
(lexical-rules (first words))))) 

(defun extend-parse (Ihs sem rhs rem needed) 
"Look for the categories needed to complete the parse. 
This version has semantics." 
(if (null needed) 

If nothing is needed, return this parse and upward extensions, 
:; unless the semantics fails 
(let ((parse (make-parse rtree (new-tree Ihs sem rhs) :rem rem))) 

(unless (null (apply-semantics (parse-tree parse))) 
(cons parse 
(mapcan 
#'(lambda (rule) 

(extend-parse (rule-lhs rule) (rule-semrule) 
(list (parse-tree parse)) rem 
(rest (rule-rhs rule)))) 

(rules-starting-with Ihs))))) 
;; otherwise try to extend rightward 
(mapcan 

#*(lambda (p) 
(if (eq (parse-lhs p) (first needed)) 
(extend-parse Ihs sem (appendl rhs (parse-tree p)) 
(parse-rem p) (rest needed)))) 

(parse rem)))) 

We need to add some new functions to support this: 
(defun apply-semantics (tree) 
"For terminal nodes, just fetch the semantics. 
Otherwise, apply the sem function to its constituents." 
(if (terminal-tree-p tree) 
(tree-sem tree) 
(setf (tree-sem tree) 
(apply (tree-sem tree) 
(mapcar #'tree-sem (tree-rhs tree)))))) 

(defun terminal-tree-p (tree) 
"Does this tree have a single word on the rhs?" 
(and (length=1 (tree-rhs tree)) 

(atom (first (tree-rhs tree))))) 


<a id='page-669'></a>

(defun meanings (words) 
"Return all possible meanings of a phrase. Throw away the syntactic part." 
(remove-duplicates (mapcar #'tree-sem (parser words)) :test #'equal)) 

Here are some examples of the meanings that the parser can extract: 

> (meanings '(1 to 5 without 3)) 
((1 2 4 5)) 

> (meanings '(1 to 4 and 7 to 9)) 
((123478 9)) 

> (meanings '(1 to 6 without 3 and 4)) 
((12 4 5 6) 
(1 2 5 6)) 

The example "(1 to 6 without 3 and 4)" is ambiguous. The first reading corresponds 
to "((1 to 6) without 3) and 4/' while the second corresponds to "(1 to 6) 
without (3 and 4)." The syntactic ambiguity leads to a semantic ambiguity - the two 
meanings have different lists of numbers in them. However, it seems that the second 
reading is somehow better, in that it doesn't make a lot of sense to talk of adding 4 to 
a set that already includes it, which is what the first translation does. 

We can upgrade the lexicon to account for this. The following lexicon insists 
that "and" conjoins disjoint sets and that "without" removes only elements that were 
already in the first argument. If these conditions do not hold, then the translation 
will return nil, and the parse will fail. Note that this also means that an empty list, 
such as "3 to 2," will also fail. 

The previous grammar only allowed for the numbers 0 to 9. We can allow larger 
numbers by stringing together digits. So now we have two rules for numbers: a 
number is either a single digit, in which case the value is the digit itself (the i dent i ty 
function), or it is a number followed by another digit, in which case the value is 10 
times the number plus the digit. We could alternately have specified a number to be 
a digit followed by a number, or even a number followed by a number, but either of 
those formulations would require a more complex semantic interpretation. 

(use 

'((NP -> (NP CONJ NP) infix-funcall) 
(NP -> (N) list) 
(NP -> (N . .) infix-funcall) 
(. -> (DIGIT) identity) 
(N -> (N DIGIT) 10*N+D) 
(P -> to integers) 
(CONJ -> and union*) 
(CONJ -> without set-diff) 
(DIGIT -> 1 1) (DIGIT -> 2 2) (DIGIT -> 3 3) 


<a id='page-670'></a>

(DIGIT -> 4 4) (DIGIT -> 5 5) (DIGIT -> 6 6) 
(DIGIT -> 7 7) (DIGIT -> 8 8) (DIGIT -> 9 9) 
(DIGIT -> 0 0))) 

(defun union* (x y) (if (null (intersection . y)) (append . y))) 
(defun set-diff (. y) (if (subsetp y .) (set-difference . y))) 
(defun 10*N-^D (N D) (+ (* 10 N) D)) 

With this new grammar, we can get single interpretations out of most reasonable 
inputs: 

> (meanings '(1 to 6 without 3 and 4)) 
((1 2 5 6)) 

> (meanings '(1 and 3 to 7 and 9 without 5 and 6)) 
((13 4 7 9)) 

> (meanings '(1 and 3 to 7 and 9 without 5 and 2)) 
((134679 2)) 

> (meanings '(1 9 8 to 2 0 D) 
((198 199 200 201)) 

> (meanings '(1 2 3)) 
(123 (123)) 

The example "1 2 3" shows an ambiguity between the number 123 and the list (123), 
but all the others are unambiguous. 

19.6 Parsing with Preferences 
One reason we have unambiguous interpretations is that we have a very limited 
domain of interpretation: we are dealing with sets of numbers, not lists. This is 
perhaps typical of the requests faced by a CD player, but it does not account for 
all desired input. For example, if you had a favorite song, you couldn't hear it 
three times with the request "1 and 1 and 1" under this grammar. We need some 
compromise between the permissive grammar, which generated all possible parses, 
and the restrictive grammar, which eliminates too many parses. To get the "best" 
interpretation out of an arbitrary input, we will not only need a new grammar, we 
will also need to modify the program to compare the relative worth of candidate 
interpretations. In other words, we will assign each interpretation a numeric score, 
and then pick the interpretation with the highest score. 

We start by once again modifying the rule and tree data types to include a score 
component. As with the sem component, this will be used to hold first a function to 
compute a score and then eventually the score itself. 


<a id='page-671'></a>

(defstruct (rule (:type list) 
(:constructor 
rule (Ihs -> rhs &optional sem score))) 
Ihs -> rhs sem score) 

(defstruct (tree (itype list) (rinclude rule) (:copiernil) 
(:constructor new-tree (Ihs sem score rhs)))) 

Note that we have added the constructor function rul e. The intent is that the sem 
and score component of grammar rules should be optional. The user does not have 
to supply them, but the function use will make sure that the function rul e is called 
to fill in the missing sem and score values with ni 1. 

(defun use (grammar) 
"Switch to a new grammar." 
(clear-memoize 'rules-starting-with) 
(clear-memoize 'lexical-rules) 
(length (setf *grammar* 

(mapcar #'(lambda (r) (apply #'rule r)) 
grammar)))) 

Now we modify the parser to keep track of the score. The changes are again minor, 
and mirror the changes needed to add semantics. There are two places where we 
put the score into trees as we create them, and one place where we apply the scoring 
function to its arguments. 

(defun parse (words) 
"Bottom-up parse, returning all parses of any prefix of words. 
This version has semantics and preference scores." 
(unless (null words) 

(mapcan #'(lambda (rule) 

(extend-parse 
(rule-lhs rule) (rule-sem rule) 
(rule-score rule) (list (first words)) 
(rest words) nil)) 

(lexical-rules (first words))))) 

(defun extend-parse (Ihs sem score rhs rem needed) 
"Look for the categories needed to complete the parse. 
This version has semantics and preference scores." 
(if (null needed) 

If nothing is needed, return this parse and upward extensions, 
;; unless the semantics fails 
(let ((parse (make-parse :tree (new-tree Ihs sem score rhs) 

:rem rem))) 
(unless (null (apply-semantics (parse-tree parse))) 


<a id='page-672'></a>

(apply-scorer (parse-tree parse)) 
(cons parse 
(mapcan 
#'(lambda (rule) 

(extend-parse 
(rule-lhs rule) (rule-sem rule) 
(rule-score rule) (list (parse-tree parse)) 
rem (rest (rule-rhs rule)))) 

(rules-starting-with Ihs))))) 
otherwise try to extend rightward 
(mapcan 
#*(lambda (p) 
(if (eq (parse-lhs p) (first needed)) 

(extend-parse Ihs sem score 
(appendl rhs (parse-tree p)) 
(parse-rem p) (rest needed)))) 

(parse rem)))) 

Again we need some new functions to support this. Most important is appl y - scorer, 
which computes the score for a tree. If the tree is a terminal (a word), then the function 
just looks up the score associated with that word. In this grammar all words have 
a score of 0, but in a grammar with ambiguous words it would be a good idea to 
give lower scores for infrequently used senses of ambiguous words. If the tree is 
a nonterminal, then the score is computed in two steps. First, all the scores of the 
constituents of the tree are added up. Then, this is added to a measure for the tree 
as a whole. The rule associated with each tree will have either a number attached to 
it, which is added to the sum, or a function. In the latter case, the function is applied 
to the tree, and the result is added to obtain the final score. Asa final special case, if 
the function returns nil, then we assume it meant to return zero. This will simplify 
the definition of some of the scoring functions. 

(defun apply-scorer (tree) 
"Compute the score for this tree." 
(let ((score (or (tree-score tree) 0))) 

(setf (tree-score tree) 
(if (terminal-tree-p tree) 
score 

Add up the constituent's scores, 
;; along with the tree's score 
(+ (sum (tree-rhs tree) #'tree-score-or-0) 

(if (numberp score) 
score 

(or (apply score (tree-rhs tree)) 0))))))) 

Here is an accessor function to pick out the score from a tree: 


<a id='page-673'></a>

(defun tree-score-or-O (tree) 

(if (numberp (tree-score tree)) 
(tree-score tree) 
0)) 

Here is the updated grammar. First, I couldn't resist the chance to add more features 
to the grammar. I added the postnominal adjectives "shuffled," which randomly 
permutes the list of songs, and "reversed," which reverses the order of play. I also 
added the operator "repeat," as in "1 to 3 repeat 5," which repeats a list a certain 
number of times. 1 also added brackets to allow input that says explicitly how it 
should be parsed. 

(use 

'((NP -> (NP CONJ NP) infix-funcall infix-scorer) 
(NP -> (N . .) infix-funcall infix-scorer) 
(NP -> (.) list) 
(NP ([ NP ]) arg2) 
(NP (NP ADJ) rev-funcal1 rev-scorer) 
(NP -> (NP OP N) infix-funcall) 
(N -> (D) identity) 
(N (N D) 10*N+D) 
(P -> to integers prefer<) 

([ -> [ [) 
(] -> ] ]) 
(OP -> repeat repeat) 
(CONJ -> and append prefer-disjoint) 
(CONJ -> without set-difference prefer-subset) 
(ADJ -> reversed reverse inv-span) 
(ADJ -> shuffled permute prefer-not-singleton) 
(D -> 1 1) (D -> 2 2) (D -> 3 3) (D -> 4 4) (D -> 5 5) 
(D -> 6 6) (D -> 7 7) (D -> 8 8) (D -> 9 9) (D -> 0 0))) 

The following scoring functions take trees as inputs and compute bonuses or penalties 
for those trees. The scoring function pref er<, used for the word "to," gives a 
one-point penalty for reversed ranges: "5 to 1" gets a score of -1, while "1 to 5" gets 
a score of 0. The scorer for "and," prefer-di s joi nt, gives a one-point penalty for 
intersecting lists: "1 to 3 and 7 to 9" gets a score of 0, while "1 to 4 and 2 to 5" gets -1. 
The "x without y" scorer, prefer-subset, gives a three-point penalty when the y list 
has elements that aren't in the . list. It also awards points in inverse proportion to the 
length (in words) of the . phrase. The idea is that we should prefer to bind "without" 
tightly to some small expression on the left. If the final scores come out as positive 
or as nonintegers, then this scoring component is responsible, since all the other 
components are negative intgers. The "x shuffled" scorer, prefer-not-singleton, 
is similar, except that there the penalty is for shuffling a list of less than two songs. 


<a id='page-674'></a>

(defun prefer< (x y) 
(if (>= (sem X) (sem y)) -1)) 

(defun prefer-disjoint (x y) 
(if (intersection (sem x) (sem y)) -1)) 

(defun prefer-subset (x y) 
(+ (inv-span x) (if (subsetp (sem y) (sem x)) 0 -3))) 

(defun prefer-not-singleton (x) 
(+ (inv-span x) (if (< (length (sem x)) 2) -4 0))) 

The inf i x-scorer and rev-scorer functionsdon'taddanythingnew,theyjustassure 
that the previously mentioned scoring functions will get applied in the right place. 

(defun infix-scorer (argl scorer arg2) 
(funcall (tree-score scorer) argl arg2)) 

(defun rev-scorer (arg scorer) (funcall (tree-score scorer) arg)) 

Here are the functions mentioned in the grammar, along with some useful utilities: 

(defun arg2 (al a2 &rest a-n) (declare (ignore al a-n)) a2) 

(defun rev-funcall (arg function) (funcall function arg)) 

(defun repeat (list n) 
"Append list . times." 
(if (= . 0) 

nil 
(append list (repeat list (- . 1))))) 

(defun span-length (tree) 
"How many words are in tree?" 
(if (terminal-tree-p tree) 1 

(sum (tree-rhs tree) #'span-length))) 

(defun inv-span (tree) (/ 1 (span-length tree))) 

(defun sem (tree) (tree-sem tree)) 

(defun integers (start end) 
"A list of all the integers in the range [start...end]inclusive. 
This version allows start > end." 
(cond ((< start end) (cons start (integers (+ start 1) end))) 

((> start end) (cons start (integers (- start 1) end))) 
(t (list start)))) 

(defun sum (numbers &optional fn) 
"Sum the numbers, or sum (mapcar fn numbers)." 
(if fn 

(loop for X in numbers sum (funcall fn x)) 
(loop for X in numbers sum x))) 


<a id='page-675'></a>

(defun permute (bag) 
"Return a random permutation of the given input list. " 
(if (null bag) 

nil 
(let ((e (random-elt bag))) 
(cons e (permute (remove e bag rcount 1 :test #*eq)))))) 

We will need a way to show off the preference rankings: 

(defun all-parses (words) 
(format t "~%Score Semantics^ZBT^a" words) 
(format t "~% = --251 -~%") 
(loop for tree in (sort (parser words) #*> :key#'tree-score) 

do (format t "~5,lf ~9a~25T''a~%" (tree-score tree) (tree-sem tree) 
(bracketing tree))) 
(values)) 

(defun bracketing (tree) 
"Extract the terminals, bracketed with parens." 
(cond ((atom tree) tree) 

((length=1 (tree-rhs tree)) 
(bracketing (first (tree-rhs tree)))) 
(t (mapcar #'bracketing (tree-rhs tree))))) 

Now we can try some examples: 

> (all-parses '(1 to 6 without 3 and 4)) 
Score Semantics (1 TO 6 WITHOUT 3 AND 4) 

0.3 (1 2 5 6) ((1 TO 6) WITHOUT (3 AND 4)) 
-0.7 (1 2 4 5 6 4) (((1 TO 6) WITHOUT 3) AND 4) 
> (all -parses '(1 and 3 to 7 and 9 without 5 and 6)) 
Score Semantics (1 AND 3 TO 7 AND 9 WITHOUT 5 AND 6) 

0.2 (1 3 4 7 9) (1 AND (((3 TO 7) AND 9) WITHOUT (5 AND 6))) 
0.1 (1 3 4 7 9) (((1 AND (3 TO 7)) AND 9) WITHOUT (5 AND 6)) 
0.1 (1 3 4 7 9) ((1 AND ((3 TO 7) AND 9)) WITHOUT (5 AND 6)) 
-0.8 (1 3 4 6 7 9 6) ((1 AND (((3 TO 7) AND 9) WITHOUT 5)) AND 6) 
-0.8 (1 3 4 6 7 9 6) (1 AND ((((3 TO 7) AND 9) WITHOUT 5) AND 6)) 
-0.9 (1 3 4 6 7 9 6) ((((1 AND (3 TO 7)) AND 9) WITHOUT 5) AND 6) 
-0.9 (1 3 4 6 7 9 6) (((1 AND ((3 TO 7) AND 9)) WITHOUT 5) AND 6) 
-2.0 (1 3 4 5 6 7 9) ((1 AND (3 TO 7)) AND (9 WITHOUT (5 AND 6))) 
-2.0 (1 3 4 5 6 7 9) (1 AND ((3 TO 7) AND (9 WITHOUT (5 AND 6)))) 
-3.0 (1 3 4 5 6 7 9 6) (((1 AND (3 TO 7)) AND (9 WITHOUT 5)) AND 6) 
-3.0 (1 3 4 5 6 7 9 6) ((1 AND (3 TO 7)) AND ((9 WITHOUT 5) AND 6)) 
-3.0 (1 3 4 5 6 7 9 6) ((1 AND ((3 TO 7) AND (9 WITHOUT 5))) AND 6) 

<a id='page-676'></a>

-3.0 (1 3 4 5 6 7 9 6) (1 AND (((3 TO 7) AND (9 WITHOUT 5)) AND 6)) 
-3.0 (13 4 5 6 7 9 6) (1 AND ((3 TO 7) AND ((9 WITHOUT 5) AND 6))) 

> (all -parses '(1 and 3 :o 7 and 9 without 5 and 2)) 
Score Semantics (1 AND 3 TO 7 AND 9 WITHOUT 5 AND 2) 

0.2 (1 3 4 6 7 9 2) ((1 AND (((3 TO 7) AND 9) WITHOUT 5)) AND 2) 
0.2 (1 3 4 6 7 9 2) (1 AND ((((3 TO 7) AND 9) WITHOUT 5) AND 2)) 
0.1 (1 3 4 6 7 9 2) ((((1 AND (3 TO 7)) AND 9) WITHOUT 5) AND 2) 
0.1 (1 3 4 6 7 9 2) (((1 AND ((3 TO 7) AND 9)) WITHOUT 5) AND 2) 
-2.0 (1 3 4 5 6 7 9 2) (((1 AND (3 TO 7)) AND (9 WITHOUT 5)) AND 2) 
-2.0 (1 3 4 5 6 7 9 2) ((1 AND (3 TO 7)) AND ((9 WITHOUT 5) AND 2)) 
-2.0 (1 3 4 5 6 7 9) ((1 AND (3 TO 7)) AND (9 WITHOUT (5 AND 2))) 
-2.0 (1 3 4 5 6 7 9 2) ((1 AND ((3 TO 7) AND (9 WITHOUT 5))) AND 2) 
-2.0 (1 3 4 5 6 7 9 2) (1 AND (((3 TO 7) AND (9 WITHOUT 5)) AND 2)) 
-2.0 (1 3 4 5 6 7 9 2) (1 AND ((3 TO 7) AND ((9 WITHOUT 5) AND 2))) 
-2.0 (1 3 4 5 6 7 9) (1 AND ((3 TO 7) AND (9 WITHOUT (5 AND 2)))) 
-2.8 (1 3 4 6 7 9) (1 AND (((3 TO 7) AND 9) WITHOUT (5 AND 2))) 
-2.9 (1 3 4 6 7 9) (((1 AND (3 TO 7)) AND 9) WITHOUT (5 AND 2)) 
-2.9 (1 3 4 6 7 9) ((1 AND ((3 TO 7) AND 9)) WITHOUT (5 AND 2)) 
In each case, the preference rules are able to assign higher scores to more reasonable 
interpretations. It turns out that, in each case, all the interpretations with positive 
scores represent the same set of numbers, while interpretations with negative scores 
seem worse. Seeing all the scores in gory detail may be of academic interest, but what 
we really want is something to pick out the best interpretation. The following code 
is appropriate for many situations. It picks the top scorer, if there is a unique one, 
or queries the user if several interpretations tie for the best score, and it complains 
if there are no valid parses at all. The query-user function may be useful in many 
applications, but note that meani ng uses it only as a default; a program that had some 
automatic way of deciding could supply another ti e-breaker function to meani ng. 

(defun meaning (words &optional (tie-breaker #'query-user)) 
"Choose the single top-ranking meaning for the words." 
(let* ((trees (sort (parser words) #*> :key #'tree-score)) 

(best-score (if trees (tree-score (first trees)) 0)) 
(best-trees (delete best-score trees 
:key #*tree-score :test-not #'eql)) 
(best-sems (delete-duplicates (mapcar #'tree-sem best-trees) 
.-test #'equal))) 

(case (length best-sems) 
(0 (format t "~&Sorry. I didn't understand that.") nil) 
(1 (first best-sems)) 
(t (funcall tie-breaker best-sems))))) 


<a id='page-677'></a>

(defun query-user (choices &optiona1 
(header-str "~&Please pick one:") 
(footer-str "~&Your choice? ")) 

"Ask user to make a choice." 
(format *query-io* header-str) 
(loop for choice in choices for i from 1 do 

(format *query-io* "~&~3d: ~a" i choice)) 
(format *query-io* footer-str) 
(nth (- (read) 1) choices)) 

Here we see some final examples: 

> (meaning '(1 to 5 without 3 and 4)) 
(1 2 5) 

> (meaning '(1 to 5 without 3 and 6)) 
(12 4 5 6) 

> (meaning '(1 to 5 without 3 and 6 shuffled)) 
(64125) 

> (meaning '([ 1 to 5 without C 3 and 6 ] ] reversed)) 
(5 4 2 1) 

> (meaning '(1 to 5 to 9)) 

Sorry. I didn't understand that. 
NIL 

> (meaning '(1 to 5 without 3 and 7 repeat 2)) 
Please pick one: 

1: (12 4 5 7 12 4 5 7) 
2: (12 4 5 7 7) 
Your choice? 1 
(12 4 5 7 12 4 5 7) 

> (all-parses '(1 to 5 without 3 and 7 repeat 2)) 
Score Semantics (1 TO 5 WITHOUT 3 AND 7 REPEAT 2) 

0.3 (12 4 5 7 12 4 5 7) ((((1 TO 5) WITHOUT 3) AND 7) REPEAT 2) 
0.3 (12 4 5 7 7) (((1 TO 5) WITHOUT 3) AND (7 REPEAT 2)) 
-2.7 (12 4 5 12 4 5) (((1 TO 5) WITHOUT (3 AND 7)) REPEAT 2) 
-2.7 (12 4 5) ((1 TO 5) WITHOUT ((3 AND 7) REPEAT 2)) 
-2.7 (12 4 5) ((1 TO 5) WITHOUT (3 AND (7 REPEAT 2))) 
This last example points out a potential problem: I wasn't sure what was a good 
scoring function for "repeat," so I left it blank, it defaulted to 0, and we end up 
with two parses with the same score. This example suggests that "repeat" should 
probably involve inv-span like the other modifiers, but perhaps other factors should 
be involved as well. There can be a complicated interplay between phrases, and it 


<a id='page-678'></a>

is not always clear where to assign the score. For example, it doesn't make much 
sense to repeat a "without" phrase; that is, the bracketing (. without (y repeat 
.)) is probably a bad one. But the scorer for "without" nearly handles that already. 
It assigns a penalty if its right argument is not a subset of its left. Unfortunately, 
repeated elements are not counted in sets, so for example, the list (1 2 3 1 2 3) is a 
subset of (1 2 3 4). However, we could change the scorer for "without" to test for 
sub-bag-. (not a built-in Common Lisp function) instead, and then "repeat" would 
not have to be concerned with that case. 

19.7 The Problem with Context-Free 
Phrase-Structure Rules 
The fragment of English grammar we specified in section 19.2 admits a variety of 
ungrammatical phrases. For example, it is equally happy with both "I liked her" and 
"me liked she." Only the first of these should be accepted; the second should be 
ruled out. Similarly, our grammar does not state that verbs have to agree with their 
subjects in person and number. And, since the grammar has no notion of meaning, 
it will accept sentences that are semantically anomalous (or at least unusual), such 
as "the table liked the man." 

There are also some technical problems with context-free grammars. For example, 
it can be shown that no context-free grammar can be written to account for the 
language consisting of just the strings ABC, AABBCC, AAABBBCCC, and so forth, 
where each string has an equal number of As, Bs, and Cs. Yet sentences roughly of 
that form show up (admittedly rarely) in natural languages. An example is "Robin 
and Sandy loved and hated Pat and Kim, respectively." While there is still disagreement 
over whether it is possible to generate natural languages with a context-free 
grammar, clearly it is much easier to use a more powerful grammatical formalism. 
For example, consider solving the subject-predicate agreement problem. It is possible 
to do this with a context-free language including categories like singular-NP, 
plural-NP, singular-VP, and plural-VP, but it is far easier to augment the grammatical 
formahsm to allow passing features between constituents. 

It should be noted that context-free phrase-structure rules turned out to be very 
useful for describing programming languages. Starting with Algol 60, the formalism 
has been used under the name Bflcfcus-Nflwr Form (BNF) by computer scientists. In this 
book we are more interested in natural languages, so in the next chapter we will see a 
more powerful formalism known as unification grammar that can handle the problem 
of agreement, as well as other difficulties. Furthermore, unification grammars allow a 
natural way of attaching semantics to a parse. 


<a id='page-679'></a>

19.8 History and References 
There is a class of parsing algorithms known as chart parsers that explicitly cache 
partial parses and reuse them in constructing larger parses. Barley's algorithm (1970) 
is the first example, and Martin Kay (1980) gives a good overview of the field and 
introduces a data structure, the chart, for storing substrings of a parse. Winograd 
(1983) gives a complex (five-page) specification of a chart parser. None of these 
authors have noticed that one can achieve the same results by augmenting a simple 
(one-page) parser with memoization. In fact, it is possible to write a top-down parser 
that is even more succinct. (See exercise 19.3 below.) 

For a general overview of natural language processing, my preferences (in order) 
are Allen 1987, Winograd 1983 or Gazdar and Mellish 1989. 

19.9 Exercises 
&#9635; Exercise 19.2 [m-h] Experiment with the grammar and the parser. Find sentences 
it cannot parse correctly, and try to add new syntactic rules to account for them. 

&#9635; Exercise 19.3 [m-h] The parser works in a bottom-up fashion. Write a top-down 
parser, and compare it to the bottom-up version. Can both parsers work with the 
same grammar? If not, what constraints on the grammar does each parsing strategy 
impose? 

&#9635; Exercise 19.4 [h] Imagine an interface to a dual cassette deck. Whereas the CD 
player had one assumed verb, "play," this unit has three explicit verb forms: "record," 
"play," and "erase." There should also be modifiers "from" and "to," where the object 
of a "to" is either 1 or 2, indicating which cassette to use, and the object of a "from" 
is either 1 or 2, or one of the symbols PHONO, CD, or AUX. It's up to you to design 
the grammar, but you should allow input something like the following, where I have 
chosen to generate actual Lisp code as the meaning: 

> (meaning '(play 1 to 5 from CD shuffled and 
record 1 to 5 from CD and 1 and 3 and 7 from 1)) 

(PROGN (PLAY '(15 2 3 4) :FROM 'CD) 
(RECORD '(12345) :FROM 'CD) 
(RECORD '(1 3 7) :FROM .)) 

This assumes that the functions play and record take keyword arguments (with 
defaults) for : from and : to. You could also extend the grammar to accommodate an 
automatic timer, with phrases like "at 3:00." 


<a id='page-680'></a>

&#9635; Exercise 19.5 [m] In the definition of permute, repeated here, why is the :test 
#'eq needed? 

(defun permute (bag) 
"Return a random permutation of the given input list. " 
(if (null bag) 

nil 
(let ((e (random-elt bag))) 
(cons e (permute (remove e bag :count 1 :test #'eq)))))) 

&#9635; Exercise 19.6 [m] The definition of permute takes 0{n^). Replace it by an 0 (n) 
algorithm. 

19.10 Answers 
Answer 19.1 

(defun parser (words) 
"Return all complete parses of a list of words." 
(let* ((table (make-array (+ (length words) 1) :initial-element 0)) 

(parses (parse words (length words) table))) 
(mapcar #'parse-tree (complete-parses parses)))) 

(defun parse (words num-words table) 
"Bottom-up parse, returning all parses of any prefix of words." 
(unless (null words) 

(let ((ans (aref table num-words))) 

(if (not (eq ans 0)) 
ans 
(setf (aref table num-words) 

(mapcan #*(lambda (rule) 

(extend-parse (rule-lhs rule) 
(list (firstwords)) 
(rest words) nil 
(- num-words 1) table)) 

(lexical-rules (first words)))))))) 


<a id='page-681'></a>

(defun extend-parse (Ihs rhs rem needed num-words table) 
"Look for the categories needed to complete the parse." 
(if (null needed) 

If nothing is needed, return this parse and upward extensions 
(let ((parse (make-parse :tree (new-tree Ihs rhs) :rem rem))) 
(cons parse 
(mapcan 
#*(lambda (rule) 

(extend-parse (rule-lhs rule) 
(list (parse-tree parse)) 
rem (rest (rule-rhs rule)) 
num-words table)) 

(rules-starting-with Ihs)))) 
otherwise try to extend rightward 
(mapcan 
#'(lambda (p) 
(if (eq (parse-lhs p) (first needed)) 

(extend-parse Ihs (appendl rhs (parse-tree p)) 
(parse-rem p) (rest needed) 
(length (parse-rem p)) table))) 

(parse rem num-words table)))) 

It turns out that, for the Lisp system used in the timings above, this version is no 
faster than normal memoization. 

Answer 19.3 Actually, the top-down parser is a little easier (shorter) than the 

bottom-up version. The problem is that the most straightforward way of imple


menting a top-down parser does not handle so-called left recursive rules - rules of the 

form(X -> (X ...)). This includes rules we've used, like (NP -> (NP and NP)). 

The problem is that the parser will postulate an NP, and then postulate that it is of 

the form (NP and NP), and that the first NP of that expression is of the form (NP and 

NP), and so on. An infinite structure of NPs is explored before even the first word is 

considered. 

Bottom-up parsers are stymied by rules with null right-hand sides: (X -> ()) . 
Note that I was careful to exclude such rules in my grammars earlier. 

(defun parser (words &optional (cat *S)) 
"Parse a list of words; return only parses with no remainder." 
(mapcar #*parse-tree (complete-parses (parse words cat)))) 

(defun parse (tokens start-symbol) 
"Parse a list of tokens, return parse trees and remainders." 
(if (eq (first tokens) start-symbol) 

(list (make-parse rtree (first tokens) :rem (rest tokens))) 
(mapcan #*(lambda (rule) 
(extend-parse (Ihs rule) nil tokens (rhs rule))) 
(rules-for start-symbol)))) 


<a id='page-682'></a>

(defun extend-parse (Ihs rhs rem needed) 
"Parse the remaining needed symbols." 
(if (null needed) 

(list (make-parse :tree (cons Ihs rhs) :rem rem)) 
(mapcan 
#'(lambda (p) 
(extend-parse Ihs (append rhs (list (parse-tree p))) 
(parse-rem p) (rest needed))) 
(parse rem (first needed))))) 

(defun rules-for (cat) 
"Return all the rules with category on Ihs" 
(find-all cat ^grammar* :key #'rule-lhs)) 

Answer 19.5 If it were omitted, then : tes t would default to #'eql, and it would be 
possible to remove the "wrong" element from the list. Consider the list (1.0 1.0) in 
an implementation where floating-point numbers are eql but not eq. if random-el t 
chooses the first 1.0 first, then everything is satisfactory - the result Ust is the same 
as the input list. However, if random-el t chooses the second 1.0, then the second 

1.0will be the first element of the answer, but remove will remove the wrong 1.0! It 
will remove the first 1.0, and the final answer will be a list with two pointers to the 
second 1.0 and none to the first. In other words, we could have: 
> (member (first x) (permute x) .-test #'eq) 
NIL 

Answer 19.6 

(defun permute (bag) 

"Return a random permutation of the bag." 
It is done by converting the bag to a vector, but the 
result is always the same type as the input bag. 

(let ((bag-copy (replace (make-array (length bag)) bag)) 
(bag-type (if (listp bag) 'list (type-of bag)))) 
(coerce (permute-vector! bag-copy) bag-type))) 

(defun permute-vector! (vector) 
"Destructively permute (shuffle) the vector." 
(loop for i from (length vector) downto 2 do 

(rotatef (aref vector (-i D) 
(aref vector (random i)))) 
vector) 

The answer uses rotatef, a relative of setf that swaps 2 or more values. That is, 
(rotatef a b) is like: 


<a id='page-683'></a>
(let ((temp a)) 
(setf a b) 
(setf b temp) 
nil) 
Rarely, rotatef is used with more than two arguments, (rotatef a b c) is like: 
(let ((temp a)) 
(setf a b) 
(setf b c) 
(setf c temp) 
nil) 


## Chapter 20
<a id='page-684'></a>

Unification Grammars 

P
P
rolog was invented because Alain Colmerauer wanted a formalism to describe the grammar 
of French. His intuition was that the combination of Horn clauses and unification 
resulted in a language that was just powerful enough to express the kinds of constraints 
that show up in natural languages, while not as powerful as, for example, full predicate calculus. 
This lack of power is important, because it enables efficient implementation of Prolog, and 
hence of the language-analysis programs built on top of it. 

Of course, Prolog has evolved and is now used for many applications besides natural language, 
but Colmerauer's underlying intuition remains a good one. This chapter shows how 
to view a grammar as a set of logic programming clauses. The clauses define what is a legal 
sentence and what isn't, without any explicit reference to the process of parsing or generation. 
The amazing thing is that the clauses can be defined in a way that leads to a very efficient 
parser. Furthermore, the same grammar can be used for both parsing and generation (at least 
in some cases). 


<a id='page-685'></a>
20. 1 Parsing as Deduction 
Here's how we could express the grammar rule "A sentence can be composed of a 
noun phrase followed by a verb phrase" in Prolog: 

(<- (S ?s) 
(NP ?np) 
(VP ?vp) 
(concat ?np ?vp ?s)) 

The variables represent strings of words. As usual, they will be implemented as lists 
of symbols. The rule says that a given string of words ? s is a sentence if there is a string 
that is noun phrase and one that is a verb phrase, and if they can be concatenated to 
form ?s. Logically, this is fine, and it would work as a program to generate random 
sentences. However, it is a very inefficient program for parsing sentences. It will 
consider all possible noun phrases and verb phrases, without regard to the input 
words. Only when it gets to the concat goal (defined on [page 411](chapter12.md#page-411)) will it test to see if 
the two constituents can be concatenated together to make up the input string. Thus, 
a better order of evaluation for parsing is: 

(<- (S ?s) 
(concat ?np ?vp ?s) 
(NP ?np) 
(VP ?vp)) 

The first version had NP and VP guessing strings to be verified by concat. In most 
grammars, there will be a very large or infinite number of NPs and VPs. This second 
version has concat guessing strings to be verified by NP and VP. If there are . words 
in the sentence, then concat can only make . -h 1 guesses, quite an improvement. 
However, it would be better still if we could in effect have concat and .. work together 
to make a more constrained guess, which would then be verified by VP. 

We have seen this type of problem before. In Lisp, the answer is to return multiple 
values. NP would be a function that takes a string as input and returns two values: 
an indication of success or failure, and a remainder string of words that have not yet 
been parsed. When the first value indicates success, then VP would be called with 
the remaining string as input. In Prolog, return values are just extra arguments. So 
each predicate will have two parameters: an input string and a remainder string. 
Following the usual Prolog convention, the output parameter comes after the input. 
In this approach, no calls to concat are necessary, no wild guesses are made, and 
Prolog's backtracking takes care of the necessary guessing: 


<a id='page-686'></a>

(<- (S ?sO ?s2) 
(NP ?sO ?sl) 
(VP ?sl ?s2)) 

This rule can be read as "The string from s0 to s2 is a sentence if there is an si such 
that the string from sq to si is a noun phrase and the string from 5i to S2 is a verb 
phrase." 

A sample query would be (? - (S (The boy ate the apple) ())). With 
suitable definitions of . . and VP, this would succeed, with the following bindings 
holding within S: 

?sO = (The boy ate the apple) 

?sl = (ate the apple) 

?s2 = () 

Another way of reading the goal (NP ?sO ?sl), for example, is as "IS the Hst ?sO 
minus the Ust ?sl a noun phrase?" In this case, ?sO minus ?sl is the Ust (The boy). 
The combination of two arguments, an input list and an output list, is often called a 
difference list, to emphasize this interpretation. More generally, the combination of an 
input parameter and output parameter is caUed an accumulator. Accumulators, particularly 
difference lists, are an important technique throughout logic programming 
and are also used in functional programming, as we saw on [page 63](chapter3.md#page-63). 

In our rule for S, the concatenation of difference lists was implicit. If we prefer, 
we could define a version of concat for difference lists and call it explicitly: 

(<- (S ?s-in ?s-rem) 
(NP ?np-in ?np-rem) 
(VP ?vp-in ?vp-rem) 

(concat ?np-in ?np-rem ?vp-in ?vp-rem ?s-in ?s-rem)) 

(<- (concat ?a ?b ?b ?c ?a ?c)) 

Because this version of concat has a different arity than the old version, they can 
safely coexist. It states the difference list equation {a -b) -\- {b -c) = {a - c). 

In the last chapter we stated that context-free phrase-structure grammar is inconvenient 
for expressing things like agreement between the subject and predicate of a 
sentence. With the Horn-clause-based grammar formalism we are developing here, 
we can add an argument to the predicates NP and VP to represent agreement. In 
English, the agreement rule does not have a big impact. For all verbs except be, the 
difference only shows up in the third-person singular of the present tense: 


<a id='page-687'></a>
Singular Plural 

first person I sleep we sleep 

second person you sleep you sleep 

third person he/she sleeps they sleep 

Thus, the agreement argument will take on one of the two values 3sg or ~3sg to 
indicate third-person-singular or not-third-person-singular. We could write: 

(<- (S ?sO ?s2) 
(NP ?agr ?sO ?sl) 
(VP ?agr ?sl ?s2)) 

(<- (NP 3sg (he . ?s) ?s)) 
(<- (NP ~3sg (they . ?s) ?$)) 

(<- (VP 3sg (sleeps . ?s) ?s)) 
(<- (VP ~3sg (sleep . Is) Is)) 

This grammar parses just the right sentences: 

> (?- (S (He sleeps) ())) 
Yes. 

> (?- (S (He sleep) ())) 
No. 

Let's extend the grammar to allow common nouns as well as pronouns: 

(<- (NP ?agr ?sO ?s2) 
(Det ?agr ?sO ?sl) 
(N ?agr ?sl ?s2)) 

(<- (Det ?any (the . ?s) ?s)) 
(<- (N 3sg (boy . Is) Is)) 
(<- (N 3sg (girl . ?s) ?s)) 

The same grammar rules can be used to generate sentences as well as parse. Here 
are all possible sentences in this trivial grammar: 

> (?- (S ?words ())) 
7W0RDS = (HE SLEEPS); 
7W0RDS = (THEY SLEEP); 
?WORDS = (THE BOY SLEEPS); 
7W0RDS = (THE GIRL SLEEPS); 
No. 

So far all we have is a recognizer: a predicate that can separate sentences from 


<a id='page-688'></a>

nonsentences. But we can add another argument to each predicate to build up the 
semantics. The result is not just a recognizer but a true parser: 

(<- (S (?pred ?subj) ?sO ?s2) 

(NP ?agr ?subj ?sO ?sl) 

(VP ?agr ?pred ?sl ?s2)) 

(<- (NP 3sg (the male) (he . ?s) ?s)) 
(<- (NP ~3sg (some objects) (they . ?s) ?s)) 

(<- (NP ?agr (?det ?n) ?sO ?s2) 

(Det ?agr ?det ?sO ?sl) 

(N ?agr ?n ?sl ?s2)) 

(<- (VP 3sg sleep (sleeps . ?s) ?s)) 
(<- (VP ~3sg sleep (sleep . ?s) ?s)) 

(<- (Det ?any the (the . ?s) ?s)) 
(<- (N 3sg (young male human) (boy . ?s) ?s)) 
(<- (N 3sg (young female human) (girl . ?s) ?s)) 

The semantic translations of individual words is a bit capricious. In fact, it is not too 
important at this point if the translation of boy is (young mal e human) or just boy. 
There are two properties of a semantic representation that are important. First, it 
should be unambiguous. The representation of orange the fruit should be different 
from orange the color (although the representation of the fruit might well refer to 
the color, or vice versa). Second, it should express generalities, or allow them to 
be expressed elsewhere. So either sleep and sleeps should have the same or similar 
representation, or there should be an inference rule relating them. Similarly, if the 
representation of boy does not say so explicitly, there should be some other rule 
saying that a boy is a male and a human. 

Once the semantics of individual words is decided, the semantics of higher-level 
categories (sentences and noun phrases) is easy. In this grammar, the semantics of 
a sentence is the application of the predicate (the verb phrase) to the subject (the 
noun phrase). The semantics of a compound noun phrase is the application of the 
determiner to the noun. 

This grammar returns the semantic interpretation but does not build a syntactic 
tree. The syntactic structure is implicit in the sequence of goals: S calls NP and VP, 
and . . can call Det and N. If we want to make this explicit, we can provide yet another 
argument to each nonterminal: 

(<- (S (?pred ?subj) (s ?np ?vp)?sO ?s2) 
(NP ?agr ?subj ?np ?sO ?sl) 
(VP ?agr ?pred ?vp ?sl ?s2)) 

(<- (NP 3sg (the male) (np he) (he . Is) ?s)) 
(<- (NP ~3sg (some objects) (np they) (they . ?s) ?s)) 


<a id='page-689'></a>
(<- (NP ?agr (?det ?n) (np ?det-syn ?n-syn) ?sO ?s2) 
(Det ?agr ?det ?det-syn ?sO ?sl) 
(N ?agr ?n ?n-syn ?sl ?s2)) 

(<- (VP 3sg sleep (vp sleeps) (sleeps . ?s) ?s)) 
(<- (VP ""Ssg sleep (vp sleep) (sleep . ?s) ?s)) 

(<- (Det ?any the (det the) (the . ?s) ?s)) 
(<- (N 3sg (young male human) (n boy) (boy . ?s) ?s)) 
(<- (N 3sg (young female human) (n girl) (girl . ?s) ?s)) 

This grammar can still be used to parse or generate sentences, or even to enumerate 
all syntax/semantics/sentence triplets: 

Parsing: 
> (?- (S ?sem ?syn (He sleeps) ())) 
?SEM = (SLEEP (THE MALE)) 
?SYN = (S (NP HE) (VP SLEEPS)). 

Generating: 
> (?- (S (sleep (the male)) ? ?words ())) 
7W0RDS = (HE SLEEPS) 

Enumerating: 
> (?- (S ?sem ?syn ?words ())) 
?SEM = (SLEEP (THE MALE)) 
?SYN = (S (NP HE) (VP SLEEPS)) 
?WORDS = (HE SLEEPS); 

?SEM = (SLEEP (SOME OBJECTS)) 

?SYN = (S (NP THEY) (VP SLEEP)) 

7W0RDS = (THEY SLEEP); 

?SEM = (SLEEP (THE (YOUNG MALE HUMAN))) 
?SYN = (S (NP (DET THE) (N BOY)) (VP SLEEPS)) 
7W0RDS = (THE BOY SLEEPS); 

?SEM = (SLEEP (THE (YOUNG FEMALE HUMAN))) 
?SYN = (S (NP (DET THE) (N GIRD) (VP SLEEPS)) 
7W0RDS = (THE GIRL SLEEPS); 

No. 

20.2 Definite Clause Grammars 
We now have a powerful and efficient tool for parsing sentences. However, it is 
getting to be a very messy tool - there are too many arguments to each goal, and it 


<a id='page-690'></a>

is hard to tell which arguments represent syntax, which represent semantics, which 
represent in/out strings, and which represent other features, like agreement. So, 
we will take the usual step when our bare programming language becomes messy: 
define a new language. 

Edinburgh Prolog recognizes assertions called definite clause grammar (DCG) rules. 
The term definite clause is just another name for a Prolog clause, so DCGs are also 
called "logic grammars." They could have been called "Horn clause grammars" or 
"Prologgrammars" as well. 

DCG rules are clauses whose main functor is an arrow, usually written - ->. They 
compile into regular Prolog clauses with extra arguments. In normal DCG rules, only 
the string arguments are automatically added. But we will see later how this can be 
extended to add other arguments automatically as well. 

We will implement DCG rules with the macro rule and an infix arrow. Thus, we 
want the expression: 

(rule (S) --> (NP) (VP)) 

to expand into the clause: 

(<- (S ?sO ?s2) 

(NP ?sO ?sl) 

(VP ?sl ?s2)) 

While we're at it, we may as well give rul e the ability to deal with different types of 
rules, each one represented by a different type of arrow. Here's the rul e macro: 

(defmacro rule (head &optional (arrow *:-) &body body) 
"Expand one of several types of logic rules into pure Prolog." 
This is data-driven, dispatching on the arrow 
(funcall (get arrow 'rule-function) head body)) 

As an example of a rule function, the arrow: - will be used to represent normal Prolog 
clauses. That is, the form (rul e head : -body) will be equivalent to (<-head body). 

(setf (get *:- 'rule-function) 
#'(lambda (head body) .(<- .head .,body))) 

Before writing the rule function for DCG rules, there are two further features of the 
DCG formalism to consider. First, some goals in the body of a rule may be normal 
Prolog goals, and thus do not require the extra pair of arguments. In Edinburgh 
Prolog, such goals are surrounded in braces. One would write: 


<a id='page-691'></a>
s(Sem) --> np(Subj), vp(Pred). 
{combi ne(Subj,Pred. Sem)}. 

where the idea is that combi ne is riot a grammatical constituent, but rather a Prolog 
predicate that could do some calculations on Subj and Pred to arrive at the proper 
semantics, Sem. We will mark such a test predicate not by brackets but by a list 
headed by the keyword : test, as in: 

(rule (S ?sem) --> (NP ?subj) (VP ?pred) 
(:test (combine ?subj ?pred ?sem))) 

Second, we need some way of introducing individual words on the right-hand side, 
as opposed to categories of words. In Prolog, brackets are used to represent a word 
or Ust of words on the right-hand side: 

verb --> [sleeps]. 

We will use a list headed by the keyword : word: 

(rule (NP (the male) 3sg) --> (:word he)) 
(rule (VP sleeps 3sg) --> (:word sleeps)) 

The following predicates test for these two special cases. Note that the cut is also 
allowed as a normal goal. 

(defun dcg-normal-goal-p (x) (or (starts-with . :test) (eq . '!))) 
(defun dcg-word-list-p (x) (starts-with . 'iword)) 

At last we are in a position to present the rule function for DCG rules. The function 
make-deg inserts variables to keep track of the strings that are being parsed. 

(setf (get '--> 'rule-function) 'make-dcg) 

(defun make-dcg (head body) 
(let ((n (count-if (complement #'dcg-normal-goal-p) body))) 
.(<- (,@head ?sO .(symbol *?s n)) 
.,(make-dcg-body body 0)))) 


<a id='page-692'></a>

(defun make-dcg-body (body n) 
"Make the body of a Definite Clause Grammar (DCG) clause. 
Add ?string-in and -out variables to each constituent. 
Goals like (:test goal) are ordinary Prolog goals, 
and goals like (:word hello) are literal words to be parsed." 
(if (null body) 

nil 
(let ((goal (first body))) 

(cond 
((eq goal '!) (cons . (make-dcg-body (rest body) n))) 
((dcg-normal-goal-p goal) 

(append (rest goal) 
(make-dcg-body (rest body) n))) 
((dcg-word-list-p goal) 
(cons 
'(= .(symbol 'Is n) 
(.(rest goal) ..(symbol '?s (+ . 1)))) 
(make-dcg-body (rest body) (+ . 1)))) 
(t (cons 
(append goal 
(list (symbol '?s n) 
(symbol '?s (+ . 1)))) 
(make-dcg-body (rest body) (+ . 1)))))))) 

&#9635; Exercise 20.1 [m] make - dcg violates one of the cardinal rules of macros. What does 
it do wrong? How would you fix it? 

20.3 A Simple Grammar in DCG Format 
Here is the trivial grammar from [page 688](chapter20.md#page-688) in DCG format. 

(rule (S (?pred ?subj)) --> 
(NP ?agr ?subj) 
(VP ?agr ?pred)) 

(rule (NP ?agr (?det ?n)) --> 
(Det ?agr ?det) 
(N ?agr ?n)) 


<a id='page-693'></a>

(rule (NP 3sg (the male)) --> (:word he)) 

(rule (NP ~3sg (some objects)) --> (:word they)) 

(rule (VP 3sg sleep) --> (:word sleeps)) 

(rule (VP ~3sg sleep) --> (:word sleep)) 

(rule (Det ?any the) --> (:word the)) 

(rule (N 3sg (young male human)) --> (:word boy)) 

(rule (N 3sg (young female human)) --> (:word girl)) 

This grammar is quite limited, generating only four sentences. The first way we will 
extend it is to allow verbs with objects: in addition to "The boy sleeps," we will allow 
"The boy meets the girl." To avoid generating ungrammatical sentences like "* The 
boy meets,"^ we will separate the category of verb into two subcategories: transitive 
verbs, which take an object, and intransitive verbs, which don't. 

Transitive verbs complicate the semantic interpretation of sentences. We would 
liketheinterpretationof "Terry kisses Jean" tobe (kiss Terry Jean). The interpretation 
of the noun phrase "Terry" is just Te r ry, but then what should the interpretation 
of the verb phrase "kisses Jean" be? To fit our predicate application model, it must 
be something equivalent to (lambda (x) (kiss . Jean)). When applied to the 
subject, we want to get the simplification: 

((lambda (x) (kiss . Jean)) Terry) => (kiss Terry Jean) 

Such simplification is not done automatically by Prolog, but we can write a predicate 
to do it. We will call it funcall, because it is similar to the Lisp function of that name, 
although it only handles replacement of the argument, not full evaluation of the 
body. (Technically, this is the lambda-calculus operation known as beta-reduction.) 
The predicate funcall is normally used with two input arguments, a function and its 
argument, and one output argument, the resulting reduction: 

(<- (funcall (lambda (?x) ?body) ?x ?body)) 

With this we could write our rule for sentences as: 

(rule (S ?sem) --> 
(NP ?agr ?subj) 
(VP ?agr ?pred) 
(:test (funcall ?pred ?subj ?sem))) 

An alternative is to, in effect, compile away the call to funcall. Instead of having the 
semantic representation of VP be a single lambda expression, we can represent it as 

^The asterisk at the start of a sentence is the standard linguistic notation for an utterance 
that is ungrammatical or otherwise ill-formed. 


<a id='page-694'></a>

two arguments: an input argument, ?subj, which acts as a parameter to the output 
argument, ?pred, which takes the place of the body of the lambda expression. By 
explicitly manipulating the parameter and body, we can eliminate the call to funcall. 
The trick is to make the parameter and the subject one and the same: 

(rule (S ?pred) --> 
(NP ?agr ?subj) 
(VP ?agr ?subj ?pred)) 

One way of reading this rule is "To parse a sentence, parse a noun phrase followed 
bya verb phrase. If they have different agreement features then fail, but otherwise 
insert the interpretation of the noun phrase, ?subj, into the proper spot in the 
interpretation of the verb phrase, ?pred, and return ?pred as the final interpretation 
of the sentence." 

The next step is to write rules for verb phrases and verbs. Transitive verbs are 
Usted under the predicate Verb/tr, and intransitive verbs are Usted as Verb/intr. 
The semantics of tenses (past and present) has been ignored. 

(rule (VP ?agr ?subj ?pred) --> 
(Verb/tr ?agr ?subj ?pred ?obj) 
(NP ?any-agr ?obj)) 

(rule (VP ?agr ?subj ?pred) --> 
(Verb/intr ?agr ?subj ?pred)) 

(rule (Verb/tr ~3sg ?x (kiss ?x ?y) ?y) --> (iword kiss)) 
(rule (Verb/tr 3sg ?x (kiss ?x ?y) ?y) --> (:word kisses)) 
(rule (Verb/tr ?any ?x (kiss ?x ?y) ?y) --> (:word kissed)) 

(rule (Verb/intr ~3sg ?x (sleep ?x)) --> (iword sleep)) 
(rule (Verb/intr 3sg ?x (sleep ?x)) --> (iword sleeps)) 
(rule (Verb/intr ?any ?x (sleep ?x)) --> (:word slept)) 

Here are the rules for noun phrases and nouns: 

(rule (NP ?agr ?sem) --> 
(Name ?agr ?sem)) 

(rule (NP ?agr (?det-sem ?noun-sem)) --> 
(Det ?agr ?det-sem) 
(Noun ?agr ?noun-sem)) 

(rule (Name 3sg Terry) --> (iword Terry)) 
(rule (Name 3sg Jean) --> (iword Jean)) 


<a id='page-695'></a>
(rule (Noun 3sg (young male human)) --> (:word boy)) 
(rule (Noun 3sg (young female human)) --> (rword girl)) 
(rule (Noun ~3sg (group (young male human))) --> (:word boys)) 
(rule (Noun ~3sg (group (young female human))) --> (:word girls)) 

(rule (Det ?any the) --> (:word the)) 
(rule (Det 3sg a) --> (rword a)) 

This grammar and lexicon generates more sentences, although it is still rather limited. 
Here are some examples: 

> (?- (S ?sem (The boys kiss a girl) ())) 
?SEM = (KISS (THE (GROUP (YOUNG MALE HUMAN))) 
(A (YOUNG FEMALE HUMAN))). 

> (?- (S ?sem (The girls kissed the girls) ())) 
?SEM = (KISS (THE (GROUP (YOUNG FEMALE HUMAN))) 
(THE (GROUP (YOUNG FEMALE HUMAN)))). 

> (?- (S ?sem (Terry kissed the girl) ())) 
?SEM = (KISS TERRY (THE (YOUNG FEMALE HUMAN))). 

> (?- (S ?sem (The girls kisses the boys) ())) 
No. 

> (?- (S ?sem (Terry kissed a girls) ())) 
No. 

> (?- (S ?sem (Terry sleeps Jean) ())) 
No. 

The first three examples are parsed correctly, while the final three are correctly 
rejected. The inquisitive reader may wonder just what is going on in the interpretation 
of a sentence like "The girls kissed the girls." Do the subject and object represent the 
same group of girls, or different groups? Does everyone kiss everyone, or are there 
fewer kissings going on? Until we define our representation more carefully, there is no 
way to tell. Indeed, it seems that there is a potential problem in the representation, in 
that the predicate ki ss sometimes has individuals as its arguments, and sometimes 
groups. More careful representations of "The girls kissed the girls" include the 
following candidates, using predicate calculus: 

VxVy xegirls . yegirls => kiss(x,y) 
VxVy xegirls . yegirls . x^^y => kiss(x,y) 
Vx3y,z xegirls . yegirls . zegirls => kiss(x,y) . kiss(z,x) 
Vx3y xegirls . yegirls => kiss(x,y) V kiss(y,x) 

The first of these says that every girl kisses every other girl. The second says the same 
thing, except that a girl need not kiss herself. The third says that every girl kisses 


<a id='page-696'></a>

and is kissed by at least one other girl, but not necessarily all of them, and the fourth 
says that everbody is in on at least one kissing. None of these interpretations says 
anything about who "the girls" are. 

Clearly, the predicate calculus representations are less ambiguous than the representation 
produced by the current system. On the other hand, it would be wrong 
to choose one of the representations arbitrarily, since in different contexts, "The girls 
kissed the girls" can mean different things. Maintaining ambiguity in a concise form 
is useful, as long as there is some way eventually to recover the proper meaning. 

20.4 A DCG Grammar with Quantifiers 
The problem in the representation we have been using becomes more acute when we 
consider other determiners, such as "every." Consider the sentence "Every picture 
paints a story." The preceding DCG, if given the right vocabulary, would produce 
the interpretation: 

(paints (every picture) (a story)) 

This can be considered ambiguous between the following two meanings, in predicate 
calculus form: 

VX picture(x) 3 y story(y) . paint(x,y) 
3 y story(y) . V . picture(x) => paint(x,y) 

The first says that for each picture, there is a story that it paints. The second says that 
there is a certain special story that every picture paints. The second is an unusual 
interpretation for this sentence, but for "Every U.S. citizen has a president," the 
second interpretation is perhaps the preferred one. In the next section, we will see 
how to produce representations that can be transformed into either interpretation. 
For now, it is a useful exercise to see how we could produce just the first representation 
above, the interpretation that is usually correct. First, we need to transcribe it into 
Lisp: 

(all ?x (-> (picture ?x) (exists ?y (and (story ?y) (paint ?x ?y))))) 

The first question is how the a 11 and exi sts forms get in there. They must come from 
the determiners, "every" and "a." Also, it seems that a 11 is followed by an implication 
arrow, ->, while exi sts is followed by a conjunction, and. So the determiners will 
have translations looking like this: 


<a id='page-697'></a>
(rule (Det ?any ?x ?p ?q (the ?x (and ?p ?q))) --> (:word the)) 
(rule (Det 3sg ?x ?p ?q (exists ?x (and ?p ?q))) --> (:word a)) 
(rule (Det 3sg ?x ?p ?q (all ?x (-> ?p ?q))) --> (:word every)) 

Once we have accepted these translations of the determiners, everything else follows. 
The formulas representing the determiners have two holes in them, ?p and ?q. The 
first will be filled by a predicate representing the noun, and the latter will be filled 
by the predicate that is being applied to the noun phrase as a whole. Notice that a 
curious thing is happening. Previously, translation to logical form was guided by 
the sentence's verb. Linguisticly, the verb expresses the main predicate, so it makes 
sense that the verb's logical translation should be the main part of the sentence's 
translation. In linguistic terms, we say that the verb is the head of the sentence. 

With the new translations for determiners, we are in effect turning the whole 
process upside down. Now the subject's determiner carries the weight of the whole 
sentence. The determiner's interpretation is a function of two arguments; it is applied 
to the noun first, yielding a function of one argument, which is in turn applied to the 
verb phrase's interpretation. This primacy of the determiner goes against intuition, 
but it leads directly to the right interpretation. 

The variables ?p and ?q can be considered holes to be filled in the final interpretation, 
but the variable ?x fills a quite different role. At the end of the parse, ?x will 
not be filled by anything; it will still be a variable. But it will be referred to by the 
expressions filling ?p and ?q. We say that ?x is a metavariable, because it is a variable 
in the representation, not a variable in the Prolog implementation. It just happens 
that Prolog variables can be used to implement these metavariables. 

Here are the interpretations for each word in our target sentence and for each 
intermediate constituent: 

Every = (all ?x (-> ?pl ?ql)) 
picture = (picture ?x) 
paints = (paint ?x ?y) 
a = (exists ?y (and ?p2 ?q2)) 
story = (story ?y) 

Every picture = (all ?x (-> (picture ?x) ?ql)) 
a story = (exists ?y (and (story ?y) ?q2)) 
paints a story = (exists ?y (and (story ?y) (paint ?x ?y))) 

The semantics of a noun has to fill the ?p hole of a determiner, possibly using the 
metavariable ?x. The three arguments to the Noun predicate are the agreement, the 
metavariable ?x, and the assertion that the noun phrase makes about ?x: 


<a id='page-698'></a>

(rule (Noun 3sg ?x (picture ?x)) --> (:word picture)) 
(rule (Noun 3sg ?x (story ?x)) --> (:word story)) 
(rule (Noun 3sg ?x (and (young ?x) (male ?x) (human ?x))) --> 

(iword boy)) 

The NP predicate is changed to take four arguments. First is the agreement, then 
the metavariable ?x. Third is a predicate that will be supplied externally, by the verb 
phrase. The final argument returns the interpretation of the NP as a whole. As we 
have stated, this comes from the determiner: 

(rule (NP ?agr ?x ?pred ?pred) --> 
(Name ?agr ?name)) 

(rule (NP ?agr ?x ?pred ?np) --> 
(Det ?agr ?x ?noun ?pred ?np) 
(Noun ?agr ?x ?noun)) 

The rule for an NP with determiner is commented out because it is convenient to 
introduce an extended rule to replace it at this point. The new rule accounts for 
certain relative clauses, such as "the boy that paints a picture": 

(rule (NP ?agr ?x ?pred ?np) --> 
(Det ?agr ?x ?noun&rel ?pred ?np) 
(Noun ?agr ?x ?noun) 
(rel-clause ?agr ?x ?noun ?noun&rel)) 

(rule (rel-clause ?agr ?x ?np ?np) --> ) 

(rule (rel-clause ?agr ?x ?np (and ?np ?rel)) --> 
(iword that) 
(VP ?agr ?x ?rel)) 

The new rule does not account for relative clauses where the object is missing, such 
as "the picture that the boy paints." Nevertheless, the addition of relative clauses 
means we can now generate an infinite language, since we can always introduce a 
relative clause, which introduces a new noun phrase, which in turn can introduce 
yet another relative clause. 

The rules for relative clauses are not complicated, but they can be difficult to 
understand. Of the four arguments to rel -clause, the first two hold the agreement 
features of the head noun and the metavariable representing the head noun. 
The last two arguments are used together as an accumulator for predications about 
the metavariable: the third argument holds the predications made so far, and the 
fourth will hold the predications including the relative clause. So, the first rule for 
rel -cl ause says that if there is no relative clause, then what goes in to the accumulator 
is the same as what goes out. The second rule says that what goes out is the 
conjunction of what comes in and what is predicated in the relative clause itself. 


<a id='page-699'></a>
Verbs apply to either one or two metavariables, just as they did before. So we can 
use the definitions of Verb/tr and Verb/i ntr unchanged. For variety, I've added a 
few more verbs: 

(rule (Verb/tr ~3sg ?x ?y (paint ?x ?y)) --> (rword paint)) 
(rule (Verb/tr 3sg ?x ?y (paint ?x ?y)) --> (iword paints)) 
(rule (Verb/tr ?any ?x ?y (paint ?x ?y)) --> (.-word painted)) 

(rule (Verb/intr ''3sg ?x (sleep ?x)) --> (:word sleep)) 
(rule (Verb/intr 3sg ?x (sleep ?x)) --> (:word sleeps)) 
(rule (Verb/intr ?any ?x (sleep ?x)) --> (:word slept)) 

(rule (Verb/intr 3sg ?x (sells ?x)) --> (:word sells)) 

(rule (Verb/intr 3sg ?x (stinks ?x)) --> (:word stinks)) 

Verb phrases and sentences are almost as before. The only difference is in the call to 
NP, which now has extra arguments: 

(rule (VP ?agr ?x ?vp) --> 
(Verb/tr ?agr ?x ?obj ?verb) 
(NP ?any-agr ?obj ?verb ?vp)) 

(rule (VP ?agr ?x ?vp) --> 
(Verb/intr ?agr ?x ?vp)) 

(rule (S ?np) --> 
(NP ?agr ?x ?vp ?np) 
(VP ?agr ?x ?vp)) 

With this grammar, we get the following correspondence between sentences and 
logical forms: 

Every picture paints a story. 
(ALL ?3 (-> (PICTURE ?3) 
(EXISTS ?14 (AND (STORY ?14) (PAINT ?3 ?14))))) 

Every boy that paints a picture sleeps. 
(ALL ?3 (-> (AND (AND (YOUNG ?3) (MALE ?3) (HUMAN ?3)) 
(EXISTS ?19 (AND (PICTURE ?19) 
(PAINT ?3 ?19)))) 
(SLEEP ?3))) 

Every boy that sleeps paints a picture. 
(ALL ?3 (-> (AND (AND (YOUNG ?3) (MALE ?3) (HUMAN ?3)) 
(SLEEP ?3)) 
(EXISTS ?22 (AND (PICTURE ?22) (PAINT ?3 ?22))))) 


<a id='page-700'></a>

Every boy that paints a picture that sells 
paints a picture that stinks. 
(ALL ?3 (-> (AND (AND (YOUNG ?3) (MALE ?3) (HUMAN ?3)) 

(EXISTS ?19 (AND (AND (PICTURE ?19) (SELLS ?19)) 
(PAINT ?3 ?19)))) 
(EXISTS ?39 (AND (AND (PICTURE ?39) (STINKS ?39)) 
(PAINT ?3 ?39))))) 

20.5 Preserving Quantifier Scope Ambiguity 
Consider the simple sentence "Every man loves a woman." This sentence is ambiguous 
between the following two interpretations: 

Vm3w man(m) . woman(w) . loves(m,w) 
3wVm man(m) . woman(w) . Ioves(m,w) 

The first interpretation is that every man loves some woman - his wife, perhaps. 
The second interpretation is that there is a certain woman whom every man loves - 
Natassja Kinski, perhaps. The meaning of the sentence is ambiguous, but the structure 
is not; there is only one syntactic parse. 

In the last section, we presented a parser that would construct one of the two 
interpretations. In this section, we show how to construct a single interpretation 
that preserves the ambiguity, but can be disambiguated by a postsyntactic process. 
The basic idea is to construct an intermediate logical form that leaves the scope of 
quantifiers unspecified. This intermediate form can then be rearranged to recover 
the final interpretation. 

To recap, here is the interpretation we would get for "Every man loves a woman," 
given the grammar in the previous section: 

(all ?m (-> (man ?m) (exists ?w) (and (woman ?w) (loves ?m ?w)))) 

We will change the grammar to produce instead the intermediate form: 

(and (all ?m (man ?m)) 
(exists ?w (wowan ?w)) 
(loves ?m ?w)) 

The difference is that logical components are produced in smaller chunks, with 
unscoped quantifiers. The typical grammar rule will build up an interpretation by 
conjoining constituents with and, rather than by fitting pieces into holes in other 


<a id='page-701'></a>
pieces. Here is the complete grammar and a just-large-enough lexicon in the new 
format: 

(rule (S (and ?np ?vp)) --> 
(NP ?agr ?x ?np) 
(VP ?agr ?x ?vp)) 

(rule (VP ?agr ?x (and ?verb ?obj)) --> 
(Verb/tr ?agr ?x ?o ?verb) 
(NP ?any-agr ?o ?obj)) 

(rule (VP ?agr ?x ?verb) --> 
(Verb/intr ?agr ?x ?verb)) 

(rule (NP ?agr ?name t) --> 
(Name ?agr ?name)) 

(rule (NP ?agr ?x ?det) --> 
(Det ?agr ?x (and ?noun ?rel) ?det) 
(Noun ?agr ?x ?noun) 
(rel-clause ?agr ?x ?rel)) 

(rule (rel-clause ?agr ?x t) --> ) 

(rule (rel-clause ?agr ?x ?rel) --> 
(:word that) 
(VP ?agr ?x ?rel)) 

(rule (Name 3sg Terry) --> (:word Terry)) 
(rule (Name 3sg Jean) --> (:word Jean)) 
(rule (Det 3sg ?x ?restr (all ?x ?restr)) --> (:word every)) 
(rule (Noun 3sg ?x (man ?x)) --> (:word man)) 
(rule (Verb/tr 3sg ?x ?y (love ?x ?y)) --> (iword loves)) 
(rule (Verb/intr 3sg ?x (lives ?x)) --> (iword lives)) 
(rule (Det 3sg ?x ?res (exists ?x ?res)) --> (iword a)) 
(rule (Noun 3sg ?x (woman ?x)) --> (iword woman)) 

This gives us the following parse for "Every man loves a woman": 

(and (all ?4 (and (man ?4) t)) 
(and (love ?4 ?12) (exists ?12 (and (woman ?12) t)))) 

If we simplified this, eliminating the ts and joining ands, we would get the desired 
representation: 

(and (all ?m (man ?m)) 
(exists ?w (wowan ?w)) 
(loves ?m ?w)) 

From there, we could use what we know about syntax, in addition to what we know 


<a id='page-702'></a>

about men, woman, and loving, to determine the most likely final interpretation. 
This will be covered in the next chapter. 

20.6 Long-Distance Dependencies 
So far, every syntactic phenomena we have considered has been expressible in a 
rule that imposes constraints only at a single level. For example, we had to impose 
the constraint that a subject agree with its verb, but this constraint involved two 
immediate constituents of a sentence, the noun phrase and verb phrase. We didn't 
need to express a constraint between, say, the subject and a modifier of the verb's 
object. However, there are linguistic phenomena that require just these kinds of 
constraints. 

Our rule for relative clauses was a very simple one: a relative clause consists of the 
word "that" followed by a sentence that is missing its subject, as in "every man that 
loves a woman." Not all relative clauses follow this pattern. It is also possible to form 
a relative clause by omitting the object of the embedded sentence: "every man that a 
woman loves In this sentence, the symbol u indicates a gap, which is understood 
as being filled by the head of the complete noun phrase, the man. This has been 
called a filler-gap dependency. It is also known as a long-distance dependency, because 
the gap can occur arbitrarily far from the filler. For example, all of the following are 
valid noun phrases: 

The person that Lee likes u 

The person that Kim thinks Lee likes ' 

The person that Jan says Kim thinks Lee likes u 

In each case, the gap is filled by the head noun, the person. But any number of relative 
clauses can intervene between the head noun and the gap. 

The same kind of filler-gap dependency takes place in questions that begin with 
"who," "what," "where," and other interrogative pronouns. For example, we can ask 
a question about the subject of a sentence, as in "Who likes Lee?", or about the object, 
as in "Who does Kim like '?" 

Here is a grammar that covers relative clauses with gapped subjects or objects. 
The rules for S, VP, and .. are augmented with a pair of arguments representing 
an accumulator for gaps. Like a difference list, the first argument minus the second 
represents the presence or absence of a gap. For example, in the first two rules for 
noun phrases, the two arguments are the same, ?gO and ?gO. This means that the rule 
as a whole has no gap, since there can be no difference between the two arguments. 
In the third rule for NP, the first argument is of the form (gap ...), and the second 
is nogap. This means that the right-hand side of the rule, an empty constituent, can 
be parsed as a gap. (Note that if we had been using true difference lists, the two 


<a id='page-703'></a>

arguments would be ((gap ...) ?gO) and ?gO. But since we are only dealing with 
one gap per rule, we don't need true difference lists.) 

The rule for S says that a noun phrase with gap ?gO minus ?gl followed by a verb 
phrase with gap ?gl minus ?g2 comprise a sentence with gap ?gO minus ?g2. The 
rule for relative clauses finds a sentence with a gap anywhere; either in the subject 
position or embedded somewhere in the verb phrase. Here's the complete grammar: 

(rule (S ?gO ?g2 (and ?np ?vp)) --> 
(NP ?gO ?gl ?agr ?x ?np) 
(VP ?gl ?g2 ?agr ?x ?vp)) 

(rule (VP ?gO ?gl ?agr ?x (and ?obj ?verb)) --> 
(Verb/tr ?agr ?x ?o ?verb) 
(NP ?gO ?gl ?any-agr ?o ?obj)) 

(rule (VP ?gO ?gO ?agr ?x ?verb) --> 
(Verb/intr ?agr ?x ?verb)) 

(rule (NP ?gO ?gO ?agr ?name t) --> 
(Name ?agr ?name)) 

(rule (NP ?gO ?gO ?agr ?x ?det) --> 
(Det ?agr ?x (and ?noun ?rel) ?det) 
(Noun ?agr ?x ?noun) 
(rel-clause ?agr ?x ?rel)) 

(rule (NP (gap NP ?agr ?x) nogap ?agr ?x t) --> ) 

(rule (rel-clause ?agr ?x t) --> ) 

(rule (rel-clause ?agr ?x ?rel) --> 
(:word that) 

(S (gap NP ?agr ?x) nogap ?rel)) 

Here are some sentence/parse pairs covered by this grammar: 
Every man that ' loves a woman likes a person. 
(AND (ALL ?28 (AND (MAN ?28) 
(AND . (AND (LOVE ?28 ?30) 
(EXISTS ?30 (AND (WOMAN ?30) 
T)))))) 
(AND (EXISTS ?39 (AND (PERSON ?39) T)) (LIKE ?28 ?39))) 

Every man that a woman loves yUkes a person. 
(AND (ALL ?37 (AND (MAN ?37) 
(AND (EXISTS ?20 (AND (WOMAN ?20) T)) 
(AND . (LOVE ?20 137))))) 
(AND (EXISTS ?39 (AND (PERSON ?39) T)) (LIKE ?37 ?39))) 


<a id='page-704'></a>

Every man that loves a bird that u^Hes likes a person. 
(AND (ALL ?28 (AND (MAN ?28) 
(AND . (AND (EXISTS ?54 
(AND (BIRD ?54) 
(AND . (FLY ?54)))) 
(LOVE ?28 ?54))))) 
(AND (EXISTS ?60 (AND (PERSON ?60) T)) (LIKE ?28 ?60))) 

Actually, there are limitations on the situations in which gaps can appear. In particular, 
it is rare to have a gap in the subject of a sentence, except in the case of a relative 
clause. In the next chapter, we will see how to impose additional constraints on gaps. 

20.7 Augmenting DCG Rules 
In the previous section, we saw how to build up a semantic representation of a 
sentence by conjoining the semantics of the components. One problem with this 
approach is that the semantic interpretation is often something of the form (and 
(and t a) when we would prefer (and ab). There are two ways to correct 
this problem: either we add a step that takes the final semantic interpretation and 
simplifies it, or we complicate each individual rule, making it generate the simplified 
form. The second choice would be slightly more efficient, but would be very ugly 
and error prone. We should be doing all we can to make the rules simpler, not more 
complicated; that is the whole point of the DCG formalism. This suggests a third 
approach: change the rule interpreter so that it automatically generates the semantic 
interpretation as a conjunction of the constituents, unless the rule explicitly says 
otherwise. This section shows how to augment the DCG rules to handle common 
cases like this automatically. 

Consider again a rule from section 20.4: 

(rule (S (and ?np ?vp))--> 
(NP ?agr ?x ?np) 
(VP ?agr ?x ?vp)) 

If we were to alter this rule to produce a simplified semantic interpretation, it would 
look like the following, where the predicate and* simplifies a list of conjunctions into 
a single conjunction: 


<a id='page-705'></a>
(rule (S ?sem) --> 
(np ?agr ?x ?np) 
(vp ?agr ?x ?vp) 
(:test (ancl*(?np ?vp) ?sem))) 

Many rules will have this form, so we adopt a simple convention: if the last argument 
of the constituent on the left-hand side of a rule is the keyword : sem, then we will 
build the semantics by replacing : sem with a conjunction formed by combining all 
the last arguments of the constituents on the right-hand side of the rule. A==> arrow 
will be used for rules that follow this convention, so the following rule is equivalent 
to the one above: 

(rule (S :sem) ==> 
(NP ?agr ?x ?np) 
(VP ?agr ?x ?vp)) 

It is sometimes useful to introduce additional semantics that does not come from one 
of the constituents. This can be indicated with an element of the right-hand side that 
is a list starting with : sem. For example, the following rule adds to the semantics the 
fact that ?x is the topic of the sentence: 

(rule (S ;sem) ==> 
(NP ?agr ?x ?np) 
(VP ?agr ?x ?vp) 
(:sem (topic ?x))) 

Before implementing the rule function for the ==> arrow, it is worth considering if 
there are other ways we could make things easier for the rule writer. One possibility is 
to provide a notation for describing examples. Examples make it easier to understand 
what a rule is designed for. For the S rule, we could add examples like this: 

(rule (S :sem) ==> 
(:ex "John likes Mary" "He sleeps") 
(NP ?agr ?x ?np) 
(VP ?agr ?x ?vp)) 

These examples not only serve as documentation for the rule but also can be stored 
under S and subsequently run when we want to test if S is in fact implemented 
properly. 

Another area where the rule writer could use help is in handling left-recursive 

rules. Consider the rule that says that a sentence can consist of two sentences joined 

by a conjunction: 


<a id='page-706'></a>

(rule (S (?conj ?sl ?s2)) ==> 
(:ex "John likes Mary and Mary likes John") 
(S ?sl) 
(Conj ?conj) 
(S ?s2)) 

While this rule is correct as a declarative statement, it will run into difficulty when 
run by the standard top-down depth-first DCG interpretation process. The top-level 
goal of parsing an S will lead immediately to the subgoal of parsing an S, and the 
result will be an infinite loop. 

Fortunately, we know how to avoid this kind of infinite loop: split the offending 
predicate, S, into two predicates: one that supports the recursion, and one that is at 
a lower level. We will call the lower-level predicate S_. Thus, the following rule says 
that a sentence can consist of two sentences, where the first one is not conjoined and 
the second is possibly conjoined: 

(rule (S (?conj ?sl ?s2)) ==> 

(S- ?sl) 

(Conj ?conj) 

(S ?s2)) 

We also need a rule that says that a possibly conjoined sentence can consist of a 
nonconjoined sentence: 

(rule (S ?sem) ==> (S_ ?sem)) 

To make this work, we need to replace any mention of S in the left-hand side of a rule 
with S_. References to S in the right-hand side of rules remain unchanged. 

(rule (S_ ?sem) ==> ...) 

To make this all automatic, we will provide a macro, conj-rule, that declares a 
category to be one that can be conjoined. Such a declaration will automatically 
generate the recursive and nonrecursive rules for the category, and will insure that 
future references to the category on the left-hand side of a rule will be replaced with 
the corresponding lower-level predicate. 

One problem with this approach is that it imposes a right-branching parse on 
multiple conjoined phrases. That is, we will get parses like "spaghetti and (meatballs 
and salad)" not "(spaghetti and meatballs) and salad." Clearly, that is the wrong 
interpretation for this sentence. Still, it can be argued that it is best to produce 
a single canonical parse, and then let the semantic interpretation functions worry 
about rearranging the parse in the right order. We will not attempt to resolve this 


<a id='page-707'></a>
debate but will provide the automatic conjunction mechanism as a tool that can be 
convenient but has no cost for the user who prefers a different solution. 

We are now ready to implement the extended DCG rule formalism that handles 
:sem, :ex, and automatic conjunctions. The function make-augmented-dcg, stored 
under the arrow = =>, will be used to implement the formalism: 

(setf (get '==> 'rule-function) 'make-augmented-dcg) 

(defun make-augmented-dcg (head body) 
"Build an augmented DCG rule that handles :sem. :ex, 
and automatic conjunctiontive constituents." 
(if (eq (lastl head) :sem) 

;; Handle :sem 

(let* ((?sem (gensym "?SEM"))) 

(make-augmented-dcg 
'(.(butlast head) .?sem) 
'(.(remove :sem body :key #'first-or-nil) 

(:test .(collect-sems body ?sem))))) 
Separate out examples from body 
(multiple-value-bind (exs new-body) 
(partition-if #'(lambda (x) (starts-with . :ex)) body) 
Handle conjunctions 
(let ((rule '(rule .(handle-conj head) --> .new-body))) 

(if (null exs) 
rule 
'(progn (:ex .head ..(mappend #'rest exs)) 

.rule)))))) 

First we show the code that collects together the semantics of each constituent and 
conjoins them when :sem is specified. The function collect-sems picks out the 
semantics and handles the trivial cases where there are zero or one constituents on 
the right-hand side. If there are more than one, it inserts a call to the predicate and*. 

(defun collect-sems (body ?sem) 
"Get the semantics out of each constituent in body, 
and combine them together into ?sem." 
(let ((sems (loop for goal in body 

unless (or (dcg-normal-goal-p goal) 
(dcg-word-list-p goal) 
(starts-with goal :ex) 
(atom goal)) 

collect (lastl goal)))) 

(case (length sems) 
(0 '(= .?sem t)) 
(1 '(= .?sem .(first sems))) 
(t '(and* .sems .?sem))))) 


<a id='page-708'></a>

We could have implemented and* with Prolog clauses, but it is slightly more efficient 
to do it directly in Lisp. A call to conjuncts collects all the conjuncts, and we then 
add an and if necessary: 

(defun and*/2 (in out cont) 
"IN is a list of conjuncts that are conjoined into OUT." 
E.g.: (and* (t (and a b) t (and c d) t) ?x) ==> 
;; ?x= (and abed) 
(if (unify! out (maybe-add 'and (conjuncts (cons 'and in)) t)) 
(funcall cont))) 

(defun conjuncts (exp) 
"Get all the conjuncts from an expression." 
(deref exp) 
(cond ((eq exp t) nil) 

((atom exp) (list exp)) 
((eq (deref (first exp)) 'nil) nil) 
((eq (first exp) 'and) 

(mappend #'conjuncts (rest exp))) 
(t (list exp)))) 

The next step is handling example phrases. The code in make-augmented-dcg turns 
examples into expressions of the form: 

(:ex (S ?sem) "John likes Mary" "He sleeps") 

To make this work, : ex will have to be a macro: 

(defmacro :ex ((category . args) &body examples) 
"Add some example phrases, indexed under the category." 
'(add-examples ',category ',args ',examples)) 

: ex calls add-exampl es to do all the work. Each example is stored in a hash table 
indexed under the the category. Each example is transformed into a two-element list: 
the example phrase string itself and a call to the proper predicate with all arguments 
supplied. The function add-exampl es does this transformation and indexing, and 
run-examples retrieves the examples stored under a category, prints each phrase, 
and calls each goal. The auxiliary functions get-exampl es and cl ear-exampl es are 
provided to manipulate the example table, and remove-punction, punctuation-p 
and stri ng ->1 i st are used to map from a string to a Hst of words. 

(defvar *examples* (make-hash-table :test #'eq)) 

(defun get-examples (category) (gethash category *examples*)) 

(defun clear-examples () (clrhash *examples*)) 


<a id='page-709'></a>
(defun add-examples (category args examples) 
"Add these example strings to this category, 
and when it comes time to run them, use the args." 
(dolist (example examples) 

(when (stringp example) 
(let ((ex '(.example 
(.category .@args 
.(string->list 
(remove-punctuation example)) ())))) 
(unless (member ex (get-examples category) 
:test #'equal) 
(setf (gethash category ^examples*) 
(nconc (get-examples category) (1 ist ex)))))))) 

(defun run-examples (&optional category) 
"Run all the example phrases stored under a category. 
With no category, run ALL the examples." 
(prolog-compi1e-symbols) 
(if (null category) 

(maphash #'(lambda (cat val) 
(declare (ignore val)) 
(format t "~2&Examples of ~a:~&" cat) 
(run-examples cat)) 

^examples*) 

(dolist (example (get-examples category)) 
(format t "~2&EXAMPLE: ~{~a~r9T~a~}" example) 
(top-level-prove (cdr example))))) 

(defun remove-punctuation (string) 
"Replace punctuation with spaces in string. " 
(substitute-if #\space #'punctuation-p string)) 

(defun string->list (string) 
"Convert a string to a list of words." 
(read-from-string(concatenate 'string "("string ")"))) 

(defun punctuation-p (char) (find char "*...;:'!?#-()\\\"")) 

The final part of our augmented DCG formalism is handling conjunctive constituents 
automatically. We already arranged to translate category symbols on the left-hand 
side of rules into the corresponding conjunctive category, as specified by the function 
handl e-con j. We also want to generate automatically (or as easily as possible) rules 
of the following form: 

(rule (S (?conj ?sl ?s2)) ==> 
(S_ ?sl) 
(Conj ?conj) 
(S ?s2)) 


<a id='page-710'></a>

(rule (S ?sem) ==> (S_ ?sem)) 

But before we generate these rules, let's make sure they are exactly what we want. 
Consider parsing a nonconjoined sentence with these two rules in place. The first 
rule would parse the entire sentence as a S_, and would then fail to see a Con j, and thus 
fail. The second rule would then duplicate the entire parsing process, thus doubling 
the amount of time taken. If we changed the order of the two rules we would be able 
to parse nonconjoined sentences quickly, but would have to backtrack on conjoined 
sentences. 

The following shows a better approach. A single rule for S parses a sentence 
with S_, and then calls Conj.S, which can be read as "either a conjunction followed 
by a sentence, or nothing." If the first sentence is followed by nothing, then we just 
use the semantics of the first sentence; if there is a conjunction, we have to form a 
combined semantics. I have added ... to show where arguments to the predicate 
other than the semantic argument fit in. 

(rule (S ... ?s-combi ned) ==> 
(S_ ... ?seml) 
(Conj_S ?seml ?s-combined)) 

(rule (Conj.S ?seml (?conj ?seml ?sem2)) ==> 
(Conj ?conj) 
(S ... ?sem2)) 

(rule (Conj_S ?seml ?seml) ==>) 

Now all we need is a way for the user to specify that these three rules are desired. 
Since the exact method of building up the combined semantics and perhaps even 
the call to Conj may vary depending on the specifics of the grammar being defined, 
the rules cannot be generated entirely automatically. We will settle for a macro, 
conj - rule, that looks very much like the second of the three rules above but expands 
into all three, plus code to relate S_ to S. So the user will type: 

(conj-rule (Conj.S ?seml (?conj ?seml ?sem2)) ==> 
(Conj ?conj) 
(S ?a ?b ?c ?sem2)) 

Here is the macro definition: 

(defmacro conj-rule ((conj-cat semi combined-sem) ==> 

conj (cat . args)) 
"Define this category as an automatic conjunction." 
'(progn 

(setf (get ',cat 'conj-cat) '.(symbol cat '_)) 


<a id='page-711'></a>
(rule (.cat ,@(butlast args) ?combined-sem) ==> 
(.(symbol cat '_) .(butlast args) .semi) 
(.conj-cat ,seml ?combined-sem)) 

(rule (,conj-cat .semi .combined-sem) ==> 
.conj 
(.cat .args)) 

(rule (.conj-cat ?seml ?seml) ==>))) 

and here we define handl e-conj to substitute S_for S in the left-hand side of rules: 

(defun handle-conj (head) 
"Replace (Cat ...) with (Cat. ...) if Cat is declared 
as a conjunctive category." 
(if (and (listp head) (conj-category (predicate head))) 

(cons (conj-category (predicate head)) (args head)) 
head)) 

(defun conj-category (predicate) 
"If this is a conjunctive predicate, return the Cat. symbol." 
(get predicate 'conj-category)) 

20.8 History and References 
As we have mentioned, Alain Colmerauer invented Prolog to use in his grammar of 
French (1973). His metamorphosis grammar formalismwas more expressive but much 
less efficient than the standard DCG formalism. 

The grammar in section 20.4 is essentially the same as the one presented in Fernando 
Pereira and David H. D. Warren's 1980 paper, which introduced the Definite 
Clause Grammar formalism as it is known today. The two developed a much more 
substantial grammar and used it in a very influential question-answering system 
called Chat-80 (Warren and Pereira, 1982). Pereira later teamed with Stuart Shieber 
on an excellent book covering logic grammars in more depth: Prolog and Natural-
Language Analysis (1987). The book has many strong points, but unfortunately it does 
not present a grammar anywhere near as complete as the Chat-80 grammar. 

The idea of a compositional semantics based on mathematical logic owes much 
to the work of the late linguist Richard Montague. The introduction by Dowty, Wall, 
and Peters (1981) and the collection by Rich Thomason (1974) cover Montague's 
approach. 

The grammar in section 20.5 is based loosely on Michael McCord's modular logic 
grammar, as presented in Walker et al. 1990. 
It should be noted that logic grammars are by no means the only approach to 
natural language processing. Woods (1970) presents an approach based on the 


<a id='page-712'></a>

augmented transition network, or ATN. A transition network is like a context-free 
grammar. The augmentation is a way of manipulating features and semantic values. 
This is just like the extra arguments in DCGs, except that the basic operations are 
setting and testing variables rather than unification. So the choice between ATNs and 
DCGs is largely a matter of what programming approach you are most comfortable 
with: procedural for ATNs and declarative for DCGs. My feeling is that unification is 
a more suitable primitive than assignment, so I chose to present DCGs, even though 
this required bringing in Prolog's backtracking and unification mechanisms. 
In either approach, the same linguistic problems must be addressed - agreement, 
long-distance dependencies, topicalization, quantifier-scope ambiguity, and so on. 
Comparing Woods's (1970) ATN grammar to Pereira and Warren's (1980) DCG grammar, 
the careful reader will see that the solutions have much in common. The analysis 
is more important than the notation, as it should be. 
20.9 Exercises 
&#9635; Exercise 20.2 [m] Modify the grammar (from section 20.4, 20.5,
for adjectives before a noun. 
or 20.6) to allow 

&#9635; Exercise 20.3 [m] Modify the grammar to allow for prepositional phrase modifiers 
on verb and noun phrases. 

&#9635; Exercise 20.4 [m] Modify the grammar to allow for ditransitive verbs?erbs that 
take two objects, as in "give the dog a bone." 

&#9635; Exercise 20.5 Suppose we wanted to adopt the Prolog convention of writing DCG 
tests and words in brackets and braces, respectively. Write a function that will alter 
the readtable to work this way. 

&#9635; Exercise 20.6 [m] Define a rule function for a new type of DCG rule that automatically 
builds up a syntactic parse of the input. For example, the two rules: 
(rule is) => (np) (vp)) 
(rule (np) => (iword he)) 
should be equivalent to: 


<a id='page-713'></a>
(rule (s (s ?1 ?2)) --> (np ?1) (vp 12)) 
(rule (np (np he)) --> (:word he)) 

&#9635; Exercise 20.7 [m] There are advantages and disadvantages to the approach that 
Prolog takes in dividing predicates into clauses. The advantage is that it is easy to 
add a new clause. The disadvantage is that it is hard to alter an existing clause. If 
you edit a clause and then evaluate it, the new clause will be added to the end of the 
clause list, when what you really wanted was for the new clause to take the place 
of the old one. To achieve that effect, you have to call cl ear-predicate, and then 
reload all the clauses, not just the one that has been changed. 

Write a macro named - rul e that is just like rul e, except that it attaches names to 
clauses. When a named rule is reloaded, it replaces the old clause rather than adding 
a new one. 

&#9635; Exercise 20.8 [h] Extend the DCG rule function to allow or goals in the right-hand 
side. To make this more useful, also allow and goals. For example: 

(rule (A) --> (B) (or (C) (and (D) (E))) (F)) 

should compile into the equivalent of: 

(<- (A ?S0 ?S4) 
(B ?S0 ?S1) 
(OR (AND (C ?S1 ?S2) (= ?S2 ?S3)) 

(AND (D ?S1 ?S2) (E ?S2 ?S3))) 
(F ?S3 ?S4)) 

20.10 Answers 
Answer 20.1 It uses local variables (?s0, ?sl ...) that are not guaranteed to be 
unique. This is a problem if the grammar writer wants to use these symbols anywhere 
in his or her rules. The fix is to gensym symbols that are guaranteed to be unique. 


<a id='page-714'></a>

Answer 20.5 

(defun setup-braces (&optional (on? t) (readtable *readtable*)) 
"Make Ca b] read as (:word a b) and {a b} as (rtest a b c) 
if ON? is true; otherwise revert {[]} to normal." 
(if (not on?) 

(map nil #'(lambda (c) 
(set-macro-character c (get-macro-character #\a) 
t readtable)) 
"{[]}") 
(progn 
(set-macro-character 
#\] (get-macro-character #\)) nil readtable) 
(set-macro-character 
#\} (get-macro-character #\)) nil readtable) 
(set-macro-character 
#\[ #'(lambda (s ignore) 
(cons :word (read-delimited-1ist #\] s t))) 
nil readtable) 
(set-macro-character 
#\{ #'(lambda (s ignore) 
(cons rtest (read-delimited-1ist #\} s t))) 
nil readtable)))) 


## Chapter 21
<a id='page-715'></a>

A Grammar of English 

Prefer geniality to grammar. 

- Henry Watson Fowler 

The King's English 906) 

I I 1 he previous two chapters outline techniques for writing grammars and parsers based on 

I those grammars. It is quite straightforward to apply these techniques to applications 

JL like the CD player problem where input is limited to simple sentences like "Play 1 to 
8 without 3." But it is a major undertaking to write a grammar for unrestricted English input. 
This chapter develops a grammar that covers all the major syntactic constructions of English. It 
handles sentences of much greater complexity, such as "Kim would not have been persuaded 
by Lee to look after the dog." The grammar is not comprehensive enough to handle sentences 
chosen at random from a book, but when augmented by suitable vocabulary it is adequate for a 
wide variety of applications. 

This chapter is organized as a tour through the English language. We first cover noun 
phrases, then verb phrases, clauses, and sentences. For each category we introduce examples, 
analyze them linguistically, and finally show definite clause grammar rules that correspond to 
the analysis. 


<a id='page-716'></a>

As the last chapter should have made clear, analysis more often results in complication 
than in simplification. For example, starting with a simple rule like (S 
- -> . . VP), we soon find that we have to add arguments to handle agreement, semantics, 
and gapping information. Figure 21.1 lists the grammatical categories and 
their arguments. Note that the semantic argument, sem, is always last, and the gap 
accumulators, gapl and gap2, are next-to-last whenever they occur. All single-letter 
arguments denote metavariables; for example, each noun phrase (category NP) will 
have a semantic interpretation, sem, that is a conjunction of relations involving the 
variable x. Similarly, the h in modif i ers is a variable that refers to the head - the thing 
that is being modified. The other arguments and categories will be explained in turn, 
but it is handy to have this figure to refer back to. 

Category Arguments 
Preterminals 

name agr name 
verb verb inflection slots . sem 
rel-pro case type 

pronoun agr case wh . sem 
art agr quant 
adj X sem 
cardinal number agr 
ordinal number 
prep prep sem 
noun agr slots . sem 
aux inflection needs-inflection . sem 
adverb X sem 

Nonterminals 

S s sem 
aux-inv-S subject s sem 
clause inflection . int-subj . gapl gap2 sem 
subject agr . subj-slot int-subj gapl gap2 sem 
VP inflection . subject-slot . gapl gap2 vp 

NP agr case wh . gapl gap2 np 
NP2 agr case . gapl gap2 sem 

PP prep role wh np . gapl gap2 sem 
XP slot constituent wh . gapl gap2 sem 
Det agr wh . restriction sem 
rel-clause agr . sem 
modifiers pre/post cat info slots h gapl gap2 sem 
complement cat info slot h gapl gap2 sem 
adjunct pre/post cat info h gapl gap2 sem 
advp wh X gapl gap2 sem 

Figure 21.1: Grammatical Categories and their Arguments 


<a id='page-717'></a>
21.1 Noun Phrases 
The simplest noun phrases are names and pronouns, such as "Kim" and "them." 
The rules for these cases are simple: we build up a semantic expression from a name 
or pronoun, and since there can be no gap, the two gap accumulator arguments are 
the same (?gl). Person and number agreement is propagated in the variable ?agr, 
and we also keep track of the case of the noun phrase. English has three cases that 
are reflected in certain pronouns. In the first person singular, ". is the nominative or 
subjective case, "me" is the accusative or objective case, and "my" is the genitive case. To 
distinguish them from the genitive, we refer to the nominative and the objective cases 
as the common cases. Accordingly, the three cases will be marked by the expressions 
(common nom), (common obj), and gen, respectively. Many languages of the world 
have suffixes that mark nouns as being one case or another, but English does not. 
Thus, we use the expression (common ?) to mark nouns. 

We also distinguish between noun phrases that can be used in questions, like 
"who," and those that cannot. The ?wh variable has the value +wh for noun phrases 
like "who" or "which one" and - wh for nonquestion phrases. Here, then, are the rules 
for names and pronouns. The predicates name and pronoun are used to look up words 
in the lexicon. 

(rule (NP ?agr (common ?) -wh ?x ?gl ?gl (the ?x (name ?name ?x))) ==> 
(name ?agr ?name)) 

(rule (NP ?agr ?case ?wh ?x ?gl ?gl ?sem) ==> 
(pronoun ?agr ?case ?wh ?x ?sem)) 

Plural nouns can stand alone as noun phrases, as in "dogs," but singular nouns need 
a determiner, as in "the dog" or "Kim's friend's biggest dog." Plural nouns can also 
take a determiner, as in "the dogs." The category Det is used for determiners, and 
NP2 is used for the part of a noun phrase after the determiner: 

(rule (NP (---+) ?case -wh ?x ?gl ?g2 (group ?x ?sem)) ==> 
(:ex "dogs") ; Plural nouns don't need a determiner 
(NP2 ( +) ?case ?x ?gl ?g2 ?sem)) 

(rule (NP ?agr (common ?) ?wh ?x ?gl ?g2 ?sem) ==> 
(:ex "Every man" "The dogs on the beach") 
(Det ?agr ?wh ?x ?restriction ?sem) 
(NP2 ?agr (common ?) ?x ?gl ?g2 ?restriction)) 

Finally, a noun phrase may appear externally to a construction, in which case the 
noun phrase passed in by the first gap argument will be consumed, but no words 
from the input will be. An example is the u in "Whom does Kim like 


<a id='page-718'></a>

(rule (NP ?agr ?case ?wh ?x (gap (NP ?agr ?case ?x)) (gap nil) t) 
==> Gapped NP 
) 

Now we address the heart of the noun phrase, the NP2 category. The lone rule for NP2 
says that it consists of a noun, optionally preceded and followed by modifiers: 

(rule (NP2 ?agr (common ?) ?x ?gl ?g2 :sem) ==> 

(modifiers pre noun ?agr () ?x (gap nil) (gap nil) ?pre) 

(noun ?agr ?slots ?x ?noun) 

(modifiers post noun ?agr ?slots ?x ?gl ?g2 ?post)) 

21.2 Modifiers 
Modifiers are split into type types: Complements are modifiers that are expected by the 
head category that is being modified; they cannot stand alone. Adjuncts are modifiers 
that are not required but bring additional information. The distinction is clearest 
with verb modifiers. In "Kim visited Lee yesterday," "visited" is the head verb, "Lee" 
is a complement, and "yesterday" is an adjunct. Returning to nouns, in "the former 
mayor of Boston," "mayor" is the head noun, "of Boston" is a complement (although 
an optional one) and "former" is an adjunct. 

The predicate modi f i ers takes eight arguments, so it can be tricky to understand 
them all. The first two arguments tell if we are before or after the head (pre or 
post) and what kind of head we are modifying (noun, verb, or whatever). Next is 
an argument that passes along any required information - in the case of nouns, it 
is the agreement feature. The fourth argument is a list of expected complements, 
here called ?slots. Next is the metavariable used to refer to the head. The final 
three arguments are the two gap accumulators and the semantics, which work the 
same way here as we have seen before. Notice that the lexicon entry for each Noun 
can have a list of complements that are considered as postnoun modifiers, but there 
can be only adjuncts as prenoun modifiers. Also note that gaps can appear in the 
postmodifiers but not in the premodifiers. For example, we can have "What is Kevin 
the former mayor of where the answer might be "Boston." But even though 
we can construct a noun phrase like "the education president," where "education" 
is a prenoun modifier of "president," we cannot construct "* What is George the u 
president?," intending that the answer be "education." 

There are four cases for modification. First, a complement is a kind of modifier. 
Second, if a complement is marked as optional, it can be skipped. Third, an adjunct 
can appear in the input. Fourth, if there are no complements expected, then there 
need not be any modifiers at all. The following rules implement these four cases: 


<a id='page-719'></a>

(rule (modifiers ?pre/post ?cat ?info (?slot . ?slots) ?h 

?gl ?g3 :sem) ==> 
(complement ?cat ?info ?slot ?h ?gl ?g2 ?mod) 
(modifiers ?pre/post ?cat ?info ?slots ?h ?g2 ?g3 ?mods)) 

(rule (modifiers ?pre/post ?cat ?info ((? (?) ?) . ?slots) ?h 
?gl ?g2 ?mods) == > 
(modifiers ?pre/post ?cat ?info ?slots ?h ?gl ?g2 ?mods)) 

(rule (modifiers ?pre/post ?cat ?info ?slots ?h ?gl ?g3 :sem) ==> 
(adjunct ?pre/post ?cat ?info ?h ?gl ?g2 ?adjunct) 
(modifiers ?pre/post ?cat ?info ?slots ?h ?g2 ?g3 ?mods)) 

(rule (modifiers ???()? ?gl ?gl t) ==> ) 

We need to say more about the Ust of complements, or slots, that can be associated 
with words in the lexcion. Each slot is a list of the form i role number form), where 
the role refers to some semantic relation, the number indicates the ordering of the 
complements, and the form is the type of constituent expected: noun phrase, verb 
phrase, or whatever. The details will be covered in the following section on verb 
phrases, and compi ement will be covered in the section on XPs. For now, we give a 
single example. The complement list for one sense of the verb "visit" is: 

((agt 1 (NP ?)) (obj 2 (NP ?))) 

This means that the first complement, the subject, is a noun phrase that fills the agent 
role, and the second complement is also a noun phrase that fills the object role. 

21.3 Noun Modifiers 
There are two main types of prenoun adjuncts. Most common are adjectives, as 
in "big slobbery dogs." Nouns can also be adjuncts, as in "water meter" or "desk 
lamp." Here it is clear that the second noun is the head and the first is the modifier: 
a desk lamp is a lamp, not a desk. These are known as noun-noun compounds. In 
the following rules, note that we do not need to say that more than one adjective is 
allowed; this is handled by the rules for modi f i ers. 

(rule (adjunct pre noun ?info ?x ?gap ?gap ?sem) ==> 
(adj ?x ?sem)) 

(rule (adjunct pre noun ?info ?h ?gap ?gap :sem) ==> 
(:sem (noun-noun ?h ?x)) 
(noun ?agr () ?x ?sem)) 

After the noun there is a wider variety of modifiers. Some nouns have complements. 


<a id='page-720'></a>

which are primarily prepositional phrases, as in "mayor of Boston." These will be 
covered when we get to the lexical entries for nouns. Prepositional phrases can be 
adjuncts for nouns or verbs, as in "man in the middle" and "slept for an hour." We 
can write one rule to cover both cases: 

(rule (adjunct post ?cat ?info ?x ?gl ?g2 ?sem) ==> 
(PP ?prep ?prep ?wh ?np ?x ?gl ?g2 ?sem)) 

Here are the rules for prepositional phrases, which can be either a preposition 
followed by a noun phrase or can be gapped, as in "to whom are you speaking 
The object of a preposition is always in the objective case: "with him" not "*with he." 

(rule (PP ?prep ?role ?wh ?np ?x ?gl ?g2 :sem) ==> 
(prep ?prep t) 
(:sem (?role ?x ?np)) 
(NP ?agr (common obj) ?wh ?np ?gl ?g2 ?np-sem)) 

(rule (PP ?prep ?role ?wh ?np ?x 
(gap (PP ?prep ?role ?np ?x)) (gap nil) t) ==> ) 

Nouns can be modified by present participles, past participles, and relative clauses. 
Examples are "the man eating the snack," "the snack eaten by the man," and "the 
man that ate the snack," respectively. We will see that each verb in the lexicon is 
marked with an inflection, and that the marker - i ng is used for present participles 
while - en is used for past participles. The details of the clause will be covered later. 

(rule (adjunct post noun ?agr ?x ?gap ?gap ?sem) ==> 
(:ex (the man) "visiting me" (the man) "visited by me") 
(:test (member ?infl (-ing passive))) 
(clause ?infl ?x ? ?v (gap (NP ?agr ? ?x)) (gap nil) ?sem)) 

(rule (adjunct post noun ?agr ?x ?gap ?gap ?sem) ==> 
(rel-clause ?agr ?x ?sem)) 

It is possible to have a relative clause where it is an object, not the subject, that the 
head refers to: "the snack that the man ate." In this kind of relative clause the relative 
pronoun is optional: "The snack the man ate was delicious." The following rules say 
that if the relative pronoun is omitted then the noun that is being modified must be 
an object, and the relative clause should include a subject internally. The constant 
int-subj indicates this. 

(rule (rel-clause ?agr ?x :sem) ==> 
(:ex (the man) "that she liked" "that liked her" 
"that I know Lee liked") 


<a id='page-721'></a>
(opt-rel-pronoun ?case ?x ?int-subj ?rel-sem) 
(clause (finite ? ?) ? ?int-subj ?v 
(gap (NP ?agr ?case ?x)) (gap nil) ?clause-sem)) 

(rule (opt-rel-pronoun ?case ?x ?int-subj (?type ?x)) ==> 
(rword ?rel-pro) 

(:test (word ?rel-pro rel-pro ?case ?type))) 

(rule (opt-rel-pronoun (common obj) ?x int-subj t) ==> ) 

It should be noted that it is rare but not impossible to have names and pronouns 
with modifiers: "John the Baptist/' "lovely Rita, meter maid," "Lucy in the sky with 
diamonds," "Sylvia in accounting on the 42nd floor," "she who must be obeyed," 
Here and throughout this chapter we will raise the possibility of such rare cases, 
leaving them as exercises for the reader. 

21.4 Determiners 
We will cover three kinds of determiners. The simplest is the article: "a dog" or "the 
dogs." We also allow genitive pronouns, as in "her dog," and numbers, as in "three 
dogs." The semantic interpretation of a determiner-phrase is of the form (quantifier 
variable restriction). ... example A Si ?x (dog ?x)) or ((number 3) ?x (dog ?x)). 

(rule (Det ?agr ?wh ?x ?restriction (?art ?x ?restriction)) ==> 
(:ex "the" "every") 
(art ?agr ?art) 
(:test (if (= ?art wh) (= ?wh +wh) (= ?wh -wh)))) 

(rule (Det ?agr ?wh ?x ?r (the ?x ?restriction)) ==> 
(:ex "his" "her") 
(pronoun ?agr gen ?wh ?y ?sem) 
(:test (and* ((genitive ?y ?x) ?sem ?r) ?restriction))) 

(rule (Det ?agr -wh ?x ?r ((number ?n) ?x ?r)) ==> 
(:ex "three") 
(cardinal ?n ?agr)) 

These are the most important determiner types, but there are others, and there are 
pre- and postdeterminers that combine in restricted combinations. Predeterminers 
include all, both, half, double, twice, and such. Postdeterminers include every, 
many, several, and few. Thus, we can say "all her many good ideas" or "all the King's 
men." But we can not say "*all much ideas" or "*the our children." The details are 
complicated and are omitted from this grammar. 


<a id='page-722'></a>

21.5 Verb Phrases 
Now that we have defined modi f i ers, verb phrases are easy. In fact, we only need 
two rules. The first says a verb phrase consists of a verb optionally preceded and 
followed by modifiers, and that the meaning of the verb phrase includes the fact that 
the subject fills some role: 

(rule (VP ?infl ?x ?subject-slot ?v ?gl ?g2 :sem) ==> 
(:ex "sleeps" "quickly give the dog a bone") 
(modifiers pre verb ? () ?v (gap nil) (gap nil) ?pre-sem) 
(:sem (?role ?v ?x)) (:test (= ?subject-slot (?role 1 ?))) 
(verb ?verb ?infl (?subject-slot . ?slots) ?v ?v-sem) 
(modifiers post verb ? ?slots ?v ?gl ?g2 ?mod-sem)) 

The VP category takes seven arguments. The first is an inflection, which represents 
the tense of the verb. To describe the possibilities for this argument we need a quick 
review of some basic Unguistics. A sentence must have a finite verb, meaning a 
verb in the present or past tense. Thus, we say "Kim likes Lee," not "*Kim liking 
Lee." Subject-predicate agreement takes effect for finite verbs but not for any other 
tense. The other tenses show up as complements to other verbs. For example, the 
complement to "want" is an infinitive: "Kim wants to like Lee" and the complement 
to the modal auxiliary verb "would" is a nonf inite verb: "Kim would like Lee." If this 
were in the present tense, it would be "likes," not "like." The inflection argument 
takes on one of the forms in the table here: 

Expression Type Example 
(finite ?agr present) present tense eat, eats 
(finite ?agr past) past tense ate 
nonfinite nonfinite eat 
infinitive infinitive to eat 
-en past participle eaten 
-ing present participle eating 

The second argument is a metavariable that refers to the subject, and the third is 
the subject's complement slot. We adopt the convention that the subject slot must 
always be the first among the verb's complements. The other slots are handled by 
the postverb modifiers. The fourth argument is a metavariable indicating the verb 
phrase itself. The final three are the familiar gap and semantics arguments. As an 
example, if the verb phrase is the single word "slept," then the semantics of the verb 
phrase will be (and (past ?v) (sleep ?v)). Of course, adverbs, complements, 
and adjuncts will also be handled by this rule. 

The second rule for verb phrases handles auxiliary verbs, such as "have," "is" 
and "would." Each auxiliary verb (or aux) produces a verb phrase with a particular 


<a id='page-723'></a>

inflection when followed by a verb phrase with the required inflection. To repeat 
an example, "would" produces a finite phrase when followed by a nonfinite verb. 
"Have" produces a nonfinite when followed by a past participle. Thus, "would have 
liked" is a finite verb phrase. 

We also need to account for negation. The word "not" can not modify a bare main 
verb but can follow an auxiliary verb. That is, we can't say "*Kim not like Lee," but 
we can add an auxiliary to get "Kim does not like Lee." 

(rule (VP ?infl ?x ?subject-slot ?v ?gl ?g2 :sem) ==> 
(:ex "is sleeping" "would have given a bone to the dog." 
"did not sleep" "was given a bone by this old man") 

An aux verb, followed by a VP 
(aux ?infl ?needs-infl ?v ?aux) 
(modifiers post aux ? () ?v (gap nil) (gap nil) ?mod) 
(VP ?needs-infl ?x ?subject-slot ?v ?gl ?g2 ?vp)) 

(rule (adjunct post aux ? ?v ?gap ?gap (not ?v)) ==> 
(:word not)) 

21.6 Adverbs 
Adverbs can serve as adjuncts before or after a verb: "to boldly go," "to go boldly." 
There are some limitations on where they can occur, but it is difficult to come up 
with firm rules; here we allow any adverb anywhere. We define the category advp 
for adverbial phrase, but currently restrict it to a single adverb. 

(rule (adjunct ?pre/post verb ?info ?v ?gl ?g2 ?sem) ==> 
(advp ?wh ?v ?gl ?g2 ?sem)) 

(rule (advp ?wh ?v ?gap ?gap ?sem) ==> 
(adverb ?wh ?v ?sem)) 

(rule (advp ?wh ?v (gap (advp ?v)) (gap nil) t) ==> ) 

21.7 Clauses 
A clause consists of a subject followed by a predicate. However, the subject need not 
be realized immediately before the predicate. For example, in "Alice promised Bob 
to lend him her car" there is an infinitive clause that consists of the predicate "to lend 
him her car" and the subject "Alice." The sentence as a whole is another clause. In 


<a id='page-724'></a>

our analysis, then, a clause is a subject followed by a verb phrase, with the possibility 
that the subject will be instantiated by something from the gap arguments: 

(rule (clause ?infl ?x ?int-subj ?v ?gapl ?gap3 :sem) ==> 
(subject ?agr ?x ?subj-slot ?int-subj ?gapl ?gap2 ?subj-sem) 
(VP ?infl ?x ?subj-slot ?v ?gap2 ?gap3 ?pred-sem) 
(itest (subj-pred-agree ?agr ?infl))) 

There are now two possibilities for subject. In the first case it has already been 
parsed, and we pick it up from the gap list. If that is so, then we also need to find the 
agreement feature of the subject. If the subject was a noun phrase, the agreement will 
be present in the gap list. If it was not, then the agreement is third-person singular. 
An example of this is" That the Red Sox won surprises me," where the italicized phrase 
is a non-NP subject. The fact that we need to use "surprises" and not "surprise" 
indicates that it is third-person singular. We will see that the code (--->--) is used 
for this. 

(rule (subject ?agree ?x ?subj-slot ext-subj 
(gap ?subj) (gap nil) t) ==> 
Externally realized subject (the normal case for S) 
(rtest (slot-constituent ?subj-slot ?subj ?x ?) 

(if (= ?subj (NP ?agr ?case ?x)) 
(= ?agree ?agr) 
(= ?agree (-- + -))))) ;Non-NP subjects are 3sing 

In the second case we just parse a noun phrase as the subject. Note that the fourth 
argument to subject is either ext-subj or int-subj depending on if the subject is 
realized internally or externally. This will be important when we cover sentences in 
the next section. In case it was not already clear, the second argument to both clause 
and subject is the metavariable representing the subject. 

(rule (subject ?agr ?x (?role 1 (NP ?x)) int-subj ?gap ?gap ?sem) 
= => 
(NP ?agr (common nom) ?wh ?x (gap nil) (gap nil) ?sem)) 

Finally, the rules for subject-predicate agreement say that only finite predicates need 
to agree with their subject: 

(<- (subj-pred-agree ?agr (finite ?agr ?))) 
(<- (subj-pred-agree ? ?infl) (atom ?infl)) 


<a id='page-725'></a>
21.8 Sentences 
In the previous chapter we allowed only simple declarative sentences. The current 
grammar supports commands and four kinds of questions in addition to declarative 
sentences. It also supports thematic fronting: placing a nonsubject at the beginning of 
a sentence to emphasize its importance, as in "Smith he says his name is" or "Murder, 
she wrote" or "In God we trust." In the last example it is a prepositional phrase, not a 
noun phrase, that occurs first. It is also possible to have a subject that is not a noun 
phrase: "That the dog didn't hark puzzled Holmes." To support all these possibilities, 
we introduce a new category, XP, which stands for any kind of phrase. A declarative 
sentence is then just an XP followed by a clause, where the subject of the clause may 
or may not turn out to be the XP: 

(rule (S ?s :sem) ==> 

(:ex "Kim likes Lee" "Lee, I like _" "In god, we trust _" 

"Who likes Lee?" "Kim likes who?") 
(XP ?kind ?constituent ?wh ?x (gap nil) (gap nil) ?topic-sem) 
(clause (finite ? ?) ?x ? ?s (gap ?constituent) (gap nil) ?sem)) 

As it turns out, this rule also serves for two types of questions. The simplest kind 
of question has an interrogative noun phrase as its subject: "Who likes Lee?" or 
"What man likes Lee?" Another kind is the so-called echo question, which can be 
used only as a reply to another statement: if I tell you Kim likes Jerry Lewis, you 
could reasonably reply "Kim likes whoT Both these question types have the same 
structure as declarative sentences, and thus are handled by the same rule. 

The following table lists some sentences that can be parsed by this rule, showing 
the XP and subject of each. 

Sentence XP Subject 
Kim likes Lee Kim Kim 
Lee, Kim likes Lee Kim 
In god, we trust In god we 
That Kim likes Lee amazes That Kim likes Lee That Kim likes Lee 
Who likes Lee? Who Who 

The most common type of command has no subject at all: "Be quiet" or "Go to 
your room." When the subject is missing, the meaning is that the command refers 
toyou, the addressee of the command. The subject can also be mentioned explicitly, 
and it can be "you," as in "You be quiet," but it need not be: "Somebody shut the 
door" or "Everybody sing along." We provide a rule only for commands with subject 
omitted, since it can be difficult to distinguish a command with a subject from a 
declarative sentence. Note that commands are always nonfinite. 


<a id='page-726'></a>

(rule (S ?s :sem) ==> 

Commands have implied second-person subject 
(:ex "Give the dog a bone.") 
(:sem (command ?s)) 
(:sem (listener ?x)) 
(clause nonfinite ?x ext-subj ?s 

(gap (NP ? ? ?x)) (gap nil) ?sem)) 

Another form of command starts with "let," as in "Let me see what I can do" and 
"Let us all pray." The second word is better considered as the object of "let" rather 
than the subject of the sentence, since the subject would have to be "I" or "we." This 
kind of command can be handled with a lexical entry for "let" rather than with an 
additional rule. 

We now consider questions. Questions that can be answered by yes or no have 
the subject and auxiliary verb inverted: "Did you see him?" or "Should I have been 
doing this?" The latter example shows that it is only the first auxiliary verb that 
comes before the subject. The category a ux -i ..-S is used to handle this case: 

(rule (S ?s (yes-no ?s ?sem)) ==> 
(:ex "Does Kim like Lee?" "Is he a doctor?") 
(aux-inv-S nil ?s ?sem)) 

Questions that begin with a wh-phrase also have the auxihary verb before the subject, 
as in "Who did you see?" or "Why should I have been doing this?" The first 
constituent can also be a prepositional phrase: "For whom am I doing this?" The 
following rule parses an XP that must have the +wh feature and then parses an 
aux -i nv-S to arrive at a question: 

(rule (S ?s :sem) ==> 
(:ex "Who does Kim like _?" "To whom did he give it _? " 

"What dog does Kim like _?") 
(XP ?slot ?constituent +wh ?x (gap nil) (gap nil) ?subj-sem) 
(aux-inv-S ?constituent ?s ?sem)) 

A question can also be signaled by rising intonation in what would otherwise be a 
declarative statement: "You want some?" Since we don't have intonation information, 
we won't include this kind of question. 

The implementation for aux-inv-S is straightforward: parse an auxiliary and 
then a clause, pausing to look for modifiers in between. (So far, a "not" is the only 
modifier allowed in that position.) 


<a id='page-727'></a>

(rule (aux-inv-S ?constituent ?v :sem) ==> 
(:ex "Does Kim like Lee?" (who) "would Kim have liked") 
(aux (finite ?agr ?tense) ?needs-infl ?v ?aux-sem) 
(modifiers post aux ? () ?v (gap nil) (gap nil) ?mod) 
(clause ?needs-infl ?x int-subj ?v (gap ?constituent) (gap nil) 

?clause-sem)) 

There is one more case to consider. The verb "to be" is the most idiosyncratic in 
English. It is the only verb that has agreement differences for anything besides third-
person singular. And it is also the only verb that can be used in an a ux - i ..-S without 
a main verb. An example of this is "Is he a doctor?," where "is" clearly is not an 
auxihary, because there is no main verb that it could be auxiliary to. Other verb can 
not be used in this way: "*Seems he happy?" and"*Didtheyit?" are ungrammatical. 
The only possibiUty is "have," as in "Have you any wool?," but this use is rare. 

The following rule parses a verb, checks to see that it is a version of "be," and then 
parses the subject and the modifiers for the verb. 

(rule (aux-inv-S ?ext ?v :sem) ==> 
(:ex "Is he a doctor?") 
(verb ?be (finite ?agr ?) ((?role ?n ?xp) . ?slots) ?v ?sem) 
(rtest (word ?be be)) 
(subject ?agr ?x (?role ?n ?xp) int-subj 

(gap nil) (gap nil) ?subj-sem) 
(:sem (?role ?v ?x)) 
(modifiers post verb ? ?slots ?v (gap ?ext) (gap nil) ?mod-sem)) 

21.9 XPs 
All that remains in our grammar is the XP category. XPs are used in two ways: First, 
a phrase can be extraposed, as in "In god we trust," where "in god" will be parsed as 
an XP and then placed on the gap list until it can be taken off as an adjunct to "trust." 
Second, a phrase can be a complement, as in "He wants to be a fireman" where the 
infinitive phrase is a complement of "wants." 

As it turns out, the amount of information that needs to appear in a gap list 
is slightly different from the information that appears in a complement slot. For 
example, one sense of the verb "want" has the following complement list: 

((agt 1 (NP ?x)) (con 3 (VP infinitive ?x))) 

This says that the first complement (the subject) is a noun phrase that serves as the 
agent of the wanting, and the second is an infinitive verb phrase that is the concept of 


<a id='page-728'></a>

the wanting. The subject of this verb phrase is the same as the subject of the wanting, 
so in "She wants to go home," it is she who both wants and goes. (Contrast this to 
"He persuaded her to go home," where it is he that persuades, but she that goes.) 

But when we put a noun phrase on a gap list, we need to include its number and 
case as well as the fact that it is an NP and its metavariable, but we don't need to 
include the fact that it is an agent. This difference means we have two choices: either 
we can merge the notions of slots and gap lists so that they use a common notation 
containing all the information that either can use, or we need some way of mapping 
between them. I made the second choice, on the grounds that each notation was 
complicated enough without bringing in additional information. 

The relation slot-constituent maps between the slot notation used for complements 
and the constituent notation used in gap lists. There are eight types of 
complements, five of which can appear in gap lists: noun phrases, clauses, prepositional 
phrases, the word "it" (as in "it is raining"), and adverbial phrases. The three 
phrases that are allowed only as complements are verb phrases, particles (such as 
"up" in "look up the number"), and adjectives. Here is the mapping between the two 
notations. The *** indicates no mapping: 

(<- (slot-constituent (?role ?n (NP ?x)) 
(NP ?agr ?case ?x) ?x ?h)) 
(<- (slot-constituent (?role ?n (clause ?word ?infl)) 
(clause ?word ?infl ?v) ?v ?h)) 
(<- (slot-constituent (?role ?n (PP ?prep ?np)) 

(PP ?prep ?role ?np ?h) ?np ?h)) 
(<- (slot-constituent (?role ?n it) (it ? ? ?x) ?x ?)) 
(<- (slot-constituent (manner 3 (advp ?x)) (advp ?v) ? ?v)) 
(<- (slot-constituent (?role ?n (VP ?infl ?x)) *** ? ?)) 
(<- (slot-constituent (?role ?n (Adj ?x)) *** ?x ?)) 
(<- (slot-constituent (?role ?n (P ?particle)) *** ? ?)) 

We are now ready to define compi ement. It takes a slot descrption, maps it into a 
constituent, and then calls XP to parse that constituent: 

(rule (complement ?cat ?info (?role ?n ?xp) ?h ?gapl ?gap2 :sem) 

;; A complement is anything expected by a slot 
(:sem (?role ?h ?x)) 
(itest (slot-constituent (?role ?n ?xp) ?constituent ?x ?h)) 
(XP ?xp ?constituent ?wh ?x ?gapl ?gap2 ?sem)) 

The category XP takes seven arguments. The first two are the slot we are trying 
to fill and the constituent we need to fill it. The third is used for any additional 
information, and the fourth is the metavariable for the phrase. The last three supply 
gap and semantic information. 


<a id='page-729'></a>

Here are the first five XP categories: 

(rule (XP (PP ?prep ?np) (PP ?prep ?role ?np ?h) ?wh ?np 
?gapl ?gap2 ?sem) ==> 
(PP ?prep ?role ?wh ?np ?h ?gapl ?gap2 ?sem)) 

(rule (XP (NP ?x) (NP ?agr ?case ?x) ?wh ?x ?gapl ?gap2 ?sem) ==> 
(NP ?agr ?case ?wh ?x ?gapl ?gap2 ?sem)) 

(rule (XP it (it ? ? ?x) -wh ?x ?gap ?gap t) ==> 
(:word it)) 

(rule (XP (clause ?word ?infl) (clause ?word ?infl ?v) -wh ?v 

?gapl ?gap2 ?sem) ==> 
(:ex (he thinks) "that she is tall") 
(opt-word ?word) 

(clause ?infl ?x int-subj ?v ?gapl ?gap2 ?sem)) 

(rule (XP (?role ?n (advp ?v)) (advp ?v) ?wh ?v ?gapl ?gap2 ?sem) 

(advp ?wh ?v ?gapl ?gap2 ?sem)) 

The category opt-word parses a word, which may be optional. For example, one 

sense of "know" subcategorizes for a clause with an optional "that": we can say 

either "I know that he's here" or "I know he's here." The complement hst for "know" 

thuscontains the slot (con 2 (clause (that) (finite ? ?))). If the "that" had 

been obligatory, it would not have parentheses around it. 

(rule (opt-word ?word) ==> (:word ?word)) 
(rule (opt-word (?word)) ==> (iword ?word)) 
(rule (opt-word (?word)) ==>) 

Finally, here are the three XPs that can not be extraposed: 

(rule (XP (VP ?infl ?x) *** -wh ?v ?gapl ?gap2 ?sem) ==> 
(:ex (he promised her) "to sleep") 
(VP ?infl ?x ?subj-slot ?v ?gapl ?gap2 ?sem)) 

(rule (XP (Adj ?x) *** -wh ?x ?gap ?gap ?sem) ==> 
(Adj ?x ?sem)) 

(rule (XP (P ?particle) *** -wh ?x ?gap ?gap t) ==> 
(prep ?particle t)) 


<a id='page-730'></a>

21.10 Word Categories 
Each word category has a rule that looks words up in the lexicon and assigns the right 
features. The relation word is used for all lexicon access. We will describe the most 
complicated word class, verb, and just list the others. 

Verbs are complex because they often are polysemous - they have many meanings. 
In addition, each meaning can have several different complement lists. Thus, an 
entry for a verb in the lexicon will consist of the verb form, its inflection, and a list 
of senses, where each sense is a semantics followed by a list of possible complement 
lists. Here is the entry for the verb "sees," indicating that it is a present-tense verb with 
three senses. The understand sense has two complement lists, which correspond to 
"He sees" and "He sees that you are right." The 100 k sense has one complement list 
corresponding to "He sees the picture," and the dati ng sense, corresponding to "He 
sees her (only on Friday nights)," has the same complement list. 

> (?- (word sees verb ?infl ?senses)) 
?INFL = (FINITE (--+-) PRESENT) 
7SENSES = ((UNDERSTAND ((AGT 1 (NP ?3))) 

((EXP 1 (NP ?4)) 

(CON 2 (CLAUSE (THAT) (FINITE ?5 ?6))))) 
(LOOK ((AGT 1 (NP 17)) (OBJ 2 (NP ?8)))) 
(DATING ((AGT 1 (NP ?9)) (OBJ 2 (NP ?10))))) 

The category verb takes five arguments: the verb itself, its inflection, its complement 
list, its metavariable, and its semantics. The member relations are used to pick a sense 
from the list of senses and a complement Hst from the list of lists, and the semantics 
is built from semantic predicate for the chosen sense and the metavariable for the 
verb: 

(rule (verb ?verb ?infl ?slots ?v :sem) ==> 
(:word ?verb) 
(:test (word ?verb verb ?infl ?senses) 

(member (?sem . ?subcats) ?senses) 
(member ?slots ?subcats) 
(tense-sem ?infl ?v ?tense-sem)) 

(:sem ?tense-sem) 
(:sem (?sem ?v))) 

It is difficulty to know how to translate tense information into a semantic interpretation. 
Different applications will have different models of time and thus will want 
different interpretations. The relation tense-sem gives semantics for each tense. 
Here is a very simple definition of tense-sem: 


<a id='page-731'></a>
(<- (tense-sem (finite ? ?tense) ?v (?tense ?Y))) 
(<- (tense-sem -ing ?v (progressive ?v))) 
(<- (tense-sem -en ?v (past-participle ?v))) 
(<- (tense-sem infinitive ?v t)) 
(<- (tense-sem nonfinite ?v t)) 
(<- (tense-sem passive ?v (passive ?v))) 

Auxiliary verbs and modal verbs are listed separately: 

(rule (aux ?infl ?needs-infl ?v ?tense-sem) ==> 
(:word ?aux) 
(itest (word ?aux aux ?infl ?needs-infl) 

(tense-sem ?infl ?v ?tense-sem))) 

(rule (aux (finite ?agr ?tense) nonfinite ?v (?sem ?v)) ==> 
(:word ?modal) 
(:test (word ?modal modal ?sem ?tense))) 

Nouns, pronouns, and names are also listed separately, although they have much 
in common. For pronouns we use quantifier wh or pro, depending on if it is a wh-
pronoun or not. 

(rule (noun ?agr ?slots ?x (?sem ?x)) ==> 
(:word ?noun) 
(:test (word ?noun noun ?agr ?slots ?sem))) 

(rule (pronoun ?agr ?case ?wh ?x (?quant ?x (?sem ?x))) ==> 
(rword ?pro) 
(:test (word ?pro pronoun ?agr ?case ?wh ?sem) 

(if (= ?wh +wh) (= ?quant wh) (= ?quant pro)))) 

(rule (name ?agr ?name) ==> 
(iword ?name) 

(:test (word ?name name ?agr))) 

Here are the rules for the remaining word classes: 

(rule (adj ?x (?sem ?x)) ==> 
(:word ?adj) 
(:test (word ?adj adj ?sem))) 

(rule (adj ?x ((nth ?n) ?x)) ==> (ordinal ?n)) 

(rule (art ?agr ?quant) ==> 
(:word ?art) 
(:test (word ?art art ?agr ?quant))) 


<a id='page-732'></a>

(rule (prep ?prep t) ==> 
(:word ?prep) 
(:test (word ?prep prep))) 

(rule (adverb ?wh ?x ?sem) ==> 
(rword ?adv) 
(:test (word ?adv adv ?wh ?pred) 

(if (= ?wh +wh) 
(= ?sem (wh ?y (?pred ?x ?y))) 
(= ?sem (?pred ?x))))) 

(rule (cardinal ?n ?agr) ==> 
(:ex "five") 
(rword ?num) 
(rtest (word ?nuni cardinal ?n ?agr))) 

(rule (cardinal ?n ?agr) ==> 
(rex "5") 
(rword ?n) 
(rtest (numberp ?n) 

(if (= ?n 1) 
(= ?agr (-- + -)) ;3sing 
(= ?agr ( +))))) ;3plur 

(rule (ordinal ?n) ==> 
(rex "fifth") 
(rword ?num) 
(rtest (word ?num ordinal ?n))) 

21.11 The Lexicon 
The lexicon itself consists of a large number of entries in the word relation, and it 
would certainly be possible to ask the lexicon writer to make a long list of word facts. 
But to make the lexicon easier to read and write, we adopt three useful tools. First, 
we introduce a system of abbreviations. Common expressions can be abbreviated 
with a symbol that will be expanded by word. Second, we provide the macros verb 
and noun to cover the two most complex word classes. Third, we provide a macro 
word that makes entries into a hash table. This is more efficient than compiling a 
word relation consisting of hundreds of Prolog clauses. 

The implementation of these tools is left for the next section; here we show the 
actual lexicon, starting with the list of abbreviations. 

The first set of abbreviations defines the agreement features. The obvious way to 
handle agreement is with two features, one for person and one for number. So first-
person singular might be represented (1 si ng). A problem arises when we want 


<a id='page-733'></a>

to describe verbs. Every verb except "be" makes the distinction only between third-
person singular and all the others. We don't want to make five separate entries in the 
lexicon to represent all the others. One alternative is to have the agreement feature be 
a set of possible values, so all the others would be a single set of five values rather than 
five separate values. This makes a big difference in cutting down on backtracking. 
The problem with this approach is keeping track of when to intersect sets. Another 
approach is to make the agreement feature be a list of four binary features, one each 
for first-person singular, first-person plural, third-person singular, and third-person 
plural. Then "all the others" can be represented by the list that is negative in the third 
feature and unknown in all the others. There is no way to distinguish second-person 
singular from plural in this scheme, but English does not make that distinction. Here 
are the necessary abbreviations: 

(abbrev Ising (+---)) 
(abbrev Iplur (-+ - -)) 
(abbrev 3sing (--+-)) 
(abbrev Splur (---+)) 
(abbrev 2pers (--- -)) 
(abbrev ~3sing (??-?)) 

The next step is to provide abbreviations for some of the common verb complement 
lists: 

(abbrev v/intrans ((agt 1 (NP ?)))) 
(abbrev v/trans ((agt 1 (NP ?)) (obj 2 (NP ?)))) 
(abbrev v/ditrans ((agt 1 (NP ?)) (goal 2 (NP ?)) (obj 3 (NP ?)))) 
(abbrev v/trans2 ((agt 1 (NP ?)) (obj 2 (NP ?)) (goal 2 (PP to ?)))) 
(abbrev v/trans4 ((agt 1 (NP ?)) (obj 2 (NP ?)) (ben 2 (PP for ?)))) 
(abbrev v/it-null ((nil 1 it))) 
(abbrev v/opt-that ((exp 1 (NP ?)) (con 2 (clause (that) (finite ? ?))))) 
(abbrev v/subj-that ((con 1 (clause that (finite ? ?))) (exp 2 (NP ?)))) 
(abbrev v/it-that ((nil 1 it) (exp 2 (NP ?)) 

(con 3 (clause that (finite ? ?))))) 
(abbrev v/inf ((agt 1 (NP ?x)) (con 3 (VP infinitive ?x)))) 
(abbrev v/promise ((agt 1 (NP ?x)) (goal (2) (NP ?y)) 

(con 3 (VP infinitive ?x)))) 
(abbrev v/persuade ((agt 1 (NP ?x)) (goal 2 (NP ?y)) 

(con 3 (VP infinitive ?y)))) 
(abbrev v/want ((agt 1 (NP ?x)) (con 3 (VP infinitive ?x)))) 
(abbrev v/p-up ((agt 1 (NP ?)) (pat 2 (NP ?)) (nil 3 (P up)))) 
(abbrev v/pp-for ((agt 1 (NP ?)) (pat 2 (PP for ?)))) 
(abbrev v/pp-after ((agt 1 (NP ?)) (pat 2 (PP after ?)))) 


<a id='page-734'></a>

Verbs 

The macro verb allows us to list verbs in the form below, where the spellings of each 
tense can be omitted if the verb is regular: 

(verb (base past-tense past-participle present-participle present-plural) 
{semantics complement-list,..) ...) 

For example, in the following list "ask" is regular, so only its base-form spelling is 
necessary. "Do," on the other hand, is irregular, so each form is spelled out. The 
haphazard list includes verbs that are either useful for examples or illustrate some 
unusual complement list. 

(verb (ask) (query v/ditrans)) 
(verb (delete) (delete v/trans)) 
(verb (do did done doing does) (perform v/trans)) 
(verb (eat ate eaten) (eat v/trans)) 
(verb (give gave given giving) (give-1 v/trans2 v/ditrans) 

(donate v/trans v/intrans)) 
(verb (go went gone going goes)) 
(verb (have had had having has) (possess v/trans)) 
(verb (know knew known) (know-that v/opt-that) (know-of v/trans)) 
(verb (like) (like-1 v/trans)) 
(verb (look) (look-up v/p-up) (search v/pp-for) 

(take-care v/pp-after) (look v/intrans)) 
(verb (move moved moved moving moves) 

(self-propel v/intrans) (transfer v/trans2)) 
(verb (persuade) (persuade v/persuade)) 
(verb (promise) (promise v/promise)) 
(verb (put put put putting)) 
(verb (rain) (rain v/it-nulD) 
(verb (saw) (cut-with-saw v/trans v/intrans)) 
(verb (see saw seen seeing) (understand v/intrans v/opt-that) 

(look v/trans)(dating v/trans)) 
(verb (sleep slept) (sleep v/intrans)) 
(verb (surprise) (surprise v/subj-that v/it-that)) 
(verb (tell told) (tell v/persuade)) 
(verb (trust) (trust v/trans ((agt 1 (NP ?)) (obj 2 (PP in ?))))) 
(verb (try tried tried trying tries) (attempt v/inf)) 
(verb (visit) (visit v/trans)) 
(verb (want) (desire v/want v/persuade)) 


<a id='page-735'></a>

Auxiliary Verbs 

Auxiliary verbs are simple enough to be described directly with the word macro. Each 
entry lists the auxiliary itself, the tense it is used to construct, and the tense it must 
be followed by. The auxiliaries "have" and "do" are listed, along with "to," which is 
used to construct infinitive clauses and thus can be treated as if it were an auxiliary. 

(word have aux nonfinite -en) 
(word have aux (finite ~3sing present) -en) 
(word has aux (finite 3sing present) -en) 
(word had aux (finite ? past) -en) 
(word having aux -ing -en) 

(word do aux (finite ~3sing present) nonfinite) 
(word does aux (finite 3sing present) nonfinite) 
(word did aux (finite ? past) nonfinite) 

(word to aux infinitive nonfinite) 

The auxiliary "be" is special: in addition to its use as both an auxiliary and main 
verb, it also is used in passives and as the main verb in aux-inverted sentences. The 
function copul a is used to keep track of all these uses. It will be defined in the next 
section, but you can see it takes two arguments, a list of senses for the main verb, and 
a list of entries for the auxiliary verb. The three senses correspond to the examples 
"He is a fool," "He is a Republican," and "He is in Indiana," respectively. 

(copula 

'((nil ((nil 1 (NP ?x)) (nil 2 (Adj ?x)))) 
(is-a ((exp 1 (NP ?x)) (arg2 2 (NP ?y)))) 
(is-loc ((exp 1 (NP ?x)) (?prep 2 (PP ?prep ?))))) 

'((be nonfinite -ing) 
(been -en -ing) 
(being -ing -en) 
(am (finite Ising present) -ing) 
(is (finite 3sing present) -ing) 
(are (finite 2pers present) -ing) 
(were (finite (--??) past) -ing) ; 2nd sing or pi 
(was (finite (?-?-) past) -ing))) ; 1st or 3rd sing 

Following are the modal auxiliary verbs. Again, it is difficult to specify semantics 
for them. The word "not" is also listed here; it is not an auxiliary, but it does modify 
them. 


<a id='page-736'></a>

(word can modal able past) 
(word could modal able present) 
(word may modal possible past) 
(word might modal possible present) 
(word shall modal mandatory past) 
(word should modal mandatory present) 
(word will modal expected past) 
(word would modal expected present) 
(word must modal necessary present) 

(word not not) 

Nouns 

No attempt has been made to treat nouns seriously. We list enough nouns here to 
make some of the examples work. The first noun shows a complement list that is 
sufficient to parse "the destruction of the city by the enemy." 

(noun destruction * destruction 

(pat (2) (PP of ?)) (agt (2) (PP by ?))) 
(noun beach) 
(noun bone) 
(noun box boxes) 
(noun city cities) 
(noun color) 
(noun cube) 
(noun doctor) 
(noun dog dogs) 
(noun enemy enemies) 
(noun file) 
(noun friend friends friend (friend-of (2) (PP of ?))) 
(noun furniture *) 
(noun hat) 
(noun man men) 
(noun saw) 
(noun woman women) 

Pronouns 

Here we list the nominative, objective, and genitive pronouns, followed by interrogative 
and relative pronouns. The only thing missing are reflexive pronouns, such as 
"myself." 


<a id='page-737'></a>

(word I pronoun Ising (common nom) -wh speaker) 
(word we pronoun Iplur (common nom) -wh speaker+other) 
(word you pronoun 2pers (common ?) -wh 1istener) 
(word he pronoun 3sing (common nom) -wh male) 
(word she pronoun 3s ing (common nom) -wh female) 
(word it pronoun 3s ing (common ?) -wh anything) 
(word they pronoun 3plur (common nom) -wh anything) 

(word me pronoun Ising (common obj) -wh speaker) 
(word us pronoun Iplur (common obj) -wh speaker+other) 
(word him pronoun 3sing (common obj) -wh male) 
(word her pronoun 3sing (common obj) -wh female) 
(word them pronoun 3plur (common obj) -wh anything) 

(word my pronoun Ising gen -wh speaker) 
(word our pronoun Iplur gen -wh speaker+other) 
(word your pronoun 2pers gen -wh 1istener) 
(word his pronoun 3sing gen -wh male) 
(word her pronoun 3sing gen -wh female) 
(word its pronoun 3s ing gen -wh anything) 
(word their pronoun 3plur gen -wh anything) 
(word whose pronoun 3sing gen +wh anything) 

(word who pronoun ? (common ?) +wh person) 
(word whom pronoun ? (common obj) +wh person) 
(word what pronoun ? (common ?) +wh thing) 
(word which pronoun ? (common ?) +wh thing) 

(word who rel-pro ? person) 
(word which rel-pro ? thing) 
(word that rel-pro ? thing) 
(word whom rel-pro (common obj) person) 

Names 

The following names were convenient for one example or another: 

(word God name 3sing) (word Lynn name 3sing) 
(word Jan name 3sing) (word Mary name 3sing) 
(word John name 3sing) (word NY name 3sing) 
(word Kim name 3sing) (word LA name 3sing) 
(word Lee name 3sing) (word SF name 3sing) 


<a id='page-738'></a>

Adjectives 

Here are a few adjectives: 

(word big adj big) (word bad adj bad) 
(word old adj old) (word smart adj smart) 
(word green adj green) (word red adj red) 
(word tal l adj tall ) (word fun adj fun) 

Adverbs 

The adverbs covered here include interrogatives: 

(word quickly adv -wh quickly) 
(word slowly adv -wh slowly) 

(word where adv +wh loc) 
(word when adv +wh time) 
(word why adv +wh reason) 
(word how adv +wh manner) 

Articles 

The common articles are listed here: 

