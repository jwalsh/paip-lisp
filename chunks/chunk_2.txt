&#9635; Exercise 5.14 [m]
of argument lists. 
Define a version of ma ppend that, like ma pea r, accepts any number 

&#9635; Exercise 5.15 [m] Give an informal proof that segment-match always terminates. 

&#9635; Exercise 5.16 [s] Trick question: There is an object in Lisp which, when passed to 
vari abl e- p, results in an error. What is that object? 

&#9635; Exercise 5.17 [m] The current version of ELIZA takes an input, transforms it according 
to the first applicable rule, and outputs the result. One can also imagine a 
system where the input might be transformed several times before the final output 
is printed. Would such a system be more powerful? If so, in what way? 

&#9635; Exercise 5.18 [h] Read Weizenbaum's original article on ELIZA and transpose his 
list of rules into the notation used in this chapter. 


<a id='page-170'></a>

5.7 Answers 
Answer 5.1 No. If either the pattern or the input were shorter, but matched every 
existing element, the every expression would incorrectly return true. 

(every #'pat-match *(a b c) '(a)) . 

Furthermore, if either the pattern or the input were a dotted list, then the result of the 
every would be undefined—some implementations might signal an error, and others 
might just ignore the expression after the dot. 

(every #'pat-match '(a b . c) '(a b . d)) => T, NIL.orerror. 

Answer 5.4 The expression don't may look like a single word, but to the Lisp reader 
it is composed of the two elements don and ' t, or (quote t). If these elements are 
used consistently, they will match correctly, but they won't print quite right—there 
will be a space before the quote mark. In fact the :pretty t argument to write is 
specified primarily to make (quote t) print as ' t (See [page 559](chapter16.md#page-559) of Steele's Common 
Lisp the Language, 2d edition.) 

Answer 5.5 One way to do this is to read a whole line of text with read -line rather 
than read. Then, substitute spaces for any punctuation character in that string. 
Finally, wrap the string in parentheses, and read it back in as a list: 

(defun read-line-no-punct () 

"Read an input line, ignoring punctuation." 

(read-from-string 

(concatenate 'string "(" (substitute-if #\space#'punctuation-p 

(read-line)) 

")"))) 

(defun punctuation-p (char) (find char ".,;:*l?#-()\\\"")) 

This could also be done by altering the readtable, as in section 23.5, [page 821](chapter23.md#page-821). 


<a id='page-171'></a>

Answer 5.6 

(defun eliza () 
"Respond to user input using pattern matching rules." 
(loop 

(print *eliza>) 

(let* ((input (read-line-no-punct)) 

(response (flatten (use-eliza-rules input)))) 
(print-with-spaces response) 
(if (equal response '(good bye)) (RETURN))))) 

(defun print-with-spaces (list) 
(mapc #'(lambda (x) (prinl x) (princ " ")) list)) 

or 

(defun print-with-spaces (list) 
(format t "~{~a ~}" list)) 

Answer 5.10 Hint: consider (si mpl e-equal '() '(nil . nil)). 

Answer 5.14 

(defun mappend (fn &rest list) 
"Apply fn to each element of lists and append the results." 
(apply #'append (apply #'mapcar fn lists))) 

Answer 5.16 It must bea symbol, because for nonsymbols, variable-p just returns 
nil. Getting the symbol - name of a symbol is just accessing a slot, so that can't cause 
an error. The only thing left is el t; if the symbol name is the empty string, then 
accessing element zero of the empty string is an error. Indeed, there is a symbol 
whose name is the empty string: the symbol. 

Answer 5.17 Among other things, a recursive transformation system could be used 
to handle abbreviations. That is,a form like "don't" could be transformed into "do 
not" and then processed again. That way, the other rules need only work on inputs 
matching "do not." 


<a id='page-172'></a>

Answer 5.19 The following includes most of Weizenbaum's rules: 

(defparameter *eliza-rules* 
'((((?* ?x) hello (?* ?y)) 
(How do you do. Please state your problem.)) 

(((?* ?x) computer (?* ?y)) 
(Do computers worry you?) (What do you think about machines?) 
(Why do you mention computers?) 
(What do you think machines have to do with your problem?)) 

(((?* ?x) name (?* ?y)) 
(I am not interested in names)) 

(((?* ?x) sorry (?* ?y)) 
(Please don't apologize) (Apologies are not necessary) 
(What feelings do you have when you apologize)) 

(((?* ?x) I remember (?* ?y)) 
(Do you often think of ?y) 
(Does thinking of ?y bring anything else to mind?) 
(What else do you remember) (Why do you recall ?y right now?) 
(What in the present situation reminds you of ?y) 
(What is the connection between me and ?y)) 

(((?* ?x) do you remember (?* ?y)) 
(Did you think I would forget ?y ?) 
(Why do you think I should recall ?y now) 
(What about ?y) (You mentioned ?y)) 

(((?* ?x) if (?* ?y)) 
(Do you really think its likely that ?y) (Do you wish that ?y) 
(What do you think about ?y) (Really-- if ?y)) 

(((?* ?x) I dreamt (?* ?y)) 
(Really-- ?y) (Have you ever fantasized ?y while you were awake?) 
(Have you dreamt ?y before?)) 

(((?* ?x) dream about (?* ?y)) 
(How do you feel about ?y in reality?)) 

(((?* ?x) dream (?* ?y)) 
(What does this dream suggest to you?) (Do you dream often?) 
(What persons appear in your dreams?) 
(Don't you believe that dream has to do with your problem?)) 

(((?* ?x) my mother (?* ?y)) 
(Who else in your family ?y) (Tell me more about your family)) 

(((?* ?x) my father (?* ?y)) 
(Your father) (Does he influence you strongly?) 
(What else comes to mind when you think of your father?)) 


<a id='page-173'></a>

(((?* ?x) I want (?* ?y)) 
(What would it mean if you got ?y) 
(Why do you want ?y) (Suppose you got ?y soon)) 

(((?* ?x) I am glad (?* ?y)) 
(How have I helped you to be ?y) (What makes you happy just now) 
(Can you explain why you are suddenly ?y)) 

(((?* ?x) I am sad (?* ?y)) 
(I am sorry to hear you are depressed) 
(I'm sure it's not pleasant to be sad)) 

(((?* ?x) are like (?* ?y)) 
(What resemblance do you see between ?x and ?y)) 

(((?* ?x) is like (?* ?y)) 
(In what way is it that ?x is like ?y) 
(What resemblance do you see?) 
(Could there really be some connection?) (How?)) 

(((?* ?x) alike (?* ?y)) 
(In what way?) (What similarities are there?)) 
(((?* ?x) same (?* ?y)) 
(What other connections do you see?)) 

(((?* ?x) I was (?* ?y)) 
(Were you really?) (Perhaps I already knew you were ?y) 
(Why do you tell me you were ?y now?)) 

(((?* ?x) was I (?* ?y)) 
(What if you were ?y ?) (Do you think you were ?y) 
(What would it mean if you were ?y)) 

(((?* ?x) I am (?* ?y)) 
(In what way are you ?y) (Do you want to be ?y ?)) 

(((?* ?x) am I (?* ?y)) 
(Do you believe you are ?y) (Would you want to be ?y) 
(You wish I would tell you you are ?y) 
(What would it mean if you were ?y)) 

(((?* ?x) am (?* ?y)) 
(Why do you say "AM?") (I don't understand that)) 

(((?* ?x) are you (?* ?y)) 
(Why are you interested in whether I am ?y or not?) 
(Would you prefer if I weren't ?y) 
(Perhaps I am ?y in your fantasies)) 

(((?* ?x) you are (?* ?y)) 
(What makes you think I am ?y ?)) 


<a id='page-174'></a>

(((?* ?x) because (?* ?y)) 
(Is that the real reason?) (What other reasons might there be?) 
(Does that reason seem to explain anything else?)) 

(((?* ?x) were you (?* ?y)) 
(Perhaps I was ?y) (What do you think?) (What if I had been ?y)) 
(((?* ?x) I can't (?* ?y)) 
(Maybe you could ?y now) (What if you could ?y ?)) 
(((?* ?x) I feel (?* ?y)) 
(Do you often feel ?y ?)) 
(((?* ?x) I felt (?* ?y)) 
(What other feelings do you have?)) 
(((?* ?x) I (?* ?y) you (?* ?z)) 
(Perhaps in your fantasy we ?y each other)) 

(((?* ?x) why don't you (?* ?y)) 
(Should you ?y yourself?) 
(Do you believe I don't ?y) (Perhaps I will ?y in good time)) 

(((?* ?x) yes (?* ?y)) 
(You seem quite positive) (You are sure) (I understand)) 

(((?* ?x) no (?* ?y)) 
(Why not?) (You are being a bit negative) 
(Are you saying "NO" just to be negative?)) 

(((?* ?x) someone (?* ?y)) 
(Can you be more specific?)) 

(((?* ?x) everyone (?* ?y)) 
(surely not everyone) (Can you think of anyone in particular?) 
(Who for example?) (You are thinking of a special person)) 

(((?* ?x) always (?* ?y)) 
(Can you think of a specific example) (When?) 
(What incident are you thinking of?) (Really-- always)) 

(((?* ?x) what (?* ?y)) 
(Why do you ask?) (Does that question interest you?) 
(What is it you really want to know?) (What do you think?) 
(What comes to your mind when you ask that?)) 

(((?* ?x) perhaps (?* ?y)) 
(You do not seem quite certain)) 

(((?* ?x) are (?* ?y)) 
(Did you think they might not be ?y) 
(Possibly they are ?y)) 

(((?* ?x)) 
(Very interesting) (I am not sure I understand you fully) 
(What does that suggest to you?) (Please continue) (Go on) 
(Do you feel strongly about discussing such things?)))) 


## Chapter 6
<a id='page-175'></a>

Building Software Tools 

Man is a tool-using animal 
Without tools he is nothing 
with tools he is all 

-Thomas Carlyle (1795-. 881) 

I
I
n chapters 4 and 5 we were concerned with building two particular programs, GPS and ELIZA. 
In this chapter, we will reexamine those two programs to discover some common patterns. 
Those patterns will be abstracted out to form reusable software tools that will prove helpful 
in subsequent chapters. 

6.1 An Interactive Interpreter Tool 
The structure of the function el i za is a common one. It is repeated below: 

(defun eliza () 
"Respond to user input using pattern matching rules." 
(loop 

(print 'eliza>) 

(print (flatten (use-eliza-rules (read)))))) 


<a id='page-176'></a>

Many other appHcations use this pattern, including Lisp itself. The top level of Lisp 
could be defined as: 

(defun lisp () 

(loop 

(print '>) 

(print (eval (read))))) 

The top level of a Lisp system has historically been called the "read-eval-print loop." 
Most modern Lisps print a prompt before reading input, so it should really be called 
the "prompt-read-eval-print loop," but there was no prompt in some early systems 
like MacLisp, so the shorter name stuck. If we left out the prompt, we could write a 
complete Lisp interpreter using just four symbols: 

(loop (print (eval (read)))) 

It may seem facetious to say those four symbols and eight parentheses constitute a 
Lisp interpreter. When we write that line, have we really accomplished anything? 
One answer to that question is to consider what we would have to do to write a Lisp 
(or Pascal) interpreter in Pascal. We would need a lexical analyzer and a symbol table 
manager. This is a considerable amount of work, but it is all handled by read. We 
would need a syntactic parser to assemble the lexical tokens into statements, read 
also handles this, but only because Lisp statements have trivial syntax: the syntax 
of lists and atoms. Thus read serves fine as a syntactic parser for Lisp, but would 
fail for Pascal. Next, we need the evaluation or interpretation part of the interpreter; 
eval does this nicely, and could handle Pascal just as well if we parsed Pascal syntax 
into Lisp expressions, print does much less work than read or eval, but is still 
quite handy. 

The important point is not whether one line of code can be considered an implementation 
of Lisp; it is to recognize common patterns of computation. Both el iza 
and lisp can be seen as interactive interpreters that read some input, transform or 
evaluate the input in some way, print the result, and then go back for more input. We 
can extract the following common pattern: 

(defun program () 

(loop 
(print prompt) 
(print (transform (read))))) 

There are two ways to make use of recurring patterns like this: formally and informally. 
The informal alternative is to treat the pattern as a cliche or idiom that will 
occur frequently in our writing of programs but will vary from use to use. When we 


<a id='page-177'></a>
want to write a new program, we remember writing or reading a similar one, go back 
and look at the first program, copy the relevant sections, and then modify them for 
the new program. If the borrowing is extensive, it would be good practice to insert 
a comment in the new program citing the original, but there would be no "official" 
connection between the original and the derived program. 

The formal alternative is to create an abstraction, in the form of functions and perhaps 
data structures, and refer explicitly to that abstraction in each new application— 
in other words, to capture the abstraction in the form of a useable software tool. The 
interpreter pattern could be abstracted into a function as follows: 

(defun interactive-interpreter (prompt transformer) 
"Read an expression, transform it, and print the result." 
(loop 

(print prompt) 

(print (funcall transformer (read))))) 

This function could then be used in writing each new interpreter: 

(defun lisp () 
(interactive-interpreter *> #'eval)) 

(defun eliza () 
(interactive-interpreter 'eliza> 
#'(lambda (x) (flatten (use-eliza-rules x))))) 

Or, with the help of the higher-order function compose: 

(defun compose (f g) 
"Return the function that computes (f (g x))." 
#'(lambda (x) (funcall f (funcall g x)))) 

(defun eliza () 
(i nteracti ve-i nterpreter 'el iza> 
(compose #'flatten #'use-eliza-rules))) 

There are two differences between the formal and informal approaches. First, they 
look different. If the abstraction is a simple one, as this one is, then it is probably 
easier to read an expression that has the loop explicitly written out than to read one 
that calls interact! ve-i nterpreter, since that requires finding the definition of 
i nteracti ve - i nterpreter and understanding it as well. 

The other difference shows up in what's called maintenance. Suppose we find a 
missing feature in the definition of the interactive interpreter. One such omission is 
that the 1 oop has no exit. I have been assuming that the user can terminate the loop by 
hitting some interrupt (or break, or abort) key. A cleaner implementation would allow 


<a id='page-178'></a>

the user to give the interpreter an explicit termination command. Another useful 
feature would be to handle errors within the interpreter. If we use the irrformal 
approach, then adding such a feature to one program would have no effect on the 
others. Butif we use the formal approach, then improving i nteracti ve- i nterpreter 
would automatically bring the new features to all the programs that use it. 

The following version of i nteracti ve- i nterpreter adds two new features. First, 
it uses the macro handler-case^ to handle errors. This macro evaluates its first 
argument, and normally just returns that value. However, if an error occurs, the 
subsequent arguments are checked for an error condition that matches the error that 
occurred. In this use, the case error matches all errors, and the action taken is to 
print the error condition and continue. 

This version also allows the prompt to be either a string or a function of no 
arguments that will be called to print the prompt. The function prompt-generator, 
for example, returns a function that will print prompts of the form [1], C2], and 
so forth. 

(defun interactive-interpreter (prompt transformer) 
"Read an expression, transform it, and print the result." 
(loop 

(handler-case 
(progn 

(if (stringp prompt) 
(print prompt) 
(funcall prompt)) 

(print (funcall transformer (read)))) 
In case of error, do this: 
(error (condition) 
(format t "'^&;; Error ~a ignored, back to top level. " 
condition))))) 

(defun prompt-generator (&optional (num 0) (ctl-string "C^d] ")) 
"Return a function that prints prompts like [1]. C2]. etc." 
#*(lambda () (format t ctl-string (incf num)))) 

6.2 A Pattern-Matching Tool 
The pat-match function was a pattern matcher defined specifically for the ELIZA 
program. Subsequent programs will need pattern matchers too, and rather than 
write specialized matchers for each new program, it is easier to define one general 

^The macro hand! er-case is only in ANSI Common Lisp. 


<a id='page-179'></a>
pattern matcher that can serve most needs, and is extensible in case novel needs 
come up. 

The problem in designing a "general" tool is deciding what features to provide. 
We can try to define features that might be useful, but it is also a good idea to make 
the list of features open-ended, so that new ones can be easily added when needed. 

Features can be added by generalizing or specializing existing ones. For example, 
we provide segment variables that match zero or more input elements. We can 
specialize this by providing for a kind of segment variable that matches one or more 
elements, or for an optional variable that matches zero or one element. Another 
possibility is to generalize segment variables to specify a match of m to . elements, for 
any specified m and n. These ideas come from experience with notations for writing 
regular expressions, as well as from very general heuristics for generalization, such 
as "consider important special cases" and "zero and one are likely to be important 
special cases." 

Another useful feature is to allow the user to specify an arbitrary predicate that 
a match must satisfy. The notation (?i s ?n numberp) could be used to match any 
expression that is a number and bind it to the variable ?n. This would look like: 

> (pat-match '(x = (?is ?n numberp)) '(x = 34)) => ((?n . 34)) 

> (pat-match '(x = (?is ?n numberp)) '(x = x)) => NIL 

Since patterns are like boolean expressions, it makes sense to allow boolean operators 
on them. Following the question-mark convention, we will use ?and, ?or and ?not 
for the operators.^ Here is a pattern to match a relational expression with one of three 
relations. It succeeds because the < matches one of the three possibiUties specified 
by(?or < = >). 

> (pat-match '(?x (?or < = >) ?y) *(3 < 4)) => ((?Y . 4) (?X . 3)) 

Here is an example of an ?and pattern that checks if an expression is both a number 
and odd: 

> (pat-match *(x = (?and (?is ?n numberp) (?is ?n oddp))) 
'(x = 3)) 
((?N . 3)) 

^An alternative would be to reserve the question mark for variables only and use another 
notation for these match operators. Keywords would be a good choice, such as : and,: or,: i s, 
etc. 


<a id='page-180'></a>

The next pattern uses ?not to insure that two parts are not equal: 

> (pat-match '(?x /= (?not ?x)) '(3 /= 4)) => ((?X . 3)) 

The segment matching notation we have seen before. It is augmented to allow for 
three possibilities: zero or more expressions; one or more expressions; and zero or 
one expressions. Finally, the notation (?if exp) can be used to test a relationship 
between several variables. It has to be Usted as a segment pattern rather than a single 
pattern because it does not consume any of the input at all: 

> (pat-match '(?x > ?y (?if (> ?x ?y))) '(4 > 3)) => 
((?Y . 3) (?X . 4)) 

When the description of a problem gets this complicated, it is a good idea to 
attempt a more formal specification. The following table describes a grammar of 
patterns, using the same grammar rule format described in chapter 2. 

pat => var match any one expression 

constant match just this atom 

segment-pat match something against a sequence 

single-pat match something against one expression 

(pat. pat) match the first and the rest 
single-pat =4^ (lis var predicate) test predicate on one expression 
(lor pat...) match any pattern on one expression 
(?andpai...) match every pattern on one expression 
(?not pat...) succeed if pattern(s) do not match 
segment-pat ((
l*var)
(d+var)
((V.var)
((?if 
...) 
...) 
...) 
exp)...) 
match zero or more expressions 
match one or more expressions 
match zero or one expression 
test if exp (which may contain 
variables) is true 

. chars a symbol starting with ? 

var-

atom any nonvariable atom 

constant 


Despite the added complexity, all patterns can still be classified into five cases. 
The pattern must be either a variable, constant, a (generalized) segment pattern, 
a (generalized) single-element pattern, or a cons of two patterns. The following 
definition of pat -match reflects the five cases (along with two checks for failure): 


<a id='page-181'></a>
(defun pat-match (pattern input &optional (bindings no-bindings)) 
"Match pattern against input in the context of the bindings" 
(cond ((eq bindings fail) fail) 

((variable-p pattern) 

(match-variable pattern input bindings)) 
((eql pattern input) bindings) 
((segment-pattern-p pattern) 

(segment-matcher pattern input bindings)) 
((single-pattern-p pattern) ; *** 
(single-matcher pattern input bindings)) ; *** 
((and (consp pattern) (consp input)) 
(pat-match (rest pattern) (rest input) 
(pat-match (first pattern) (first input) 
bindings))) 
(t fail))) 

For completeness, we repeat here the necessary constants and low-level functions 
from ELIZA: 

(defconstant fail nil "Indicates pat-match failure") 

(defconstant no-bindings '((t . t)) 
"Indicates pat-match success, with no variables.") 

(defun variable-p (x) 
"Is . a variable (a symbol beginning with '?*)?" 
(and (symbolp x) (equal (char (symbol-name x) 0) #\?))) 

(defun get-binding (var bindings) 
"Find a (variable . value) pair in a binding list. " 
(assoc var bindings)) 

(defun binding-var (binding) 
"Get the variable part of a single binding." 
(car binding)) 

(defun binding-val (binding) 
"Get the value part of a single binding." 
(cdr binding)) 

(defun make-binding (var val) (cons var val)) 

(defun lookup (var bindings) 
"Get the value part (for var) from a binding list. " 
(binding-val (get-binding var bindings))) 


<a id='page-182'></a>

(defun extend-bindings (var val bindings) 

"Add a (var . value) pair to a binding list." 

(cons (make-binding var val) 

Once we add a "real" binding, 

we can get rid of the dummy no-bindings 

(if (eq bindings no-bindings) 

nil 

bindings) 

(defun match-variable (var input bindings) 

"Does VAR match input? Uses (or updates) and returns bindings." 

(let ((binding (get-binding var bindings))) 

(cond ((not binding) (extend-bindings var input bindings)) 

((equal input (binding-val binding)) bindings) 

(t fail)))) 

The next step is to define the predicates that recognize generalized segment and 
single-element patterns, and the matching functions that operate on them. We could 
implementsegment-matcherandsingle-matcherwithcasestatementsthatconsider 
all possible cases. However, that would make it difficult to extend the matcher. A 
programmer who wanted to add a new kind of segment pattern would have to edit 
the definitions of both segment-pattern-p and segment-matcher to install the new 
feature. This by itself may not be too bad, but consider what happens when two 
programmers each add independent features. If you want to use both, then neither 
version of segment-matcher (or segment-pattern-p) will do. You'll have to edit the 
functions again, just to merge the two extensions. 

The solution to this dilemma is to write one version of segment-pattern-p and 
segment-matcher, once and for all, but to have these functions refer to a table of 
pattern/action pairs. The table would say "if you see ?* in the pattern, then use 
the function segment-match," and so on. Then programmers who want to extend 
the matcher just add entries to the table, and it is trivial to merge different extensions 
(unless of course two programmers have chosen the same symbol to mark 
different actions). 

This style of programming, where pattern/action pairs are stored in a table, is 
called data-driven programming. It is a very flexible style that is appropriate for writing 
extensible systems. 

There are many ways to implement tables in Conunon Lisp, as discussed in 
section 3.6, [page 73](chapter3.md#page-73). In this case, the keys to the table will be symbols (like ?*), 
and it is fine if the representation of the table is distributed across memory. Thus, 
property lists are an appropriate choice. We will have two tables, represented by 
the segment-match property and the si ngl e-match property of symbols like ?*. The 
value of each property will be the name of a function that implements the match. 
Here are the table entries to implement the granunar listed previously: 


<a id='page-183'></a>
(setf (get '?is 'single-match) 'match-is) 
(setf (get '?or 'single-match) 'match-or) 
(setf (get '?and 'single-match) 'match-and) 
(setf (get '?not 'single-match) 'match-not) 

(setf (get '? * 'segment-match) 'segment-match) 
(setf (get '?+ 'segment-match) 'segment-match+) 
(setf (get '?? 'segment-match) 'segment-match?) 
(setf (get '?if 'segment-match) 'match-if) 

With the table defined, we need to do two things. First, define the "glue" that holds 
the table together: the predicates and action-taking functions. A function that looks 
upadata-driven function and calls it (such as segment-matcher and single-matcher) 
is called a dispatch function. 

(defun segment-pattern-p (pattern) 
"Is this a segment-matching pattern like ((?* var) . pat)?" 
(and (consp pattern) (consp (first pattern)) 

(symbolp (first (first pattern))) 
(segment-match-fn (first (first pattern))))) 

(defun single-pattern-p (pattern) 
"Is this a single-matching pattern? 

E.g. (?is X predicate) (?and . patterns) (?or . patterns)." 
(and (consp pattern) 
(single-match-fn (first pattern)))) 

(defun segment-matcher (pattern input bindings) 
"Call the right function for this kind of segment pattern." 
(funcall (segment-match-fn (first (first pattern))) 

pattern input bindings)) 

(defun single-matcher (pattern input bindings) 
"Call the right function for this kind of single pattern." 
(funcall (single-match-fn (first pattern)) 

(rest pattern) input bindings)) 

(defun segment-match-fn (x) 
"Get the segment-match function for x. 
if it is a symbol that has one." 
(when (symbolp x) (get . 'segment-match))) 

(defun single-match-fn (x) 
"Get the single-match function for x, 
if it is a symbol that has one." 
(when (symbolp x) (get . 'single-match))) 


<a id='page-184'></a>

The last thing to do is define the individual matching functions. First, the single-
pattern matching functions: 

(defun match-is (var-and-pred input bindings) 
"Succeed and bind var if the input satisfies pred, 
where var-and-pred is the list (var pred)." 
(let* ((var (first var-and-pred)) 

(pred (second var-and-pred)) 
(new-bindings (pat-match var input bindings))) 
(if (or (eq new-bindings fail) 

(not (funcall pred input))) 
fail 
new-bindings))) 

(defun match-and (patterns input bindings) 
"Succeed if all the patterns match the input." 
(cond ((eq bindings fail) fail) 

((null patterns) bindings) 
(t (match-and (rest patterns) input 
(pat-match (first patterns) input 
bindings))))) 

(defun match-or (patterns input bindings) 
"Succeed if any one of the patterns match the input." 
(if (null patterns) 

fail 
(let ((new-bindings (pat-match (first patterns) 
input bindings))) 

(if (eq new-bindings fail) 
(match-or (rest patterns) input bindings) 
new-bindings)))) 

(defun match-not (patterns input bindings) 
"Succeed if none of the patterns match the input. 
This will never bind any variables." 
(if (match-or patterns input bindings) 

fail 
bindings)) 

Now the segment-pattern matching functions, segment-match is similar to the version 
presented as part of ELIZA. The difference is in how we determine pos, the 
position of the first element of the input that could match the next element of the 
pattern after the segment variable. In ELIZA, we assumed that the segment variable 
was either the last element of the pattern or was followed by a constant. In the 
following version, we allow nonconstant patterns to follow segment variables. The 
function first -match - pos is added to handle this. If the following element is in fact 
a constant, the same calculation is done using posi ti on. If it is not a constant, then 


<a id='page-185'></a>
we just return the first possible starting position—unless that would put us past the 
end of the input, in which case we return nil to indicate failure: 

(defun segment-match (pattern input bindings &optional (start 0)) 
"Match the segment pattern ((?* var) . pat) against input." 
(let ((var (second (first pattern))) 

(pat (rest pattern))) 

(if (null pat) 
(match-variable var input bindings) 
(let ((pos (first-match-pos (first pat) input start))) 

(if (null pos) 
fail 
(let ((b2 (pat-match 

pat (subseq input pos) 
(match-variable var (subseq input 0 pos) 
bindings)))) 
If this match failed, try another longer one 

(if (eq b2 fail) 
(segment-match pattern input bindings (+ pos 1)) 
b2))))))) 

(defun first-match-pos (patl input start) 
"Find the first position that patl could possibly match input, 
starting at position start. If patl is non-constant, then just 
return start." 
(cond ((and (atom patl) (not (variable-p patl))) 

(position patl input :start start :test #*equal)) 
((< start (length input)) start) 
(t nil))) 

In the first example below, the segment variable ?x matches the sequence (b c). In 
the second example, there are two segment variables in a row. The first successful 
match is achieved with the first variable, ?x, matching the empty sequence, and the 
second one, ?y, matching (be). 

> (pat-match '(a (?* ?x) d) '(a b c d)) => ((?X . .) 

> (pat-match '(a (?* ?x) (?* ?y) d) '(a b c d)) => ((?Y . C) (?X)) 

In the next example, ?x is first matched against nil and ?y against (be d), but that 
fails, so we try matching ?x against a segment of length one. That fails too, but 
finally the match succeeds with ?x matching the two-element segment (be), and ?y 
matching (d). 


<a id='page-186'></a>

> (pat-match '(a (?* ?x) (?* ?y) ?x ?y) 
'(a b c d (b c) (d))) ((?Y D) (?X . .) 

Given segment - match, it is easy to define the function to match one-or-more elements 
and the function to match zero-or-one element: 

(defun segment-match+ (pattern input bindings) 
"Match one or more elements of input." 
(segment-match pattern input bindings D) 

(defun segment-match? (pattern input bindings) 
"Match zero or one element of input." 
(let ((var (second (first pattern))) 

(pat (rest pattern))) 
(or (pat-match (cons var pat) input bindings) 
(pat-match pat input bindings)))) 

Finally, we supply the function to test an arbitrary piece of Lisp code. It does this 
by evaluating the code with the bindings implied by the binding list. This is one of 
the few cases where it is appropriate to call eval: when we want to give the user 
unrestricted access to the Lisp interpreter. 

(defun match-if (pattern input bindings) 
"Test an arbitrary expression involving variables. 
The pattern looks like ((?if code) . rest)." 
(and (progv (mapcar #*car bindings) 

(mapcar #'cdr bindings) 
(eval (second (first pattern)))) 
(pat-match (rest pattern) input bindings))) 

Here are two examples using ?i f. The first succeeds because (+ 3 4) is indeed 7, 
and the second fails because (> 3 4) is false. 

> (pat-match '(?x ?op ?y is ?z (?if (eql (?op ?x ?y) ?z))) 
'(3 + 4 is 7)) 
((?Z . 7) (?Y . 4) (?0P . +) (?X . 3)) 

> (pat-match '(?x ?op ?y (?if (?op ?x ?y))) 
'(3 > 4)) 
NIL 

The syntax we have defined for patterns has two virtues: first, the syntax is very 
general, so it is easy to extend. Second, the syntax can be easily manipulated by 
pat-match. However, there is one drawback: the syntax is a little verbose, and some 
may find it ugly. Compare the following two patterns: 


<a id='page-187'></a>
(a (?* ?x) (?* ?y) d) 

(a ?x* ?y* d) 

Many readers find the second pattern easier to understand at a glance. We could 
change pat-match to allow for patterns of the form ?x*, but that would mean 
pat-match would have a lot more work to do on every match. An alternative is 
to leave pat-match as is, but define another level of syntax for use by human readers 
only. That is, a programmer could type the second expression above, and have it 
translated into the first, which would then be processed by pat-match. 

In other words, we will define a facility to define a kind of pattern-matching 
macro that will be expanded the first time the pattern is seen. It is better to do this 
expansion once than to complicate pat-match and in effect do the expansion every 
time a pattern is used. (Of course, if a pattern is only used once, then there is no 
advantage. But in most programs, each pattern will be used again and again.) 

We need to define two functions: one to define pattern-matching macros, and 
another to expand patterns that may contain these macros. We will only allow 
symbols to be macros, so it is reasonable to store the expansions on each symbol's 
property list: 

(defun pat-match-abbrev (symbol expansion) 
"Define symbol as a macro standing for a pat-match pattern." 
(setf (get symbol *expand-pat-match-abbrev) 

(expand-pat-match-abbrev expansion)) 

(defun expand-pat-match-abbrev (pat) 
"Expand out all pattern matching abbreviations in pat." 
(cond ((and (symbolp pat) (get pat 'expand-pat-match-abbrev))) 

((atom pat) pat) 
(t (cons (expand-pat-match-abbrev (first pat)) 
(expand-pat-match-abbrev (rest pat)))))) 

We would use this facility as follows: 

> (pat-match-abbrev '?x* '(?* ?x)) => (?* ?X) 

> (pat-match-abbrev '?y* '(?* ?y)) (?* ?Y) 

> (setf axyd (expand-pat-match-abbrev '(a ?x* ?y* d))) 
(A (?* ?X) (?* ?Y) D) 

> (pat-match axyd '(a b c d)) ((?Y . C) (?X)) 

&#9635; Exercise 6.1 [m] Go back and change the ELIZA rules to use the abbreviation facility. 
Does this make the rules easier to read? 


<a id='page-188'></a>

&#9635; Exercise 6.2 [h] In the few prior examples, every time there was a binding of 
pattern variables that satisfied the input, that binding was found. Informally, show 
that pat-match will always find such a binding, or show a counterexample where it 
fails to find one. 

6.3 A Rule-Based Translator Tool 
As we have defined it, the pattern matcher matches one input against one pattern. In 
el i . a, we need to match each input against a number of patterns, and then return a 
result based on the rule that contains the first pattern that matches. To refresh your 
memory, here is the function use-el i za - rul es: 

(defun use-eliza-rules (input) 
"Find some rule with which to transform the input." 
(some #*(lambda (rule) 

(let ((result (pat-match (rule-pattern rule) input))) 
(if (not (eq result fail)) 
(sublis (switch-viewpoint result) 
(random-elt (rule-responses rule)))))) 
*eliza-rules*)) 

It turns out that this will be a quite common thing to do: search through a list of rules 
for one that matches, and take action according to that rule. To turn the structure of 
use-el i za-rules into a software tool, we will allow the user to specify each of the 
following: 

* What kind of rule to use. Every rule will be characterized by an if-part and a 
then-part, but the ways of getting at those two parts may vary. 
* What list of rules to use. In general, each appHcation will have its own list of 
rules. 
* How to see if a rule matches. By default, we will use pat-match, but it should 
be possible to use other matchers. 
* What to do when a rule matches. Once we have determined which rule to use, 
we have to determine what it means to use it. The default is just to substitute 
the bindings of the match into the then-part of the rule. 

<a id='page-189'></a>
The rule-based translator tool now looks like this: 

(defun rule-based-translator 
(input rules &key (matcher #'pat-match) 

(rule-if #*first) (rule-then #*rest) (action #*sublis)) 
"Find the first rule in rules that matches input, 
and apply the action to that rule." 
(some 

#'(lambda (rule) 
(let ((result (funcall matcher (funcall rule-if rule) 
input))) 
(if (not (eq result fail)) 
(funcall action result (funcall rule-then rule))))) 
rules)) 

(defun use-eliza-rules (input) 
"Find some rule with which to transform the input.' 
(rule-based-translator input *eliza-rules* 

laction #.(lambda (bindings responses) 
(sublis (switch-viewpoint bindings) 
(random-elt responses))))) 

6.4 A Set of Searching Tools 
The GPS program can be seen as a problem in search. In general, a search problem 
involves exploring from some starting state and investigating neighboring states 
until a solution is reached. As in GPS, state means a description of any situation or 
state of affairs. Each state may have several neighbors, so there will be a choice of 
how to search. We can travel down one path until we see it is a dead end, or we can 
consider lots of different paths at the same time, expanding each path step by step. 
Search problems are called nondeterministic because there is no way to determine 
what is the best step to take next. AI problems, by their very nature, tend to be 
nondeterministic. This can be a source of confusion for programmers who are used 
to deterministic problems. In this section we will try to clear up that confusion. 
This section also serves as an example of how higher-order functions can be used to 
implement general tools that can be specified by passing in specific functions. 

Abstractly, a search problem can be characterized by four features: 

* The siarf state. 
* The^Oiz/state (or states). 

<a id='page-190'></a>

* The successors, or states that can be reached from any other state. 
* The strategy that determines the order in which we search. 
The first three features are part of the problem, while the fourth is part of the 
solution. In GPS, the starting state was given, along with a description of the goal 
states. The successors of a state were determined by consulting the operators. The 
search strategy was means-ends analysis. This was never spelled out explicitly but 
was impUcit in the structure of the whole program. In this section we will formulate 
a general searching tool, show how it can be used to implement several different 
search strategies, and then show how GPS could be implemented with this tool. 

The first notion we have to define is the state space, or set of all possible states. 
We can view the states as nodes and the successor relation as links in a graph. Some 
state space graphs will have a small number of states, while others have an infinite 
number, but they can still be solved if we search cleverly. Some graphs will have 
a regular structure, while others will appear random. We will start by considering 
only trees—that is, graphs where a state can be reached by only one unique sequence 
of successor links. Here is a tree: 

Searching Trees 

We will call our first searching tool tree-search, because it is designed to search 
state spaces that are in the form of trees. It takes four arguments: (1) a list of valid 
starting states, (2) a predicate to decide if we have reached a goal state, (3) a function 
to generate the successors of a state, and (4) a function that decides in what order 


<a id='page-191'></a>
to search. The first argument is a hst rather than a single state so that tree-search 
can recursively call itself after it has explored several paths through the state space. 
Think of the first argument not as a starting state but as a list of possible states from 
which the goal may be reached. This lists represents the fringe of the tree that has 
been explored so far. tree-search has three cases: If there are no more states to 
consider, then give up and return f ai 1. If the first possible state is a goal state, 
then return the succesful state. Otherwise, generate the successors of the first state 
and combine them with the other states. Order this combined list according to the 
particular search strategy and continue searching. Note that tree - search itself does 
not specify any particular searching strategy. 

(defun tree-search (states goal-p successors combiner) 
"Find a state that satisfies goal-p. Start with states, 
and search according to successors and combiner." 
(dbg :search ""&;; Search: ~a" states) 
(cond ((null states) fail) 

((funcall goal-p (first states)) (first states)) 
(t (tree-search 

(funcall combiner 
(funcall successors (first states)) 
(rest states)) 

goal-p successors combiner)))) 

The first strategy we will consider is called depth-first search. In depth-first search, 
the longest paths are considered first. In other words, we generate the successors 
of a state, and then work on the first successor first. We only return to one of the 
subsequent successors if we arrive at a state that has no successors at all. This 
strategy can be implemented by simply appending the previous states to the end 
of the Ust of new successors on each iteration. The function depth-first-search 
takes a single starting state, a goal predicate, and a successor function. It packages 
the starting state into a Hst as expected by tree-search, and specifies append as the 
combining function: 

(defun depth-first-search (start goal-p successors) 
"Search new states first until goal is reached." 
(tree-search (list start) goal-p successors #'append)) 

Let's see how we can search through the binary tree defined previously. First, we 
define the successor function binary-tree. It returns a list of two states, the two 
numbers that are twice the input state and one more than twice the input state. So the 
successors of 1 will be 2 and 3, and the successors of 2 will be 4 and 5. The bi na ry - tree 
function generates an infinite tree of which the first 15 nodes are diagrammed in our 
example. 


<a id='page-192'></a>

(defun binary-tree (.) (list (* 2 .) (+ 1 (* 2 .)))) 

.. make it easier to specify a goal, we define the function i s as a function that returns 
a predicate that tests for a particular value. Note that 1 s does not do the test itself. 
Rather, it returns a function that can be called to perform tests: 

(defun is (value) #*(lambda (x) (eql . value))) 

Now we can turn on the debugging output and search through the binary tree, starting 
at 1, and looking for, say, 12, as the goal state. Each line of debugging output shows 
the list of states that have been generated as successors but not yet examined: 

> (debug :search) => (SEARCH) 

> (depth-first-search 1 (is 12) #*binary-tree) 

;; Search; (1) 

;: Search: (2 3) 

;; Search: (4 5 3) 

;; Search: (8 9 5 3) 

Search: (16 17 9 5 3) 

:; Search: (32 33 17 9 5 3) 

;; Search: (64 65 33 17 9 5 3) 

Search: (128 129 65 33 17 9 5 3) 

Search: (256 257 129 65 33 17 9 5 3) 

;: Search: (512 513 257 129 65 33 17 9 5 3) 

;; Search: (1024 1025 513 257 129 65 33 17 9 5 3) 

Search: (2048 2049 1025 513 257 129 65 33 17 9 5 3) 

[Abort] 

The problem is that we are searching an infinite tree, and the depth-first search 
strategy just dives down the left-hand branch at every step. The only way to stop the 
doomed search is to type an interrupt character. 

An alternative strategy isbreadth-first search, where the shortest path is extended 
first at each step. It can be implemented simply by appending the new successor 
states to the end of the existing states: 

(defun prepend (x y) "Prepend y to start of x" (append y .)) 

(defun breadth-first-search (start goal-p successors) 
"Search old states first until goal is reached." 
(tree-search (list start) goal-p successors #'prepend)) 

The only difference between depth-first and breadth-first search is the difference 
between append and prepend. Here we see breadth-first-search inaction: 


<a id='page-193'></a>

> (breadth-first-search 1 (is 12) *binary-tree) 
Search: (1) 
Search: (2 3) 
Search: (3 4 5) 
Search: (4 5 6 7) 
Search: (56789) 
Search: (6 7 8 9 10 11) 
Search: (7 8 9 10 11 12 13) 
Search: (8 9 10 11 12 13 14 15) 
Search: (9 10 11 12 13 14 15 16 17) 
Search: (10 11 12 13 14 15 16 17 18 19) 
Search: (11 12 13 14 15 16 17 18 19 20 21) 
Search: (12 13 14 15 16 17 18 19 20 21 22 23) 

12 

Breadth-first search ends up searching each node in numerical order, and so it will 
eventually find any goal. It is methodical, but therefore plodding. Depth-first search 
will be much faster—if it happens to find the goal at all. For example, if we were 
looking for 2048, depth-first search would find it in 12 steps, while breadth-first 
would take 2048 steps. Breadth-first search also requires more storage, because it 
saves more intermediate states. 

If the search tree is finite, then either breadth-first or depth-first will eventually 
find the goal. Both methods search the entire state space, but in a different order. We 
will now show a depth-first search of the 15-node binary tree diagrammed previously. 
It takes about the same amount of time to find the goal (12) as it did with breadth-first 
search. It would have taken more time to find 15; less to find 8. The big difference is 
in the number of states considered at one time. At most, depth-first search considers 
four at a time; in general it will need to store only log2 . states to search a n-node tree, 
while breadth-first search needs to store n/2 states. 

(defun finite-binary-tree (n) 
"Return a successor function that generates a binary tree 
with . nodes." 
#'(lambda (x) 

(remove-if #*(lambda (child) (> child n)) 
(binary-tree x)))) 

> (depth-first-search 1 (is 12) (finite-binary-tree 15)) 

;; Search: (1) 
Search: (2 3) 
Search: (4 5 3) 
Search: (8 9 5 3) 
Search: (9 5 3) 

:: Search: (5 3) 
:: Search: (10 11 3) 
;: Search: (11 3) 


<a id='page-194'></a>

Search: (3) 
Search: (6 7) 
Search: (12 13 7) 

12 

Guiding the Search 

While breadth-first search is more methodical, neither strategy is able to take advantage 
of any knowledge about the state space. They both search blindly. In most real 
applications we will have some estimate of how far a state is from the solution. In 
such cases, we can implement a best-first search. The name is not quite accurate; if 
we could really search best first, that would not be a search at all. The name refers to 
the fact that the state that appears to be best is searched first. 

To implement best-first search we need to add one more piece of information: a 
cost function that gives an estimate of how far a given state is from the goal. 

For the binary tree example, we will use as a cost estimate the numeric difference 
from the goal. So if we are looking for 12, then 12 has cost 0, 8 has cost 4 and 2048 
has cost 2036. The higher-order function d i ff, shown in the following, returns a cost 
function that computes the difference from a goal. The higher-order function sorter 
takes a cost function as an argument and returns a combiner function that takes the 
lists of old and new states, appends them together, and sorts the result based on the 
cost function, lowest cost first. (The built-in function sort sorts a list according to 
a comparison function. In this case the smaller numbers come first, sort takes an 
optional : key argument that says how to compute the score for each element. Be 
careful—sort is a destructive function.) 

(defun diff (num) 
"Return the function that finds the difference from num." 
#'(lambda (x) (abs (- . num)))) 

(defun sorter (cost-fn) 
"Return a combiner function that sorts according to cost-fn." 
#'(lambda (new old) 

(sort (append new old) #'< :key cost-fn))) 

(defun best-first-search (start goal-p successors cost-fn) 
"Search lowest cost states first until goal is reached." 
(tree-search (list start) goal-p successors (sorter cost-fn))) 

Now, using the difference from the goal as the cost function, we can search using 
best-first search: 


<a id='page-195'></a>
> (best-first-search 1 (is 12) #'binary-tree (diff 12)) 
Search: (1) 

;; Search: (3 2) 
Search: (7 6 2) 
Search: (14 15 6 2) 
Search: (15 6 2 28 29) 

;; Search: (6 2 28 29 30 31) 
Search: (12 13 2 28 29 30 31) 
12 

The more we know about the state space, the better we can search. For example, if we 
know that all successors are greater than the states they come from, then we can use 
a cost function that gives a very high cost for numbers above the goal. The function 
price-is-right is like diff, except that it gives a high penalty for going over the 
goal."^ Using this cost function leads to a near-optimal search on this example. It 
makes the "mistake" of searching 7 before 6 (because 7 is closer to 12), but does not 
waste time searching 14 and 15: 

(defun price-is-right (price) 
"Return a function that measures the difference from price, 
but gives a big penalty for going over price." 
#'(lambda (x) (if (> . price) 

most-positive-fixnum 
(- price x)))) 

> (best-first-search 1 (is 12) #'binary-tree (price-is-right 12)) 
Search: (1) 
Search: (3 2) 
Search: (7 6 2) 
Search: (6 2 14 15) 
Search: (12 2 13 14 15) 

12 

All the searching methods we have seen so far consider ever-increasing lists of states 
as they search. For problems where there is only one solution, or a small number of 
solutions, this is unavoidable. To find a needle in a haystack, you need to look at a 
lot of hay. But for problems with many solutions, it may be worthwhile to discard 
unpromising paths. This runs the risk of failing to find a solution at all, but it can 
save enough space and time to offset the risk. A best-first search that keeps only a 
fixed number of alternative states at any one time is known as a beam search. Think 
of searching as shining a light through the dark of the state space. In other search 

^The built-in constant most-positive-fixnum is a large integer, the largest that can be 
expressed without using bignums. Its value depends on the implementation, but in most 
Lisps it is over 16 million. 


<a id='page-196'></a>

strategies the light spreads out as we search deeper, but in beam search the light 
remains tightly focused. Beam search is a variant of best-first search, but it is also 
similar to depth-first search. The difference is that beam search looks down several 
paths at once, instead of just one, and chooses the best one to look at next. But 
it gives up the ability to backtrack indefinitely. The function beam-search is just 
like best-first-search, except that after we sort the states, we then take only the 
first beam-width states. This is done with subseq; (subseq list start end) returns the 
sublist that starts at position start and ends just before position end. 

(defun beam-search (start goal-p successors cost-fn beam-width) 
"Search highest scoring states first until goal is reached, 
but never consider more than beam-width states at a time." 
(tree-search (list start) goal-p successors 

#'(lambda (old new) 
(let ((sorted (funcall (sorter cost-fn) oldnew))) 
(if (> beam-width (length sorted)) 
sorted 

(subseq sorted0 beam-width)))))) 

We can successfully search for 12 in the binary tree using a beam width of only 2: 

> (beam-search 1 (is 12) #*binary-tree (price-is-right 12) 2) 
Search; (1) 
Search; (3 2) 

;; Search; (7 6) 
Search; (6 14) 
Search; (12 13) 

12 

However, if we go back to the scoring function that just takes the difference from 12, 
then beam search fails. When it generates 14 and 15, it throws away 6, and thus loses 
its only chance to find the goal: 

> (beam-search 1 (is 12) #'binary-tree (diff 12) 2) 
Search; (1) 
Search; (3 2) 
Search; (7 6) 
Search; (14 15) 
Search; (15 28) 
Search; (28 30) 
Search; (30 56) 
Search; (56 60) 
Search; (60 112) 
Search; (112 120) 
Search; (120 224) 


<a id='page-197'></a>

[Abort] 

This search would succeed if we gave a beam width of 3. This illustrates a general 
principle: we can find a goal either by looking at more states, or by being smarter 
about the states we look at. That means having a better ordering function. 

Notice that with a beam width of infinity we get best-first search. With a beam 
width of 1, we get depth-first search with no backup. This could be called "depth-only 
search," but it is more commonly known as hill-climbing. Think of a mountaineer 
trying to reach a peak in a heavy fog. One strategy would be for the moimtaineer to 
look at adjacent locations, climb to the highest one, and look again. This strategy 
may eventually hit the peak, but it may also get stuck at the top of a foothill, or local 
manmum. Another strategy would be for the mountaineer to turn back and try again 
when the fog lifts, but in AI, unfortunately, the fog rarely lifts.^ 

As a concrete example of a problem that can be solved by search, consider the 
task of planning a flight across the North American continent in a small airplane, one 
whose range is limited to 1000 kilometers. Suppose we have a list of selected cities 
with airports, along with their position in longitude and latitude: 

(defstruct (city (:type list)) name long lat) 

(defparameter *cities* 

'((Atlanta 84.23 33.45) (Los-Angeles 118.15 34.03) 
(Boston 71.05 42.21) (Memphis 90.03 35.09) 
(Chicago 87.37 41.50) (New-York 73.58 40.47) 
(Denver 105.00 39.45) (Oklahoma-City 97.28 35.26) 
(Eugene 123.05 44.03) (Pittsburgh 79.57 40.27) 
(Flagstaff 111.41 35.13) (Quebec 71.11 46.49) 
(Grand-Jet 108.37 39.05) (Reno 119.49 39.30) 
(Houston 105.00 34.00) (San-Francisco 122.26 37.47) 
(Indianapolis 86.10 39.46) (Tampa 82.27 27.57) 
(Jacksonville 81.40 30.22) (Victoria 123.21 48.25) 
(Kansas-City 94.35 39.06) (Wilmington 77.57 34.14))) 

This example introduces a new option to defstruct. Instead of just giving the name 
of the structure, it is also possible to use: 

(defstruct {structure-name {option value),,,) 'Optionaldoc'' slot,,,) 

For city, the option : type is specified as 1 i st. This means that cities will be implemented 
as lists of three elements, as they are in the initial value for *ci ti es*. 

^In chapter 8 we will see an example where the fog did lift: symbolic integration was once 
handled as a problem in search, but new mathematical results now make it possible to solve 
the same class of integration problems without search. 


<a id='page-198'></a>

Figure 6.1: A Map of Some Cities 

The cities are shown on the map in figure 6.1, which has cormections between 
all cities within the 1000 kilometer range of each other.^ This map was drawn with 
the help of ai r-di stance, a function that retiutis the distance in kilometers between 
two cities "as the crow flies." It will be defined later. Two other useful fimctions are 
nei ghbors, which finds all the cities within 1000 kilometers, and ci ty, which maps 
from a name to a city. The former uses f i nd - a 11 - i f, which was defined on [page 101](chapter3.md#page-101) 
as a synonym for remove- i f-not. 

(defun neighbors (city) 
"Find all cities within 1000 kilometers." 
(find-all-if #'(lambda (c) 

(and (not (eq c city)) 
(< (air-distance c city) 1000.0))) 
*cities*)) 

(defun city (name) 
"Find the city with this name." 
(assoc name *cities*)) 

We are now ready to plan a trip. The fimction trip takes the name of a starting and 
destination city and does a beam search of width one, considering all neighbors as 

^The astute reader will recognize that this graph is not a tree. The difference between trees 
and graphs and the implications for searching will be covered later. 


<a id='page-199'></a>
successors to a state. The cost for a state is the air distance to the destination city: 

(defun trip (start dest) 
"Search for a way from the start to dest." 
(beam-search start (is dest) #'neighbors 

#'(1ambda (c) (air-distance c dest)) 

D) 

Here we plan a trip from San Francisco to Boston. The result seems to be the best 
possible path: 

> (trip (city 'san-francisco) (city 'boston)) 
Search: ((SAN-FRANCISCO 122.26 37.47)) 
Search: ((RENO 119.49 39.3)) 
Search: ((GRAND-JCT 108.37 39.05)) 
Search: ((DENVER 105.0 39.45)) 
Search: ((KANSAS-CITY 94.35 39.06)) 
Search: ((INDIANAPOLIS 86.1 39.46)) 
Search: ((PITTSBURGH 79.57 40.27)) 

:; Search: ((BOSTON 71.05 42.21)) 
(BOSTON 71.05 42.21) 

But look what happens when we plan the return trip. There are two detours, to 
Chicago and Flagstaff: 

> (trip (city 'boston) (city 'san-francisco)) 
Search: ((BOSTON 71.05 42.21)) 
Search: ((PITTSBURGH 79.57 40.27)) 
Search: ((CHICAGO 87.37 41.5)) 
Search: ((KANSAS-CITY 94.35 39.06)) 
Search: ((DENVER 105.0 39.45)) 
Search: ((FLAGSTAFF 111.41 35.13)) 
Search: ((RENO 119.49 39.3)) 
Search: ((SAN-FRANCISCO 122.26 37.47)) 

(SAN-FRANCISCO 122.26 37.47) 

Why did tri . go from Denver to San Francisco via Flagstaff? Because Flagstaff is 
closer to the destination than Grand Junction. The problem is that we are minimizing 
the distance to the destination at each step, when we should be minimizing the sum 
of the distance to the destination plus the distance already traveled. 


<a id='page-200'></a>

Search Paths 

To minimize the total distance, we need some way to talk about the path that leads 
to the goal. But the functions we have defined so far only deal with individual states 
along the way. Representing paths would lead to another advantage: we could 
return the path as the solution, rather than just return the goal state. As it is, tri . 
only returns the goal state, not the path to it. So there is no way to determine what 
trip has done, except by reading the debugging output. 

The data structure path is designed to solve both these problems. A path has 
four fields: the current state, the previous partial path that this path is extending, 
the cost of the path so far, and an estimate of the total cost to reach the goal. Here is 
the structure definition for path. It uses the : pri nt-function option to say that all 
paths are to be printed with the function pr i nt - pa th, which will be defined below. 

(defstruct (path (:print-function print-path)) 
state (previous nil) (cost-so-far 0) (total-cost 0)) 

The next question is how to integrate paths into the searching routines with the 
least amount of disruption. Clearly, it would be better to make one change to 
tree-search rather than to change depth-first-search, breadth-first-search, 
and beam-search. However, looking back at the definition of tree-search, we see 
that it makes no assumptions about the structure of states, other than the fact that 
they can be manipulated by the goal predicate, successor, and combiner fimctions. 
This suggests that we can use tree-search unchanged if we pass it paths instead of 
states, and give it functions that can process paths. 

In the following redefinition of tri ., the beam- sea rch function is called with five 
arguments. Instead of passing it a city as the start state, we pass a path that has 
the city as its state field. The goal predicate should test whether its argument is a 
path whose state is the destination; we assume (and later define) a version of i s that 
accommodates this. The successor function is the most difficult. Instead of just 
generating a Ust of neighbors, we want to first generate the neighbors, then make 
each one into a path that extends the current path, but with an updated cost so far 
and total estimated cost. The function path - saver returns a function that will do just 
that. Finally, the cost function we are trying to minimize is path-total - cost, and 
we provide a beam width, which is now an optional argument to tri . that defaults 
to one: 

(defun trip (start dest &optional (beam-width 1)) 

"Search for the best path from the start to dest." 

(beam-search 

(make-path :state start) 

(is dest :key #*path-state) 

(path-saver #'neighbors #'air-distance 


<a id='page-201'></a>
#'(lambda (c) (air-distance c dest))) 
#*path-total-cost 
beam-width)) 

The calculation of ai r-di stance involves some complicated conversion of longitude 
and latitude to x-y-z coordinates. Since this is a problem in solid geometry, not AI, 
the code is presented without further comment: 

(defconstant earth-diameter 12765.0 
"Diameter of planet earth in kilometers.") 

(defun air-distance (cityl city2) 
"The great circle distance between two cities." 
(let ((d (distance (xyz-coords cityl) (xyz-coords city2)))) 

d is the straight-1ine chord between the two cities. 
;; The length of the subtending arc is given by: 
(* earth-diameter (asin (/ d 2))))) 

(defun xyz-coords (city) 
"Returns the x.y.z coordinates of a point on a sphere. 
The center is (0 0 0) and the north pole is (0 0 D." 
(let ((psi (deg->radians (city-lat city))) 

(phi (deg->radians (city-long city)))) 

(list (* (cos psi) (cos phi)) 
(* (cos psi) (sin phi)) 
(sin psi)))) 

(defun distance (pointl point2) 
"The Euclidean distance between two points. 
The points are coordinates in n-dimensional space." 
(sqrt (reduce #*+ (mapcar #'(lambda (a b) (expt (- a b) 2)) 

pointl point2)))) 

(defun deg->radians (deg) 
"Convert degrees and minutes to radians. " 
(* (+ (truncate deg) (* (rem deg 1) 100/60)) pi 1/180)) 

Before showing the auxiliary functions that implement this, here are some examples 
that show what it can do. With a beam width of 1, the detour to Flagstaff is eliminated, 
but the one to Chicago remains. With a beam width of 3, the correct optimal path is 
found. In the following examples, each call to the new version of t r i . returns a path, 
which is printed by s how- ci ty - pa th: 

> (show-city-path (trip (city 'san-francisco) (city 'boston) 1)) 
#<Path 4514.8 km: San-Francisco - Reno - Grand-Jet - Denver Kansas-
City - Indianapolis - Pittsburgh - Boston> 


<a id='page-202'></a>

> (show-city-path (trip (city 'boston) (city 'san-francisco) 1)) 
#<Path 4577.3 km: Boston - Pittsburgh - Chicago - Kansas-City Denver 
- Grand-Jet - Reno - San-Francisco> 

> (show-city-path (trip (city 'boston) (city 'san-francisco) 3)) 
#<Path 4514.8 km: Boston - Pittsburgh - Indianapolis Kansas-
City - Denver - Grand-Jet - Reno - San-Francisco> 

This example shows how search is susceptible to irregularities in the search space. It 
was easy to find the correct path from west to east, but the return trip required more 
search, because Flagstaff is a falsely promising step. In general, there may be even 
worse dead ends lurking in the search space. Look what happens when we limit the 
airplane's range to 700 kilometers. The map is shown in figure 6.2. 

Figure 6.2: A Map of Cities within 700km 

If we try to plan a trip from Tampa to Quebec, we can run into problems with 
the dead end at Wilmington, North Carolina. With a beam width of 1, the path to 
Jacksonville and then Wilmington will be tried first. From there, each step of the path 
alternates between Atlanta and Wilmington. The search never gets any closer to the 
goal. But with a beam width of 2, the path from Tampa to Atlanta is not discarded, 
and it is eventually continued on to Indianapolis and eventually to Quebec. So the 
capability to back up is essential in avoiding dead ends. 

Now for the implementation details. The function i s still returns a predicate that 
tests for a value, but now it accepts : key and : test keywords: 


<a id='page-203'></a>
(defun is (value &key (key #'identity) (test #'eql)) 
"Returns a predicate that tests for a given value." 
#'(lambda (path) (funcall test value (funcall key path)))) 

The path - saver function returns a function that will take a path as an argument and 
generate successors paths, path-saver takes as an argument a successor function 
that operates on bare states. It calls this function and, for each state returned, builds 
up a path that extends the existing path and stores the cost of the path so far as well 
as the estimated total cost: 

(defun path-saver (successors cost-fn cost-left-fn) 
#*(lambda (old-path) 
(let ((old-state (path-state old-path))) 
(mapcar 
#*(lambda (new-state) 
(let ((old-cost 
(+ (path-cost-so-far old-path) 
(funcall cost-fn old-state new-state)))) 

(make-path 
:state new-state 
rprevious old-path 
:cost-so-far old-cost 
:total-cost (+ old-cost (funcall cost-left-fn 

new-state))))) 
(funcall successors old-state))))) 

By default a path structure would be printed as #S (PATH ...). But because each path 
has a previ ous field that is filled by another path, this output would get quite verbose. 
That is why we installed pr i nt- pat h as the print function for paths when we defined 
the structure. It uses the notation #<...>, which is a Common Lisp convention for 
printing output that can not be reconstructed by read. The function show- ci ty - pa th 
prints a more complete representation of a path. We also define map-path to iterate 
over a path, collecting values: 

(defun print-path (path &optional (stream t) depth) 
(declare (ignore depth)) 
(format stream "#<Path to '"a cost ~.lf> " 

(path-state path) (path-total-cost path))) 

(defun show-city-path (path &optional (stream t)) 
"Show the length of a path, and the cities along it." 
(format stream "#<Path ~,lf km: "{^..^.^ -~}>" 

(path-total-cost path) 
(reverse (map-path #'city-name path))) 
(values)) 


<a id='page-204'></a>

(defun map-path (fn path) 

"Can fn on each state in the path, collecting results." 

(if (null path) 

nil 

(cons (funcall fn (path-state path)) 

(map-path fn (path-previous path))))) 

Guessing versus Guaranteeing a Good Solution 

Elementary AI textbooks place a great emphasis on search algorithms that are gusiranteed 
to find the best solution. However, in practice these algorithms are hardly 
ever used. The problem is that guaranteeing the best solution requires looking at a lot 
of other solutions in order to rule them out. For problems with large search spaces, 
this usually takes too much time. The alternative is to use an algorithm that will 
probably return a solution that is close to the best solution, but gives no guarantee. 
Such algorithms, traditionally known as non-admissible heuristic search algorithms, 
can be much faster. 

Of the algorithms we have seen so far, best-first search almost, but not quite, 
guarantees the best solution. The problem is that it terminates a little too early. 
Suppose it has calculated three paths, of cost 90, 95 and 110. It will expand the 90 
path next. Suppose this leads to a solution of total cost 100. Best-first search will 
then retimi that solution. But it is possible that the 95 path could lead to a solution 
with a total cost less than 100. Perhaps the 95 path is only one unit away from the 
goal, so it could result in a complete path of length 96. This means that an optimal 
search should examine the 95 path (but not the 110 path) before exiting. 

Depth-first seeu-ch and beam search, on the other hand, are definitely heuristic 
algorithms. Depth-first search finds a solution without any regard to its cost. With 
beam search, picking a good value for the beam width can lead to a good, quick 
solution, while picking the wrong value can lead to failure, or to a poor solution. 
One way out of this dilemma is to start with a narrow beam width, and if that does 
not lead to an acceptable solution, widen the beam and try again. We will call this 
iterative widening, although that is not a standard term. There are many variations on 
this theme, but here is a simple one: 

(defun iter-wide-search (start goal-p successors cost-fn 

&key (width 1) (max 100)) 

"Search, increasing beam width from width to max. 

Return the first solution found at any width." 

(dbg .-search "; Width: ~d" width) 

(unless (> width max) 

(or (beam-search start goal-p successors cost-fn width) 

(iter-wide-search start goal-p successors cost-fn 


<a id='page-205'></a>
:width (+ width 1) :max max)))) 

Here i ter-wide-search is used to search through a binary tree, failing with beam 
width 1 and 2, and eventually succeeding with beam width 3: 

> (iter-wide-search 1 (is 12) (finite-binary-tree 15) (diff 12)) 

Width: 1 

; Search: (1) 

; Search: (3) 

; Search: (7) 

: Search: (14) 

; Search: NIL 

Width: 2 

; Search: (1) 

; Search: (3 2) 

: Search: (7 6) 

: Search: (14 15) 

; Search: (15) 

: Search: NIL 

Width: 3 

; Search: (1) 

; Search: (3 2) 

; Search: (7 6 2) 

; Search: (14 15 6) 

; Search: (15 6) 

; Search: (6) 

; Search: (12 13) 

12 

The name iterative widening is derived from the established term iterative deepening. 
Iterative deepening is used to control depth-first search when we don't know the 
depth of the desired solution. The idea is first to limit the search to a depth of 1, 
then 2, and so on. That way we are guaranteed to find a solution at the minimum 
depth, just as in breadth-first search, but without wasting as much storage space. Of 
course, iterative deepening does waste some time because at each increasing depth 
it repeats all the work it did at the previous depth. But suppose that the average 
state has ten successors. That means that increasing the depth by one results in ten 
times more search, so only 10% of the time is wasted on repeated work. So iterative 
deepening uses only slightly more time and much less space. We will see it again in 
chapters 11 and 18. 


<a id='page-206'></a>

Searching Graphs 

So far, tree-search has been the workhorse behind all the searching routines. This 
is curious, when we consider that the city problem involves a graph that is not a tree 
at all. The reason tree - sea rch works is that any graph can be treated as a tree, if we 
ignore the fact that certain nodes are identical. For example, the graph in figure 6.3 
can be rendered as a tree. Figure 6.4 shows only the top four levels of the tree; each 
of the bottom nodes (except the 6s) needs to be expanded further. 

J L 

Figure 6.3: A Graph with Six Nodes 

In searching for paths through the graph of cities, we were implicitly turning the 
graph into a tree. That is, if tree - sea rch found two paths from Pittsburgh to Kansas 
City (via Chicago or Indianapolis), then it would treat them as two independent 
paths, just as if there were two distinct Kansas Cities. This made the algorithms 
simpler, but it also doubles the number of paths left to examine. If the destination is 
San Francisco, we will have to search for a path from Kansas City to San Francisco 
twice instead of once. In fact, even though the graph has only 22 cities, the tree is 
infinite, because we can go back and forth between adjacent cities any number of 
times. So, while it is possible to treat the graph as a tree, there are potential savings 
in treating it as a true graph. 

The function g raph- sea rch does just that. It is similar to tree - sea rch, but accepts 
two additional cirguments: a comparison function that tests if two states are equal, 
and a list of states that are no longer being considered, but were examined in the past. 
The difference between graph-search and tree -search is in the call to new-states, 
which generates successors but eliminates states that are in either the list of states 
currently being considered or the list of old states considered in the past. 

(defun graph-search (states goal-p successors combiner 
&optional (state= #'eql) old-states) 
"Find a state that satisfies goal-p. Start with states. 


<a id='page-207'></a>
Figure 6.4: The Corresponding Tree 

and search according to successors and combiner. 
Don't try the same state twice." 
(dbg :search ""&;; Search: '"a" states) 
(cond ((null states) fail) 

((funcall goal-p (first states)) (first states)) 
(t (graph-search 

(funcall 
combiner 
(new-states states successors state= old-states) 
(rest states)) 

goal-p successors combiner state= 
(adjoin (first states) old-states 
:test state=))))) 

(defun new-states (states successors state= old-states) 
"Generate successor states that have not been seen before." 
(remove-if 

#'(lambda (state) 
(or (member state states :test state=) 
(member state old-states :test state=^))) 
(funcall successors (first states)))) 

Using the successor function next2, we can search the graph shown here either as a 
tree or as a graph. If we search it as a graph, it takes fewer iterations and less storage 
space to find the goal. Of course, there is additional overhead to test for identical 


<a id='page-208'></a>

States, but on graphs Uke this one we get an exponential speed-up for a constant 
amount of overhead. 

(defun next2 (x) (list (+ . 1) (+ . 2))) 
> (tree-search '(1) (is 6) #'next2 #*prepend) 

Search: (1) 
Search: (2 3) 
Search: 4) 
Search: 4 5) 
Search: 5 4 5) 
Search: 4 5 5 6) 
Search: 5 5 6 5 6) 
Search: 5 6 5 6 6 7) 
Search: 6 5 6 6 7 5 6) 
Search: 5 6 6 7 5 6 6 7) 
Search: 6 6 7 5 6 6 7 6 7) 

> (graph-search '(1) (is 6) #'next2 #*prepend) 
Search: (1) 
Search: (2 3) 
Search: (3 4) 
Search: (4 5) 
Search: (5 6) 
Search: (6 7) 

The next step is to extend the graph-sea rch algorithm to handle paths. The complication 
is in deciding which path to keep when two paths reach the same state. If we 
have a cost function, then the answer is easy: keep the path with the cheaper cost. 
Best-first search of a graph removing duplicate states is called A * search. 

A* search is more complicated than graph-search because of the need both to 
add and to delete paths to the lists of current and old paths. For each new successor 
state, there are three possibilities. The new state may be in the list of current paths, in 
the Ust of old paths, or in neither. Within the first two cases, there are two subcases. 
If the new path is more expensive than the old one, then ignore the new path—it can 
not lead to a better solution. If the new path is cheaper than a corresponding path 
in the list of current paths, then replace it with the new path. If it is cheaper than a 
corresponding path in the list of the old paths, then remove that old path, and put 
the new path in the list of current paths. 

Also, rather than sort the paths by total cost on each iteration, they are kept sorted, 
and new paths are inserted into the proper place one at a time using i nsert-path. 
Two more functions, better-path and find-path, are used to compare paths and 
see if a state has already appeared. 


<a id='page-209'></a>
(defun a*-search (paths goal-p successors cost-fn cost-left-fn 

&optional (state= #'eql) old-paths) 
"Find a path whose state satisfies goal-p. Start with paths, 
and expand successors, exploring least cost first. 
When there are duplicate states, keep the one with the 
lower cost and discard the other." 
(dbg :search ";; Search: ~a" paths) 
(cond 

((null paths) fail) 
((funcall goal-p (path-state (first paths))) 
(values (first paths) paths)) 
(t (let* ((path (pop paths)) 
(state (path-state path))) 
;; Update PATHS and OLD-PATHS to reflect 

the new successors of STATE: 
(setf old-paths (insert-path path old-paths)) 
(dolist (state2 (funcall successors state)) 

(let* ((cost (+ (path-cost-so-far path) 

(funcall cost-fn state state2))) 
(cost2 (funcall cost-left-fn state2)) 
(path2 (make-path 

:state state2 :previous path 
:cost-so-far cost 
:total-cost (+ cost cost2))) 

(old nil) 
Place the new path, path2, in the right list: 
(cond 
((setf old (find-path state2 paths state=)) 
(when (better-path path2 old) 
(setf paths (insert-path 
path2 (delete old paths))))) 
((setf old (find-path state2 old-paths state=)) 

(when (better-path path2 old) 
(setf paths (insert-path path2 paths)) 
(setf old-paths (delete old old-paths)))) 

(t (setf paths (insert-path path2 paths)))))) 
Finally, call A* again with the updated path lists: 
(a*-search paths goal-p successors cost-fn cost-left-fn 
state= old-paths))))) 


<a id='page-210'></a>

Here are the three auxiliary functions: 

(defun find-path (state paths state=) 
"Find the path with this state among a list of paths." 
(find state paths :key #'path-state :test state=)) 

(defun better-path (pathl path2) 
"Is pathl cheaper than path2?" 
(< (path-total-cost pathl) (path-total-cost path2))) 

(defun insert-path (path paths) 
"Put path into the right position, sorted by total cost." 
MERGE is a built-in function 
(merge 'list (list path) paths #'< :key #'path-total-cost)) 

(defun path-states (path) 
"Collect the states along this path." 
(if (null path) 

nil 
(cons (path-state path) 
(path-states (path-previous path))))) 

Below we use a*-search to search for 6 in the graph previously shown in figure 6.3. 
The cost function is a constant 1 for each step. In other words, the total cost is the 
length of the path. The heuristic evaluation function is just the difference from the 
goal. The A* algorithm needs just three search steps to come up with the optimal 
solution. Contrast that to the graph search algorithm, which needed five steps, and 
the tree search algorithm, which needed ten steps—and neither of them found the 
optimal solution. 

> (path-states 
(a*-search (list (make-path :state D) (is 6) 

#'next2 #'(lambda (x y) 1) (diff 6))) 
Search: (#<Path to 1 cost 0.0>) 
Search: (#<Path to 3 cost 4.0> #<Path to 2 cost 5.0>) 
Search: (#<Path to 5 cost 3.0> #<Path to 4 cost 4.0> 

#<Path to 2 cost 5.0>) 
Search: (#<Path to 6 cost 3.0> #<Path to 7 cost 4.0> 
#<Path to 4 cost 4.0> #<Path to 2 cost 5.0>) 
(6 5 3 1) 

It may seem limiting that these search functions all return a single answer. In some 
applications, we may want to look at several solutions, or at all possible solutions. 
Other applications are more naturally seen as optimization problems, where we 
don't know ahead of time what counts as achieving the goal but are just trying to find 
some action with a low cost. 


<a id='page-211'></a>
It turns out that the functions we have defined are not Umiting at all in this respect. 
They can be used to serve both these new purposes—provided we carefully specify 
the goal predicate. To find all solutions to a problem, all we have to do is pass in a 
goal predicate that always fails, but saves all the solutions in a list. The goal predicate 
will see all possible solutions and save away just the ones that are real solutions. 
Of course, if the search space is infinite this will never terminate, so the user has 
to be careful in applying this technique. It would also be possible to write a goal 
predicate that stopped the search after finding a certain number of solutions, or after 
looking at a certain number of states. Here is a function that finds all solutions, using 
beam search: 

(defun search-all (start goal-p successors cost-fn beam-width) 
"Find all solutions to a search problem, using beam search." 

Be careful: this can lead to an infinite loop, 
(let ((solutions nil)) 
(beam-search 
start #'(lambda (x) 

(when (funcall goal-p x) (push . solutions)) 
nil) 
successors cost-fn beam-width) 
solutions)) 

6.5 GPS as Search 
The GPS program can be seen as a problem in search. For example, in the three-block 
blocks world, there are only 13 different states. They could be arranged in a graph and 
searched just as we searched for a route between cities. Figure 6.5 shows this graph. 

The function search-gps does just that. Like the gps function on [page 135](chapter4.md#page-135), it 
computes a final state and then picks out the actions that lead to that state. But 
it computes the state with a beam search. The goal predicate tests if the current 
state satisfies every condition in the goal, the successor function finds all applicable 
operators and applies them, and the cost function simply sums the number of actions 
taken so far, plus the number of conditions that are not yet satisfied: 


<a id='page-212'></a>

. 
I. 
. 
. 
.. . 
. C 
. 
Figure 6.5: The Blocks World as a Graph 
(defun search-gps (start goal &optional (beam-width 10)) 
"Search for a sequence of operators leading to goal." 
(find-all-i f 
#*action-p 
(beam-search 
(cons '(start) start) 
#'(lambda (state) (subsetp goal state :test #*equal)) 
#'gps-successors 
#'(lambda (state) 
(+ (count-if #'action-p state) 
(count-if #'(lambda (con) 
(not (member-equal con state))) 
goal))) 
beam-width))) 
Here is the successor function: 
(defun gps-successors (state) 
"Return a lis t of states reachable from this one using ops." 
(mapcar 
#.(lambda (op) 


<a id='page-213'></a>
(append 
(remove-if #'(lambda (x) 
(member-equal . (op-del-list op))) 
state) 
(op-add-list op))) 
(applicable-ops state))) 

(defun applicable-ops (state) 
"Return a list of all ops that are applicable now." 
(find-all-if 

#'(lambda (op) 
(subsetp (op-preconds op) state :test #'equal)) 
*ops*)) 

The search technique finds good solutions quickly for a variety of problems. Here 
we see the solution to the Sussman anomaly in the three-block blocks world: 

(setf start '((c on a) (a on table) (b on table) (space on c) 
(space on b) (space on table))) 

> (search-gps start '((a on b) (b on c))) 

((START) 
(EXECUTING (MOVE C FROM A TO TABLE)) 
(EXECUTING (MOVE . FROM TABLE TO O ) 
(EXECUTING (MOVE A FROM TABLE TO B))) 

> (search-gps start '((b on c) (a on b))) 

((START) 
(EXECUTING (MOVE C FROM A TO TABLE)) 
(EXECUTING (MOVE . FROM TABLE TO O ) 
(EXECUTING (MOVE A FROM TABLE TO B))) 

In these solutions we search forward from the start to the goal; this is quite different 
from the means-ends approach of searching backward from the goal for an appropriate 
operator. But we could formulate means-ends analysis as forward search simply 
by reversing start and goal: GPS's goal state is the search's start state, and the search's 
goal predicate tests to see if a state matches GPS's start state. This is left as an exercise. 

6.6 History and References 
Pattern matching is one of the most important tools for AI. As such, it is covered 
in most textbooks on Lisp. Good treatments include Abelson and Sussman 
(1984), Wilensky (1986), Winston and Horn (1988), and Kreutzer and McKenzie 
(1990). An overview is presented in the "pattern-matching" entry in Encyclopedia of 
.. (Shapiro 1990). 


<a id='page-214'></a>

Nilsson's Problem-Solving Methods in Artificial Intelligence (1971) was an early textbook 
that emphasized search as the most important defining characteristic of AI. 
More recent texts give less importance to search; Winston's Artificial Intelligence 
(1984) gives a balanced overview, and his Lisp (1988) provides implementations of 
some of the algorithms. They are at a lower level of abstraction than the ones in 
this chapter. Iterative deepening was first presented by Korf (1985), and iterative 
broadening by Ginsberg and Harvey (1990). 
6.7 Exercises 
&#9635; Exercise 6.3 [m] Write a version of i . te ra et i ve -i nterpreter that is more general 
than the one defined in this chapter. Decide what features can be specified, and 
provide defaults for them. 

&#9635; Exercise 6.4 [m] Define a version of compose that allows any number of arguments, 
not just two. Hint: You may want to use the function reduce. 

&#9635; Exercise 6.5 [m] Define a version of compose that allows any number of arguments 
but is more efficient than the answer to the previous exercise. Hint: try to make 
decisions when compose is called to build the resulting function, rather than making 
the same decisions over and over each time the resulting function is called. 

&#9635; Exercise 6.6 [m] One problem with pat-match is that it gives special significance 
to symbols starting with ?, which means that they can not be used to match a literal 
pattern. Define a pattern that matches the input literally, so that such symbols can 
be matched. 

&#9635; Exercise 6.7 [m] Discuss the pros and cons of data-driven programming compared 
to the conventional approach. 

&#9635; Exercise 6.8 [m]
recursion. 
Write a version of tree-searc h using an explicit loop rather than 

&#9635; Exercise 6.9 [m] The sorte r function is inefficient for two reasons: it calls append, 
which has to make a copy of the first argument, and it sorts the entire result, rather 
than just inserting the new states into the already sorted old states. Write a more 
efficient sorter . 


<a id='page-215'></a>

&#9635; Exercise 6.10 [m] Write versions of graph-search and a*-search that use hash 
tables rather than lists to test whether a state has been seen before. 

&#9635; Exercise 6.11 [m] Write a function that calls beam-searchtofindthefirstnsolution s 
to a problem and returns them in a list. 

&#9635; Exercise 6.12 [m] On personal computers without floating-point hardware, the 
ai r-di stanc e calculation will be rather slow. If this is a problem for you, arrange 
to compute the xyz-coords of each city only once and then store them, or store 
a complete table of air distances between cities. Also precompute and store the 
neighbors of each city. 

&#9635; Exercise 6.13 [d] Write a version of GPS that uses A* search instead of beam search. 
Compare the two versions in a variety of domains. 

&#9635; Exercise 6.14 [d] Write a version of GPS that allows costs for each operator. For 
example, driving the child to school might have a cost of 2, but calling a limousine 
to transport the child might have a cost of 100. Use these costs instead of a constant 
cost of 1 for each operation. 

&#9635; Exercise 6.15 [d] Write a version of GPS that uses the searching tools but does 
means-ends analysis. 
6.8 Answers 
Answer 6.2 Unfortunately, pat -match does not always find the answer. The problem 
is that it will only rebind a segment variable based on a failure to match the 
rest of the pattern after the segment variable. In all the examples above, the "rest of 
the pattern after the segment variable" was the whole pattern, so pat-match always 
worked properly. But if a segment variable appears nested inside a list, then the rest 
of the segment variable's sublist is only a part of the rest of the whole pattern, as the 
following example shows: 
> (pat-match '(((? * ?x) (?* ?y)) ?x ?y) 
'(( a b c d ) (a b) (c d))) ^ NIL 

The correct answer with ?x bound to (a b) and ?y bound to (c d) is not found 
because the inner segment match succeeds with ?x bound to () and ?y bound to (a 


<a id='page-216'></a>

bed), and once we leave the inner match and return to the top level, there is no 
going back for alternative bindings. 

Answer 6.3 The following version lets the user specify all four components of the 
prompt-read-eval-print loop, as well as the streams to use for input and output. 
Defaults are set up as for a Lisp interpreter. 

(defun interactive-interpreter 
(&key (read #'read) (eval #'eval) (print #*print) 

(prompt "> ") (input t) (output t)) 
"Read an expression, evaluate it, and print the result." 
(loop 

(fresh-line output) 
(princ prompt output) 
(funcall print (funcall eval (funcall read input)) 
output))) 

Here is another version that does all of the above and also handles multiple values 
and binds the various "history variables" that the Lisp top-level binds. 

(defun interactive-interpreter 
(&key (read #'read) (eval #'eval) (print #'print) 

(prompt "> ") (input t) (output t)) 
"Read an expression, evaluate it, and print the result(s). 
Does multiple values and binds: ***'''-++++++/// ///" 
(let ('**'*'-++++++/ // /// vals) 

The above variables are all special, except VALS 
The variable - holds the current input 

' ape the 3 most recent values 
+ ++ +++ are the 3 most recent inputs 
/ // /// are the 3 most recent lists of multiple-values 
(loop 
(fresh-line output) 
(princ prompt output) 

First read and evaluate an expression 
(setf - (funcall read input) 
vals (multiple-value-list (funcall eval -))) 
Now update the history variables 

(setf +++ ++ /// // *** (first ///) 
++ + // / (first //) 
+ -/ vals * (first /)) 

Finally print the computed value(s) 
(dolist (value vals) 
(funcall print value output))))) 


<a id='page-217'></a>

Answer 6.4 

(defun compose (&rest functions) 
"Return the function that is the composition of all the args. 

i.e. (compose f g h) = (lambda (x) (f (g (h x))))." 
#*(lambda (x) 
(reduce #'funcan functions :from-end t .-initial-value x))) 

Answer 6.5 

(defun compose (&rest functions) 
"Return the function that is the composition of all the args. 

i.e. (compose f g h) = (lambda (x) (f (g (h x))))." 
(case (length functions) 
(0 #'identity) 
(1 (first functions)) 
(2 (let ((f (first functions)) 

(g (second functions))) 
#'(lambda (x) (funcall f (funcall g x))))) 
(t #*(lambda (x) 
(reduce #'funcall functions :from-end t 
:initial-value x))))) 

Answer 6.8 

(defun tree-search (states goal-p successors combiner) 
"Find a state that satisfies goal-p. Start with states, 
and search according to successors and combiner." 
(loop 

(cond ((null states) (RETURN fail)) 
((funcall goal-p (first states)) 
(RETURN (first states)) 
(t (setf states 

(funcall combiner 
(funcall successors (first states)) 
(rest states)))))))) 

Answer 6.9 

(defun sorter (cost-fn) 
"Return a combiner function that sorts according to cost-fn." 
#'(lambda (new old) 

(merge 'list (sort new #'> :key cost-fn) 
old #'> :key cost-fn))) 


<a id='page-218'></a>

Answer 6.11 

(defun search-n (start . goal-p successors cost-fn beam-width) 
"Find . solutions to a search problem, using beam search." 
(let ((solutions nil)) 

(beam-search 
start #*(lambda (x) 

(cond ((not (funcall goal-p x)) nil) 
((= . 0) X) 
(t (decf n) 

(push X solutions) 
nil))) 
successors cost-fn beam-width) 
solutions)) 


## Chapter 7
<a id='page-219'></a>

STUDENT: Solving Algebra 
Word Problems 

[This] is an example par excellence of the power of 
using meaning to solve linguistic problems. 

-Marvin Minsky (1968) 
MIT computer scientist 

S
S
TUDENT was another early language understanding program, written by Daniel Bobrow 
as his Ph.D. research project in 1964. It was designed to read and solve the kind of word 
problems found in high school algebra books. An example is: 

If the number of customers Tom gets is twice the square of 20% of the number of advertisements 
he runs, and the number of advertisements is 45, then what is the number of customers 
Tom gets? 

STUDENT could correctly reply that the number of customers is 162. To do this, STUDENT must be 
far more sophisticated than ELIZA; it must process and "understand" a great deal of the input, 
rather than just concentrate on a few key words. And it must compute a response, rather than 
just fill in blanks. However, we shall see that the STUDENT program uses little more than the 
pattern-matching techniques of ELIZA to translate the input into a set of algebraic equations. 
From there, it must know enough algebra to solve the equations, but that is not very difficult. 


<a id='page-220'></a>

The version of STUDENT we develop here is nearly a full implementation of the 
original. However, remember that while the original was state-of-the-art as of 1964, 
AI has made some progress in a quarter century, as subsequent chapters will attempt 
to show. 

7.1 Translating English into Equations 
The description of STUDENT is: 

1. Break the input into phrases that will represent equations. 
2. Break each phrase into a pair of phrases on either side of the = sign. 
3. Break these phrases down further into sums and products, and so on, until 
finally we bottom out with numbers and variables. (By "variable" here, I mean 
"mathematical variable," which is distinct from the idea of a "pattern-matching 
variable" as used in pat-match in chapter 6). 
4. Translate each English phrase into a mathematical expression. We use the idea 
of a rule-based translator as developed for ELIZA. 
5. Solve the resulting mathematical equations, coming up with a value for each 
unknown variable. 
6. Print the values of all the variables. 
For example, we might have a pattern of the form (I f ?x then ?y), with an associated 
response that says that ?x and ?y will each be equations or lists of equations. 
Applying the pattern to the input above, ?y would have the value (what is the 
number of customers Tom gets). Another pattern of the form (?x i s ?y) could have 
a response corresponding to an equation where ?x and ?y are the two sides of the 
equation. We could then make up a mathematical variable for (what) and another 
for (the number of customers Tom gets). We would recognize this later phrase as 
a variable because there are no patterns to break it down further. In contrast, the 
phrase (twice the square of 20 per cent of the number of advertisements 
he r uns) could match a pattern of the form (twi ce ?x) and transform to (* 2 (the 
square of 20 per cent of the number of advertisements he runs)), and by 
furtherapplyingpatternsof the form (the square of ?x) and (?x per cent of 
?y) we could arrive at a final response of (* 2 (expt (* (/ 20 100) n) 2)), where 
. is the variable generated by (the number of advertisements he runs). 

Thus, we need to represent variables, expressions, equations, and sets of equations. 
The easiest thing to do is to use something we know: represent them just as 
Lisp itself does. Variables will be symbols, expressions and equations will be nested 


<a id='page-221'></a>
lists with prefix operators, and sets of equations will be lists of equations. With that 
in mind, we can define a list of pattern-response rules corresponding to the type of 
statements found in algebra word problems. The structure definition for a rule is 
repeated here, and the structure exp, an expression, is added. 1 hs and rhs stand for 
left- and right-hand side, respectively. Note that the constructor mkexp is defined as a 
constructor that builds expressions without taking keyword arguments. In general, 
the notation (: constructor fn args) creates a constructor function with the given 
name and argument Ust.^ 

(defstruct (rule (:type list)) pattern response) 

(defstruct (exp (:type list) 
(:constructor mkexp (Ihs op rhs))) 
op Ihs rhs) 

(defun exp-p (x) (consp x)) 
(defun exp-args (x) (rest x)) 

We ignored commas and periods in ELIZA, but they are crucial for STUDENT, SO we 
must make allowances for them. The problem is that a "," in Lisp normally can be 
used only within a backquote construction, and a "." normally can be used only as a 
decimal point or in a dotted pair. The special meaning of these characters to the Lisp 
reader can be escaped either by preceding the character with a backslash (\,) or by 
surrounding the character by vertical bars (I J) . 

(pat-match-abbrev '?x* '(?* ?x)) 
(pat-match-abbrev '?y* *(?* ?y)) 

(defparameter *student-rules* (mapcar #'expand-pat-match-abbrev 

'(((?x* I.I) ?x) 
((?x* I.I ?y*) (?x ?y)) 
((if ?x* IJ then ?y*) (?x ?y)) 
((if ?x* then ?y*) (?x ?y)) 
((if ?x* IJ ?y*) (?x ?y)) 
((?x* . and ?y*) (?x ?y)) 
((find ?x* and ?y*) ((= to-find-1 ?x) (= to-find-2 ?y))) 
((find ?x*) (= to-find ?x)) 
((?x* equals ?y*) (= ?x ?y)) 
((?x* same as ?y*) (= ?x ?y)) 
((?x* = ?y*) (= ?x ?y)) 
((?x* is equal to ?y*) (= ?x ?y)) 
((?x* is ?y*) (= ?x ?y)) 
((?x* - ?y*) (- ?x ?y)) 
((?x* minus ?y*) (- ?x ?y)) 

^Page 316 of Common Lisp the Language says, "Because a constructor of this type operates 
By Order of Arguments, it is sometimes known as a BOA constructor." 


<a id='page-222'></a>

((difference between ?x* and ?y*) (- ?y ?x)) 
((difference ?x* and ?y*) (- ?y ?x)) 
((?x* + ?y*) (+ ?x ?y)) 
((?x* plus ?y*) (+ ?x ?y)) 
((sum ?x* and ?y*) (+ ?x ?y)) 
((product ?x* and ?y*) (* ?x ?y)) 
((?x* * ?y*) (* ?x ?y)) 
((?x* times ?y*) (* ?x ?y)) 
((?x* / ?y*) (/ ?x ?y)) 
((?x* per ?y*) (/ ?x ?y)) 
((?x* divided by ?y*) (/ ?x ?y)) 
((half ?x*) (/ ?x 2)) 
((one half ?x*) (/ ?x 2)) 
((twice ?x*) (* 2 ?x)) 
((square ?x*) (* ?x ?x)) 
((?x* % less than ?y*) (* ?y (/ (- 100 ?x) 100))) 
((?x* % more than ?y*) (* ?y (/ (+ 100 ?x) 100))) 
((?x* % ?y*) (* (/ ?x 100) ?y))))) 

The main section of STUDENT will search through the list of rules for a response, just 

as ELIZA did. The first point of deviation is that before we substitute the values of the 

pat-match variables into the response, we must first recursively translate the value 

of each variable, using the same list of pattern-response rules. The other difference 

is that once we're done, we don't just print the response; instead we have to solve the 

set of equations and print the answers. The program is summarized in figure 7.1. 
Before looking carefully at the program, let's try a sample problem: "If . is 3, what 
is twice z?" Applying the rules to the input gives the following trace: 

Input: (If . is 3. what is twice z) 
Rule: ((if ?x IJ ?y) (?x ?y)) 
Binding: ((?x . (z is 3)) (?y . (what is twice z))) 

Input: (z is 3) 
Rule: ((?x is ?y) (= ?x ?y)) 
Result: (= . 3) 

Input: (what is twice . ?) 
Rule: ((?x is ?y) (= ?x ?y)) 
Binding: ((?x . what) (?y . (twice z))) 

Input: (twice z) 
Rule: ((twice ?x) (* 2 ?x)) 
Result: (* 2 z) 

Result: (= what (* 2 z)) 
Result: ((= . 3) (= what (* 2 z))) 

There are two minor complications. First, we agreed to implement sets of equations 
as lists of equations. For this example, everything worked out, and the response 


<a id='page-223'></a>
Top-Level Function 

Student Solve certain algebra word problems. 

Special Variables 

*student-rules* A list of pattern/response pairs. 

Data Types 

exp An operator and its arguments. 
rule A pattern and response. 

Major Functions 

translate-to-expression Translate an English phrase into an equation or expression. 
translate-pair Translate the value part of the pair into an equation or expression. 
create-list-of-equations Separate out equations embedded in nested parens. 
solve-equations Print the equations and their solution. 
solve Solve a system of equations by constraint propagation. 

Auxiliary Fimctions 

isolate Isolate the lone variable on the left-hand side of an expression. 

noise-word-p Is this a low-content word that can be safely ignored? 

make-variable Create a variable name based on the given list of words. 

print-equations Print a list of equations. 

inverse-op I.e., the inverse of + is -. 
unknown-p Is the argument an unknown (variable)? 

in-exp True if X appears anywhere in exp. 
no-unknown Returns true if there are no unknowns in exp. 
one-unknown Returns the single unknown in exp, if there is exactly one. 

commutative-p Is the operator commutative? 

solve-arithmetic Perform arithmetic on rhs of an equation. 

binary-exp-p Is this a binary expression? 

prefix->infix Translate prefix to infix expressions. 

mkexp Make an expression. 

Previously Defined Functions 

pat-match Match pattern against an input, (p. 180) 
rule-based-translator Apply a set of rules, (p. 189) 

Figure 7.1: Glossary for the STUDENT Program 

was a list of two equations. But if nested patterns are used, the response could be 
something like (( = a 5) ((= b (+ a 1)) (= c (->-a b)))), which is not a list of 
equations. The function create -1 i st-of-equati ons transforms a response like this 
into a proper list of equations. The other complication is choosing variable names. 
Givenalistof words like (the number of customers Tom gets), we want to choose 
a symbol to represent it. We will see below that the symbol customers is chosen, but 
that there are other possibilities. 

Here is the main function for STUDENT. It first removes words that have no content, 
then translates the input to one big expression with translate-to-expression, 
and breaks that into separate equations with create-list-of-equations. Finally, 
the function solve-equations does the mathematics and prints the solution. 


<a id='page-224'></a>

(defun Student (words) 
"Solve certain Algebra Word Problems." 
(solve-equations 

(create-list-of-equations 
(translate-to-expression (remove-if #*noise-word-p words))))) 

The function translate-to-expression is a rule-based translator. It either finds 
some rule in *student-rules* to transform the input, or it assumes that the entire input 
represents a single variable. The function translate-pair takes a variable/value 
binding pair and translates the value by a recursive call to translate-to - express ion. 

(defun translate-to-expression (words) 
"Translate an English phrase into an equation or expression." 
(or (rule-based-translator 

words *student-rules* 
:rule-if #'rule-pattern :rule-then #*rule-response 
:action #'(lambda (bindings response) 

(sublis (mapcar #'translate-pair bindings) 
response))) 
(make-variable words))) 

(defun translate-pair (pair) 
"Translate the value part of the pair into an equation or expression." 
(cons (binding-var pair) 

(translate-to-expression (binding-val pair)))) 

The function create-1 ist-of-equations takes a single expression containing embedded 
equations and separates them into a list of equations: 

(defun create-1ist-of-equations (exp) 
"Separate out equations embedded in nested parens." 
(cond ((null exp) nil) 

((atom (first exp)) (list exp)) 
(t (append (create-1ist-of-equations (first exp)) 
(create-1ist-of-equations (rest exp)))))) 

Finally, the function make-vari abl e creates a variable to represent a Ust of v^ords. 
We do that by first removing all "noise words" from the input, and then taking the 
first symbol that remains. So, for example, "the distance John traveled" and "the 
distance traveled by John" will both be represented by the same variable, di stance, 
which is certainly the right thing to do. However, "the distance Mary traveled" will 
also be represented by the same variable, which is certainly a mistake. For (the 
number of customers Tom gets), the variable will be customers, since the, of and 
number are all noise words. This will match (the customers mentioned above) and 


<a id='page-225'></a>
(the number of customers), but not (Tom's customers). For now, we will accept 
the first-non-noise-word solution, but note that exercise 7.3 asks for a correction. 

(defun make-variable (words) 
"Create a variable name based on the given list of words" 
The list of words will already have noise words removed 
(first words)) 

(defun noise-word-p (word) 
"Is this a low-content word that can be safely ignored?" 
(member word '(a an the this number of $))) 

7.2 Solving Algebraic Equations 
The next step is to write the equation-solving section of STUDENT. This is more an 
exercise in elementary algebra than in AI, but it is a good example of a symbol-
manipulation task, and thus an interesting programming problem. 

The STUDENT program mentioned the function sol ve-equati ons, passing it one 
argument, a Hst of equations to be solved, sol ve-equati ons prints the Hst of equations, 
attempts to solve them using sol ve, and prints the result. 

(defun solve-equations (equations) 
"Print the equations and their solution " 
(print-equations "The equations to be solved are:" equations) 
(print-equations "The solution is:" (solve equations nil))) 

The real work is done by sol ve, which has the following specification: (1) Find 
an equation with exactly one occurrence of an unknown in it. (2) Transform that 
equation so that the unknown is isolated on the left-hand side. This can be done if 
we limit the operators to +, -, *, and /. (3) Evaluate the arithmetic on the right-hand 
side, yielding a numeric value for the unknown. (4) Substitute the numeric value 
for the unknown in all the other equations, and remember the known value. Then 
try to solve the resulting set of equations. (5) If step (1) fails—if there is no equation 
with exactly one unknown—then just return the known values and don't try to solve 
anything else. 

The function sol ve is passed a system of equations, along with a list of known 
variable/value pairs. Initially no variables are known, so this list will be empty, 
sol ve goes through the list of equations searching for an equation with exactly one 
unknown. If it can find such an equation, it caHs isolate to solve the equation 
in terms of that one unknown, solve then substitutes the value for the variable 
throughout the list of equations and calls itself recursively on the resulting list. Each 


<a id='page-226'></a>

time solve calls itself, it removes one equation from the Ust of equations to be solved, 
and adds one to the Ust of known variable/value pairs. Since the list of equations is 
always growing shorter, sol ve must eventually terminate. 

(defun solve (equations known) 

"Solve a system of equations by constraint propagation." 
Try to solve for one equation, and substitute its value into 
the others. If that doesn't work, return what is known, 

(or (some #*(lambda (equation) 
(let ((x (one-unknown equation))) 
(when X 
(let ((answer (solve-arithmetic 
(isolate equation x)))) 
(solve (subst (exp-rhs answer) (exp-lhs answer) 
(remove equation equations)) 
(cons answer known)))))) 
equations) 
known)) 

isolate is passed an equation guaranteed to have one unknown. It returns an 
equivalent equation with the unknown isolated on the left-hand side. There are 
five cases to consider: when the unknown is alone on the left, we're done. The 
second case is when the unknown is anywhere on the right-hand side. Because 
is commutative, we can reduce the problem to solving the equivalent equation with 
left- and right-hand sides reversed. 

Next we have to deal with the case where the unknown is in a complex expression 
on the left-hand side. Because we are allowing four operators and the unknown can 
be either on the right or the left, there are eight possibilities. Letting X stand for 
an expression containing the unknown and A and . stand for expressions with no 
unknowns, the possibilities and their solutions are as follows: 

(1)X*A=B X=B/A (5)A*X=B X=B/A 
(2)X+A=B ^ X=B-A (6)A+X=B ^ X=B-A 
(3)X/A=B => X=B*A (7)A/X=B X=A/B 
(4)X-A=B X=B+A (8)A-X=B => X=A-B 

Possibilities (1) through (4) are handled by case III, (5) and (6) by case IV, and (7) 
and (8) by case V. In each case, the transformation does not give us the final answer, 
since X need not be the unknown; it might be a complex expression involving the 
unknown. So we have to caU i sol ate again on the resulting equation. The reader 
should try to verify that transformations (1) to (8) are valid, and that cases III to V 
implement them properly. 


<a id='page-227'></a>
(defun isolate (e x) 

"Isolate the lone . in e on the left-hand side of e." 
This assumes there is exactly one . in e, 
and that e is an equation, 

(cond ((eq (exp-lhs e) x) 
Case I: X = A -> X = . 
e) 

((in-exp X (exp-rhs e)) 
;; Case II: A = f(X) -> f(X) = A 
(isolate (mkexp (exp-rhs e) '= (exp-lhs e)) x)) 

((in-exp X (exp-lhs (exp-lhs e))) 
Case III: f(X)*A = . -> f(X) = B/A 
(isolate (mkexp (exp-lhs (exp-lhs e)) *= 

(mkexp (exp-rhs e) 
(inverse-op (exp-op (exp-lhs e))) 
(exp-rhs (exp-lhs e)))) x)) 

((commutative-p (exp-op (exp-lhs e))) 
:; Case IV: A*f(X) = . -> f(X) = B/A 
(isolate (mkexp (exp-rhs (exp-lhs e)) ' = 

(mkexp (exp-rhs e) 
(inverse-op (exp-op (exp-lhs e))) 
(exp-lhs (exp-lhs e)))) x)) 

(t :; Case V: A/f(X) = . -> f(X) = A/B 
(isolate (mkexp (exp-rhs (exp-lhs e)) ' = 

(mkexp (exp-lhs (exp-lhs e)) 
(exp-op (exp-lhs e)) 
(exp-rhs e))) x)))) 

Recall that to prove a function is correct, we have to prove both that it gives the correct 
answer when it terminates and that it will eventually terminate. For a recursive 
function with several alternative cases, we must show that each alternative is valid, 
and also that each alternative gets closer to the end in some way (that any recursive 
calls involve 'simpler' arguments). For i sol ate, elementary algebra will show that 
each step is valid—or at least nearly valid. Dividing both sides of an equation by 
0 does not yield an equivalent equation, and we never checked for that. It's also 
possible that similar errors could sneak in during the call to eval. However, if we 
assume the equation does have a single valid solution, then i sol ate performs only 
legal transformations. 

The hard part is to prove that 1 sol ate terminates. Case I clearly terminates, and 
the others all contribute towards isolating the unknown on the left-hand side. For 
any equation, the sequence will be first a possible use of case II, followed by a number 
of recursive calls using cases III to V. The number of calls is bounded by the number 
of subexpressions in the equation, since each successive call effectively removes an 
expression from the left and places it on the right. Therefore, assuming the input is 


<a id='page-228'></a>

of finite size, we must eventually reach a recursive call to i sol ate that will use case I 
and terminate. 

When i sol ate returns, the right-hand side must consist only of numbers and 
operators. We could easily write a function to evaluate such an expression. However, 
we don't have to go to that effort, since the function already exists. The data structure 
exp was carefully selected to be the same structure (lists with prefix functions) used 
by Lisp itself for its own expressions. So Lisp will find the right-hand side to be an 
acceptable expression, one that could be evaluated if typed in to the top level. Lisp 
evaluates expressions by calling the function eval, so we can call eval directly and 
have it return a number. The function sol ve -a ri t hmet i c returns an equation of the 
form (=var number). 

Auxiliary functions for sol ve are shown below. Most are straightforward, but 
I will remark on a few of them. The function pref ix->inf ix takes an expression 
in prefix notation and converts it to a fully parenthesized infix expression. Unlike 
i sol ate, it assumes the expressions will be implemented as lists, pref i x->i nf i . is 
used by pri nt-equati ons to produce more readable output. 

(defun print-equations (header equations) 
"Print a list of equations." 
(format t "~%~a"'{~% ~{ ~a~}~}~%" header 

(mapcar #*prefix->infix equations))) 

(defconstant operators-and-inverses 
'((+ -) (- +) (* /) (/ *) (= =))) 

(defun inverse-op (op) 
(second (assoc op operators-and-inverses))) 

(defun unknown-p (exp) 
(symbolp exp)) 

(defun in-exp (x exp) 
"True if X appears anywhere in exp" 
(or (eq . exp) 

(and (exp-p exp) 
(or (in-exp . (exp-lhs exp)) (in-exp . (exp-rhs exp)))))) 

(defun no-unknown (exp) 
"Returns true if there are no unknowns in exp." 
(cond ((unknown-p exp) nil) 

((atom exp) t) 
((no-unknown (exp-lhs exp)) (no-unknown (exp-rhs exp))) 
(t nil))) 


<a id='page-229'></a>
(defun one-unknown (exp) 
"Returns the single unknown in exp, if there is exactly one." 
(cond ((unknown-p exp) exp) 

((atom exp) nil) 
((no-unknown (exp-lhs exp)) (one-unknown (exp-rhs exp))) 
((no-unknown (exp-rhs exp)) (one-unknown (exp-lhs exp))) 
(t nil))) 

(defun commutative-p (op) 
"Is operator commutative?" 
(member op '(+*=))) 

(defun solve-arithmetic (equation) 
"Do the arithmetic for the right-hand side." 
This assumes that the right-hand side is in the right form, 
(mkexp (exp-lhs equation) *= (eval (exp-rhs equation)))) 

(defun binary-exp-p (x) 
(and (exp-p x) (= (length (exp-args x)) 2))) 

(defun prefix->infix (exp) 
"Translate prefix to infix expressions." 
(if (atom exp) exp 

(mapcar #'prefix->infix 

(if (binary-exp-p exp) 
(list (exp-lhs exp) (exp-op exp) (exp-rhs exp)) 
exp)))) 

Here's an example of sol ve-equati ons in action, with a system of two equations. 

The reader should go through the trace, discovering which case was used at each call 

to i sol ate, and verifying that each step is accurate. 

> (trace isolate solve) 
(isolate solve) 

> (solve-equations '((= (+ 3 4) (* (- 5 (+ 2 x)) 7)) 
(= (+ (* 3 X) y) 12))) 

The equations to be solved are: 
(3 + 4) = ((5 - (2 + X)) * 7) 
((3 * X) + Y) = 12 

(1 ENTER SOLVE: ((= (+ 3 4) (* (- 5 (+ 2 X)) 7)) 
(= (+ (* 3 X) Y) 12)) NIL) 
(1 ENTER ISOLATE: (= (+ 3 4) (* (- 5 (+ 2 X)) 7)) X) 
(2 ENTER ISOLATE: (= (* (- 5 (+ 2 X)) 7) (+ 3 4)) X) 
(3 ENTER ISOLATE: (= (- 5 (+ 2 X)) (/ (+ 3 4) 7)) X) 

(4 ENTER ISOLATE: (= (+ 2 X) (- 5 (/ (+ 3 4) 7))) X) 
(5 ENTER ISOLATE: (= X (- (- 5 (/ (+ 3 4) 7)) 2)) X) 
(5 EXIT ISOLATE: (= X (- (- 5 (/ (+ 3 4) 7)) 2))) 

(4 EXIT ISOLATE: (= X (- (- 5 (/ (+ 3 4) 7)) 2))) 


<a id='page-230'></a>

(3 EXIT ISOLATE: (= . (- (- 5 (/ (+ 3 4) 7)) 2))) 

(2 EXIT ISOLATE: (= X (- (- 5 (/ (+ 3 4) 7)) 2))) 

(1 EXIT ISOLATE: (= X (- (- 5 (/ (+ 3 4) 7)) 2))) 

(2 ENTER SOLVE: ((= (+ (* 3 2) Y) 12)) ((= X 2))) 

(1 ENTER ISOLATE: (= (+ (* 3 2) Y) 12) Y) 

(2 ENTER ISOLATE: (= Y (- 12 (* 3 2))) Y) 

(2 EXIT ISOLATE: (= Y (- 12 (* 3 2)))) 

(1 EXIT ISOLATE: (= Y (- 12 (* 3 2)))) 

(3 ENTER SOLVE: NIL ((= Y 6) (= X 2))) 

(3 EXIT SOLVE: ((= Y 6) (= X 2))) 

(2 EXIT SOLVE: ((= Y 6) (= X 2))) 

(1 EXIT SOLVE: ((= Y 6) (= X 2))) 

The solution is: 

Y = 6 

X = 2 

NIL 

Now let's tackle the format string "'^%~a~{~% "{ ~a''}~}~%" in print-equations. 
This may look like random gibberish, but there is actually sense behind it. format 
processes the string by printing each character, except that""" indicates some special 
formatting action, depending on the following character. The combination "~%" 
prints a newline, and "~a" prints the next argument to format that has not been 
used yet. Thus the first four characters of the format string, " ", print a newline 
followed by the argument header. The combination "~{" treats the corresponding 
argument as a list, and processes each element according to the specification between 
the " ~{" and the next" ~}". In this case, equati ons is a list of equations, so each one 
gets printed with a newline ("") followed by two spaces, followed by the processing 
of the equation itself as a list, where each element is printed in the "~a" format and 
preceded by a blank. The t given as the first argument to format means to print to 
the standard output; another output stream may be specified there. 

One of the annoying minor holes in Lisp is that there is no standard convention on 
where to print newlines! In C, for example, the very first line of code in the reference 
manual is 

printfC'hello, worldXn"); 

This makes it clear that newlines are printed after each line. This convention is so 
ingrained in the UNIX world that some UNIX programs will go into an infinite loop 
if the last line in a file is not terminated by a newline. In Lisp, however, the function 
pri nt puts in a newline before the object to be printed, and a space after. Some Lisp 
programs carry the newline-before policy over to format, and others use the newline-
af ter policy. This only becomes a problem when you want to combine two programs 
written under different policies. How did the two competing policies arise? In UNIX 
there was only one reasonable policy, because all input to the UNIX interpreter (the 


<a id='page-231'></a>

shell) is terminated by newlines, so there is no need for a newline-before. In some 
Lisp interpreters, however, input can be terminated by a matching right parenthesis. 
In that case, a newline-before is needed, lest the output appear on the same line as 
the input. 

&#9635; Exercise 7.1 [m] Implement pri nt-equati ons using only primitive printing functions 
such as terpri and pri nc, along with explicit loops. 

7.3 Examples 
Now we move on to examples, taken from Bobrow's thesis. In the first example, it is 
necessary to insert a "then" before the word "what" to get the right answer: 

> (student '(If the number of customers Tom gets is twice the square of 
20 % of the number of advertisements he runs I.I 
and the number of advertisements is 45 I J 
then what is the number of customers Tom gets ?)) 

The equations to be solved are: 
CUSTOMERS = (2 * (((20 / 100) * ADVERTISEMENTS) * 

((20 / 100) * ADVERTISEMENTS))) 
ADVERTISEMENTS = 45 
WHAT = CUSTOMERS 

The solution is: 
WHAT = 162 
CUSTOMERS = 162 
ADVERTISEMENTS = 45 

NIL 

Notice that our program prints the values for all variables it can solve for, while 
Bobrow's program only printed the values that were explicitly asked for in the text. 
This is an example of "more is less"—it may look impressive to print all the answers, 
but it is actually easier to do so than to decide just what answers should be printed. 
The following example is not solved correctly: 


<a id='page-232'></a>

> (student '(The daily cost of living for a group is the overhead cost plus 
the running cost for each person times the number of people in 
the group I.I This cost for one group equals $ 100 I,I 
and the number of people in the group is 40 I.I 
If the overhead cost is 10 times the running cost I,I 
find the overhead and running cost for each person I.I)) 

The equations to be solved are: 
DAILY = (OVERHEAD + (RUNNING * PEOPLE)) 
COST = 100 
PEOPLE = 40 
OVERHEAD = (10 * RUNNING) 
TO-FIND-1 = OVERHEAD 
TO-FIND-2 = RUNNING 

The solution is: 
PEOPLE = 40 
COST = 100 

NIL 

This example points out two important limitations of our version of student as 
compared to Bobrow's. The first problem is in naming of variables. The phrases "the 
daily cost of living for a group" and "this cost" are meant to refer to the same quantity, 
but our program gives them the names daily and cost respectively. Bobrow's 
program handled naming by first considering phrases to be the same only if they 
matched perfectly. If the resulting set of equations could not be solved, he would try 
again, this time considering phrases with words in common to be identical. (See the 
following exercises.) 

The other problem is in our sol ve function. Assuming we got the variables 
equated properly, sol ve would be able to boil the set of equations down to two: 

100 = (OVERHEAD + (RUNNING * 40)) 
OVERHEAD = (10 * RUNNING) 

This is a set of two linear equations in two unknowns and has a unique solution at 
RUNNING = 2, OVERHEAD = 20. But our version of sol ve couldn't find this solution, 
since it looks for equations with one unknown. Here is another example that student 
handles well: 

> (student '(Fran's age divided by Robin's height is one half Kelly's 10 I.I 
Kelly's IQ minus 80 is Robin's height I.I 
If Robin is 4 feet tall 1,1 how old is Fran ?)) 

The equations to be solved are: 
(FRAN / ROBIN) = (KELLY / 2) 
(KELLY - 80) = ROBIN 
ROBIN = 4 


<a id='page-233'></a>

HOW = FRAN 

The solution is: 
HOW = 168 
FRAN = 168 
KELLY = 84 
ROBIN = 4 

NIL 

But a slight variation leads to a problem: 

> (student '(Fran's age divided by Robin's height is one half Kelly's IQ I.I 
Kelly's IQ minus 80 is Robin's height I.I 
If Robin is 0 feet tall . how old is Fran ?)) 

The equations to be solved are: 
(FRAN / ROBIN) = (KELLY / 2) 
(KELLY - 80) = ROBIN 
ROBIN = 0 
HOW = FRAN 

The solution is: 
HOW = 0 
FRAN = 0 
KELLY = 80 
ROBIN = 0 

NIL 

There is no valid solution to this problem, because it involves dividing by zero (Robin's 
height). But student is willing to transform the first equation into: 

FRAN = ROBIN * (KELLY / 2) 

and then substitutes to get 0 for FRAN. Worse, dividing by zero could also come up 
inside eval: 

> (student '(Fran's age times Robin's height is one half Kelly's IQ I.I 
Kelly's IQ minus 80 is Robin's height I.I 
If Robin is 0 feet tall IJ how old is Fran ?)) 

The equations to be solved are: 
(FRAN * ROBIN) = (KELLY / 2) 
(KELLY - 80) = ROBIN 
ROBIN = 0 
HOW = FRAN 


<a id='page-234'></a>

>Error: There was an attempt to divide a number by zero 

However, one could claim that nasty examples with division by zero don't show up 
in algebra texts. 

In summary, STUDENT behaves reasonably well, doing far more than the toy 
program ELIZA. STUDENT is also quite efficient; on my machine it takes less than 
one second for each of the prior examples. However, it could still be extended to 
have more powerful equation-solving capabilities. Its linguistic coverage is another 
matter. While one could add new patterns, such patterns are really just tricks, and 
don't capture the underlying structure of English sentences. That is why the STUDENT 
approach was abandoned as a research topic. 

7.4 History and References 
Bobrow's Ph.D. thesis contains a complete description of STUDENT. It is reprinted 
in Minsky 1968. Since then, there have been several systems that address the same 
task, with increased sophistication in both their mathematical and linguistic ability. 
Wong (1981) describes a system that uses its understanding of the problem to get 
a better linguistic analysis. Sterling et al. (1982) present a much more powerful 
equation solver, but it does not accept natural language input. Certainly Bobrow's 
language analysis techniques were not very sophisticated by today's measures. But 
that was largely the point: if you know that the language is describing an algebraic 
problem of a certain type, then you don't need to know very much linguistics to get 
the right answer most of the time. 

7.5 Exercises 
&#9635; Exercise 7.2 [h] We said earlier that our program was unable to solve pairs of linear 
equations, such as: 

100 = (OVERHEAD + (RUNNING * 40)) 

OVERHEAD = (10 * RUNNING) 

The original STUDENT could solve these equations. Write a routine to do so. You may 
assume there will be only two equations in two unknowns if you wish, or if you are 
more ambitious, you could solve a system of . linear equations with . unknowns. 

&#9635; Exercise 7.3 [h] Implement a version of Bobrow's variable-naming algorithm. Instead 
of taking the first word of each equation, create a unique symbol, and associate 


<a id='page-235'></a>
with it the entire list of words. In the first pass, each nonequal list of words will be 
considered a distinct variable. If no solution is reached, word lists that share words 
in common are considered to be the same variable, and the solution is attempted 
again. For example, an input that contains the phrases "the rectangle's width" and 
"the width of the rectangle" might assign these two phrases the variables . 1 and .2. If 
an attempt to solve the problem yields no solutions, the program should realize that 
vl and . 2 have the words "rectangle" and "width" in common, and add the equation 
(= vl v2) and try again. Since the variables are arbitrary symbols, the printing 
routine should probably print the phrases associated with each variable rather than 
the variable itself. 

&#9635; Exercise 7.4 [h] The original STUDENT also had a set of "common knowledge" equations 
that it could use when necessary. These were mostly facts about conversion 
factors, such as (1 inch = 2.54 cm). Also included wereequations like (distance 
equals rate times time), which could be used to solve problems like "If the distance 
from Anabru to Champaign is 10 miles and the time it takes Sandy to travel 
this distance is 2 hours, what is Sandy's rate of speed?" Make changes to incorporate 
this facility. It probably only helps in conjunction with a solution to the previous 
exercise. 

&#9635; Exercise 7.5 [h] Change student so that it prints values only for those variables 
that are being asked for in the problem. That is, given the problem "X is 3. Y is 4. 
How much is X + Y?" it should not print values for X and Y. 

&#9635; Exercise 7.6 [m] Try STUDENT on the following examples. Make sure you handle 
special characters properly: 

(a) The price of a radio is 69.70 dollars. If this price is 15% less than the marked 
price, find the marked price. 
(b) The number of soldiers the Russians have is one half of the number of guns 
they have. The number of guns they have is 7000. What is the number of soldiers 
they have? 
(c) If the number of customers Tom gets is twice the square of 20 % of the number 
of advertisements he runs, and the number of advertisements is 45, and the profit 
Tom receives is 10 times the number of customers he gets, then what is the profit? 
(d) The average score is 73, The maximum score is 97. What is the square of the 
difference between the average and the maximum? 
(e) Tom is twice Mary's age, and Jane's age is half the difference between Mary 
and Tom. If Mary is 18 years old, how old is Jane? 
(f)Whatis4 + 5*14/7? 

{g)xxb = c-\-d.bxc = x.x = b-\-b.b = 5. 


<a id='page-236'></a>

&#9635; Exercise 7.7 [h] Student's infix-to-prefix rules account for the priority of operators 
properly, but they don't handle associativity in the standard fashion. For example, 
(12 6 
3) 
translates to (- 12 (- 6 3)) or 9, when the usual convention is to 
interpret this as ((- 
12 6) 3) or 3. Fix student to handle this convention. 

&#9635; Exercise 7.8 [d] Find a mathematically oriented domain that is sufficiently limited 
so that STUDENT can solve problems in it. The chemistry of solutions (calculating pH 
concentrations) might be an example. Write the necessary *student-rules* , and 
test the resulting program. 

&#9635; Exercise 7.9 [m]
efficient version. 
Analyze the complexity of one-unknown and implement a more 

&#9635; Exercise 7.10 [h] Bobrow's paper on STUDENT (1968) includes an appendix that 
abstractly characterizes all the problems that his system can solve. Generate a 
similar characterization for this version of the program. 

7.6 Answers 
Answer 7.1 

(defun print-equations (header(terpri) 
(princ header) 
(dolist (equation equations) 

(terpri) 
(princ " ") 

 equations) 

(dolist (x (prefix->infix equation)) 
(princ " ") 
(princ x)))) 


<a id='page-237'></a>

Answer 7.9 one-unknown is very inefficient because it searches each subcomponent 
of an expression twice. For example, consider the equation: 

(= (+ (+ . 2) (+ 3 4)) (+ (+ 5 6) (+ 7 8))) 

To decide if this has one unknown, one - unknown will call no - unknown on the left-hand 
side, and since it fails, call it again on the right-hand side. Although there are only 
eight atoms to consider, it ends up calling no-unknown 17 times and one-unknown 4 
times. In general, for a tree of depth n, approximately 2^ calls to no-unknown are 
made. This is clearly wasteful; there should be no need to look at each component 
more than once. 

The following version uses an auxiliary function, f i nd - one - un known, that has an 
accumulator parameter, unknown. This parameter can take on three possible values: 
nil, indicating that no unknown has been found; or the single unknown that has 
been found so far; or the number 2 indicating that two unknowns have been found 
and therefore the final result should be nil. The function f i nd - one - unknown has four 
cases: (1) If we have already found two unknowns, then return 2 to indicate this. (2) If 
the input expression is a nonatomic expression, then first look at its left-hand side 
for unknowns, and pass the result found in that side as the accumulator to a search 
of the right-hand side. (3) If the expression is an unknown, and if it is the second one 
found, return 2; otherwise return the unknown itself. (4) If the expression is an atom 
that is not an unknown, then just return the accumulated result. 

(defun one-unknown (exp) 
"Returns the single unknown in exp, if there is exactly one." 
(let ((answer (find-one-unknown exp nil))) 

;; If there were two unknowns, return nil; 
otherwise return the unknown (if there was one) 

(if (eql answer 2) 
nil 
answer))) 

(defun find-one-unknown (exp unknown) 
"Assuming UNKNOWN is the unknown(s) found so far, decide 
if there is exactly one unknown in the entire expression." 
(cond ((eql unknown 2) 2) 

((exp-p exp) 

(find-one-unknown 
(exp-rhs exp) 
(find-one-unknown (exp-lhs exp) unknown))) 

((unknown-p exp) 

(if unknown 
2 
exp)) 

(t unknown))) 


## Chapter 8
<a id='page-238'></a>
CHAPTER 8 

Symbolic Mathematics: 
A Simplification Program 

Our life is frittered away by detail— 
Simplify, simplify. 
-Henry David Thoreau, Waiden (1854) 

"Symbolic mathematics" is to numerical mathematics as algebra is to arithmetic: it deals 
with variables and expressions rather than just numbers. Computers were first developed 
L..^primarily to solve arithmetic problems: to add up large columns of numbers, to multiply 
many-digit numbers, to solve systems of linear equations, and to calculate the trajectories of 
ballistics. Encouraged by success in these areas, people hoped that computers could also be used 
on more complex problems; to differentiate or integrate a mathematical expression and come 
up with another expression as the answer, rather than just a number. Several programs were 
developed along these lines in the 1960s and 1970s. They were used primarily by professional 
mathematicians and physicists with access to large mainframe computers. Recently, programs 
like MATHLAB, DERIVE, and .......... have given these capabilities to the average personal 
computer user. 


<a id='page-239'></a>

It is interesting to look at some of the history of symbolic algebra, beginning 
in 1963 with SAINT, James Slagle's program to do symbolic integration. Originally, 
SAINT was heralded as a triumph of AI. It used general problem-solving techniques, 
similar in kind to GPS, to search for solutions to difficult problems. The program 
worked its way through an integration problem by choosing among the techniques 
known to it and backing up when an approach failed to pan out. SAINT'S behavior 
on such problems was originally similar to (and eventually much better than) the 
performance of undergraduate calculus students. 

Over time, the AI component of symbolic integration began to disappear. Joel 
Moses implemented a successor to SAINT called SiN. It used many of the same techniques, 
but instead of relying on search to find the right combination of techniques, 
it had additional mathematical knowledge that led it to pick the right technique at 
each step, without any provision for backing up and trying an alternative. SiN solved 
more problems and was much faster than SAINT, although it was not perfect: it still 
occasionally made the wrong choice and failed to solve a problem it could have. 

By 1970, the mathematician R. Risch and others developed algorithms for indefinite 
integration of any expression involving algebraic, logarithmic, or exponential 
extensions of rational functions. In other words, given a "normal" function, the Risch 
algorithm will return either the indefinite integral of the function or an indication 
that no closed-form integral is possible in terms of elementary functions. Such work 
effectively ended the era of considering integration as a problem in search. 

SIN was further refined, merged with parts of the Risch algorithm, and put into the 
evolving MACSYMA^ program. For the most part, refinement of MACSYMA consisted 
of the incorporation of new algorithms. Few heuristics of any sort survive. Today 
MACSYMA is no longer considered an AI program. It is used daily by scientists and 
mathematicians, while ELIZA and STUDENT are now but historical footnotes. 

With ELIZA and STUDENT we were able to develop miniature programs that duplicated 
most of the features of the original. We won't even try to develop a program 
worthy of the name MACSYMA; instead we will settle for a modest program to do symbolic 
simplification, which we will call (simply) simpli f i er. Then, we will extend 
simpli f i er to do differentiation, and some integration problems. The idea is that 
given an expression like (2-1)a: + 0, we want the program to compute the simplified 
form X. 

According to the Mathematics Dictionary Qames and James 1949), the word "simplified" 
is "probably the most indefinite term used seriously in mathematics." The 
problem is that "simplified" is relative to what you want to use the expression for 
next. Which is simpler, x^ 3x + 2 or (x -\-l){x -\-2)? The first makes it easier to 

^MACSYMA is the Project MAC SYMbolic MAthematics program. Project MAC is the MIT 
research organization that was the precursor of MIT's Laboratory for Computer Science. 
MAC stood either for Machine-Aided Cognition or Multiple-Access Computer, according to 
one of their annual reports. The cynical have claimed that MAC really stood for Man Against 
Computer. 


<a id='page-240'></a>

integrate or differentiate, the second easier to find roots. We will be content to limit 
ourselves to "obvious" simplifications. For example, . is almost always preferable 
toIx + 0. 

8.1 Converting Infix to Prefix Notation 
We will represent simplifications as a list of rules, much like the rules for STUDENT 
and ELIZA. But since each simplification rule is an algebraic equation, we will store 
each one as an exp rather than as a rul e. To make things more legible, we will write 
each expression in infix form, but store them in the prefix form expected by exp. This 
requires an i nf i x->pref i . function to convert infix expressions into prefix notation. 
We have a choice as to how general we want our infix notation to be. Consider: 

(((a * (X ^ 2)) + (b * X)) + c) 
(a*x^2 + b*x + c) 
(a . ^ 2 + b . + c) 
a x^2 + b*x+c 

The first is fully parenthesized infix, the second makes use of operator precedence 
(multiplication binds tighter than addition and is thus performed first), and the third 
makes use of implicit multiplication as well as operator precedence. The fourth 
requires a lexical analyzer to break Lisp symbols into pieces. 

Suppose we only wanted to handle the fully parenthesized case. To write 
i nf i x->pref i ., one might first look at pref i x->i nf i . (on [page 228](chapter7.md#page-228)) trying to adapt 
it to our new purposes. In doing so, the careful reader might discover a surprise: 
infix->prefix and pref ix->inf ix are in fact the exact same function! Both leave 
atoms unchanged, and both transform three-element lists by swapping the exp-op 
and exp -1 hs. Both apply themselves recursively to the (possibly rearranged) input 
list. Once we discover this fact, it would be tempting to avoid writing i .f i . - >p ref i x, 
and just call pref i x->i nf i . instead. Avoid this temptation at all costs. Instead, define 
i nf i x->pref i . as shown below. The intent of your code will be clearer: 

(defun infix->prefix (infix-exp) 
"Convert fully parenthesized infix-exp to a prefix expression" 
;; Don't use this version for non-fully parenthesized exps! 
(prefix->infix infix-exp)) 

As we saw above, fully parenthesized infix can be quite ugly, with all those extra 
parentheses, so instead we will use operator precedence. There are a number of 
ways of doing this, but the easiest way for us to proceed is to use our previously 
defined tool rule-based-translator and its subtool, pat-match. Note that the third 


<a id='page-241'></a>
clause of infix->prefix, the one that calls rule-based-translator is unusual in 
that it consists of a single expression. Most cond-clauses have two expressions: a test 
and a result, but ones like this mean, "Evaluate the test, and if it is non-nil, return it. 
Otherwise go on to the next clause." 

(defun infix->prefix (exp) 
"Translate an infix expression into prefix notation." 
Note we cannot do implicit multiplication in this system 

(cond ((atom exp) exp) 
((= (length exp) 1) (infix->prefix (first exp))) 
((rule-based-translator exp *infix->prefix-rules* 

:rule-if #'rule-pattern :rule-then #*rule-response 

:action 

#*(lambda (bindings response) 
(sublis (mapcar 
#*(lambda (pair) 
(cons (first pair) 
(infix->prefix (rest pair)))) 
bindings) 
response)))) 
((symbolp (first exp)) 

(list (first exp) (infix->prefix (rest exp)))) 
(t (error "Illegal exp")))) 

Because we are doing mathematics in this chapter, we adopt the mathematical con


vention of using certain one-letter variables, and redefine vari abl e-p so that vari


ables are only the symbols m through z. 

(defun variable-p (exp) 
"Variables are the symbols . through Z." 
put x.y.z first to find them a little faster 
(member exp '(xyzmnopqrstuvw))) 

(pat-match-abbrev 'x+ *(?+ x)) 
(pat-match-abbrev 'y+ '(?+ y)) 

(defun rule-pattern (rule) (first rule)) 
(defun rule-response (rule) (second rule)) 


<a id='page-242'></a>

(defpa rameter *i nfi x->prefi x-rules* 
(mapcar #'expand-pat-match-abbrev 

'(((x+ = y+) (= X y)) 
((- x+) (- X)) 
((+X+) (+x)) 
((x+ + y+) (+ X y)) 
((x+ - y+) (-X y)) 
((x+ * y+) (* X y)) 
((x+ / y+) (/ X y)) 
((x+ ^ y+) r X y)))) 

"A list of rules, ordered by precedence.") 

8.2 Simplification Rules 
Now we are ready to define the simplification rules. We use the definition of the data 
types rule and exp ([page 221](chapter7.md#page-221)) and prefix->inf ix ([page 228](chapter7.md#page-228)) from STUDENT. They 
are repeated here: 

(defstruct (rule (:type list)) pattern response) 

(defstruct (exp (:type list) 
(.'constructor mkexp (Ihs op rhs))) 
op Ihs rhs) 

(defun exp-p (x) (consp x)) 
(defun exp-args (x) (rest x)) 

(defun prefix->infix (exp) 
"Translate prefix to infix expressions." 
(if (atom exp) exp 

(mapcar #'prefix->infix 

(if (binary-exp-p exp) 
(list (exp-lhs exp) (exp-op exp) (exp-rhs exp)) 
exp)))) 

(defun binary-exp-p (x) 
(and (exp-p x) (= (length (exp-args x)) 2))) 

We also use rule-based-translator ([page 188](chapter6.md#page-188)) once again, this time on a list of 

simplification rules. A reasonable list of simplification rules is shown below. This 

list covers the four arithmetic operators, addition, subtraction, multipHcation, and 

division, as well as exponentiation (raising to a power), denoted by the symbol 
Again, it is important to note that the rules are ordered, and that later rules will 
be applied only when earlier rules do not match. So, for example, 0/0 simplifies to 


<a id='page-243'></a>
undef i ned, and not to 1 or 0, because the rule for 0 / 0 comes before the other rules. 
See exercise 8.8 for a more complete treatment of this. 

(defparameter *simplification-rules* (mapcar #'infix->prefix *( 
(x + 0 = X) 
(0 + X = X) 
(x + X = 2 * X) 
(X -0 = X) 
(0 -X = - X) 
(X -X = 0) 
(--X = X) 
(X * 1 = X) 
(1 * X = X) 
(X * 0 = 0) 
(0 * X = 0) 
(X * X = X ^ 2) 
(X / 0 = undefined) 
(0 / X = 0) 
(X / 1 = X) 
(X / X = 1) 
(0 ^ 0 = undefined) 
(X ^ 0 = 1) 
(0 X = 0) 
(1 X = 1) 
(X ^ 1 = X) 
(X ^ -1 = 1 / X) 
(X * (y / X) = y) 
((y / X) * X = y) 
((y * X) / X = y) 
((X * y) / X = y) 
(x + -X = 0) 
((- X) + X = 0) 
(x + y -X = y) 
))) 

(defun " (x y) "Exponentiation" (expt . y)) 

We are now ready to go ahead and write the simplif ier. The main function, s i mp 11 f i e r 
will repeatedly print a prompt, read an input, and print it in simplified form. Input 
and output is in infix and the computation is in prefix, so we need to convert accordingly; 
the function simp does this, and the function simpl 1 fy takes care of a single 
prefix expression. It is summarized in figure 8.1. 


<a id='page-244'></a>

Top-Level Functions 

simplifier A read-simplify-print loop. 
simp Simplify an infix expression. 
simplify Simplify a prefix expression. 

Special Variables 

*infix->prefix-rules* Rules to translate from infix to prefix. 
*s i mpli fi cati on-rules* Rules to simplify an expression. 

Data Types 

exp A prefix expression. 

Atixiliary Functions 

simplify-exp Simplify a non-atomic prefix expression. 
infix->prefix Convert infix to prefix notation. 
variable-p The symbols m through . are variables. 

An alias for expt, exponentiation. 
evaluable Decide if an expression can be numerically evaluated. 
simp-rule Transform a rule into proper format. 
length=1 Is the argument a list of length 1? 

Previous Functions 

pat-match Match pattern against an input, (p. 180) 
rule-based-translator Apply a set of rules, (p. 189) 
pat-match-abbrev Define an abbreviation for use in pat-match. 

Figure 8.1: Glossary for the Simplifier 

Here is the program: 

(defun simplifier () 
"Read a mathematical expression, simplify it, and print the result." 
(loop 

(print 'simplifier>) 
(print (simp (read))))) 

(defun simp (inf) (prefix->infix (simplify (infix->prefix inf)))) 

(defun simplify (exp) 
"Simplify an expression by first simplifying its components." 
(if (atom exp) exp 

(simplify-exp (mapcar #'simplify exp)))) 

(defun simplify-exp (exp) 
"Simplify using a rule, or by doing arithmetic." 
(cond ((rule-based-translator exp *simplification-rules* 

:rule-if #'exp-lhs :rule-then #'exp-rhs 
.'action #'(lambda (bindings response) 

(simplify (sublis bindings response))))) 
((evaluable exp) (eval exp)) 
(t exp))) 


<a id='page-245'></a>

(defun evaluable (exp) 
"Is this an arithmetic expression that can be evaluated?" 
(and (every #'numberp (exp-args exp)) 

(or (member (exp-op exp) '(+-*/)) 
(and (eq (exp-op exp) '") 
(integerp (second (exp-args exp))))))) 

The function simplify assures that any compound expression will be simplified by 
first simplifying the arguments and then calling simplify-exp. This latter function 
searches through the simplification rules, much like use-eliza-rules and 
translate-to-expression. When it finds a match, simplify-exp substitutes in the 
proper variable values and calls simplify on the result, simplify-exp also has the 
ability to call eval to simplify an arithmetic expression to a number. As in STUDENT, 
it is for the sake of this eval that we require expressions to be represented as lists in 
prefix notation. Numeric evaluation is done after checking the rules so that the rules 
can intercept expressions like (/ 1 0) and simplify them to undefined. If we did the 
numeric evaluation first, these expressions would yield an error when passed to e va 1. 
Because Common Lisp supports arbitrary precision rational numbers (fractions), we 
are guaranteed there will be no round-off error, unless the input explicitly includes 
inexact (floating-point) numbers. Notice that we allow computations involving the 
four arithmetic operators, but exponentiation is only allowed if the exponent is an 
integer. That is because expressions like (" 4 1/2) are not guaranteed to return 2 
(the exact square root of 4); the answer might be 2.0 (an inexact number). Another 
problem is that -2 is also a square root of 4, and in some contexts it is the correct 
one to use. 

The following trace shows some examples of the simplifier in action. First we 
show that it can be used as a calculator; then we show more advanced problems. 

> (simplifier) 

SIMPLIFIER> (2 + 2) 

4 

SIMPLIFIER> (5 * 20 + 30 + 7) 

137 

SIMPLIFIER> (5 * . - (4 + 1) * x) 
0 

SIMPLIFIER> (y / . * (5 * . - (4 + 1) * .)) 
O 

SIMPLIFIER> ((4 - 3) * . + (y / y - 1) * .) 
X 

SIMPLIFIER> (1 * f(x) + 0) 
(F X) 
SIMPLIFIER> (3 * 2 * X) 
(3 * (2 * X)) 
SIMPLIFIER> [Abort] 
> 


<a id='page-246'></a>

Here we have terminated the loop by hitting the abort key on the terminal. (The details 
of this mechanism varies from one implementation of Common Lisp to another.) The 
simplifier seems to workfairlywell, although it errs on the last example: (3 * (2 * 
X)) should simplify to (6 * X). In the next section, we will correct that problem. 

8.3 Associativity and Commutativity 
We could easily add a rule to rewrite (3 * (2 *X))as((3 * 2) * X) and hence 
(6 * X). The problem is that this rule would also rewrite (X*(2*3))as((X* 

2) * 3), unless we had a way to limit the rule to apply only when it would group 
numbers together. Fortunately, pat-match does provide just this capability, with the 
?i s pattern. We could write this rule: 
(((?is . numberp) * ((?is m numberp) * x)) = ((n * m) * x)) 

This transforms (3 * (2 * x)) into ((3 * 2) * .), and hence into (6 * x). 
Unfortunately, the problem is not as simple as that. We also want to simplify ((2 * 

x) * (y* 3)) to (6 *(x * y)). We can do a better job of gathering numbers together 
by adopting three conventions. First, make numbers first in products: change . * 
3 to 3 * X. Second, combine numbers in an outer expression with a number in an 
inner expression: change 3 *(5*x)to(3*5)*x. Third, move numbers out 
of inner expressions whenever possible: change (3 *x) *yto3*(x*y) . We 
adopt similar conventions for addition, except that we prefer numbers last there: . 
+ 1 instead of 1 + x. 
Define . and m as numbers; s as a non-number: 
(pat-match-abbrev *n '(lis . numberp)) 
(pat-match-abbrev 'm '(?is m numberp)) 
(pat-match-abbrev *s '(?is s not-numberp)) 

(defun not-numberp (x) (not (numberp x))) 

(defun simp-rule (rule) 
"Transform a rule into proper format." 
(let ((exp (infix->prefix rule))) 

(mkexp (expand-pat-match-abbrev (exp-lhs exp)) 
(exp-op exp) (exp-rhs exp)))) 


<a id='page-247'></a>

(setf *simplification-rules* 
(append *simplification-rules* (mapcar #'simp-rule 

'((s * . = . * s) 
(. * (m * X) = (. * m) * .) 
(. * (. * y) = . * (. * y)) 
((. * .) * y = . * (. * y)) 
(. + s = s + .) 
((. + m) + . = . + . ->- m) 
(. + (y + .) = (. + y) + .) 
((. + .) + y = (. + y) + .))))) 

With the new rules in place, we are ready to try again. For some problems we get just 
the right answers: 

> (simplifier) 

SIMPLIFIER> (3 * 2 * x) 
(6 * X) 

SIMPLIFIER> (2 * . * . * 3) 
(6 * (. 2)) 

SIMPLIFIER> (2*x*3*y*4*z*5*6) 
(720 * (X * (Y * .))) 

SIMPLIFIER> (3 + . + 4 + .) 
((2 * .) + 7) 

$IMPLIFIER> (2 *. *3 *. *4*(1 /. )*5*6) 
(720 * .) 

Unfortunately, there are other problems that aren't simplified properly: 

SIMPLIFIER> (3 + . + 4 - x) 
((X + (4 - X)) + 3) 
SIMPLIFIER> (x + y + y + x) 
(X + (Y + (Y + .))) 
SIMPLIFIER> (3 * . + 4 * .) 
((3 * .) + (4 * .)) 

We will return to these problems in section 8.5. 

&#9635; Exercise 8.1 Verify that the set of rules just prior does indeed implement the desired 
conventions, and that the conventions have the proper effect, and always terminate. 
As an example of a potential problem, what would happen if we used the rule (. * 
. = . * .) instead of the rule (s * . = . * s)? 


<a id='page-248'></a>

8.4 Logs, Trig, and Differentiation 
In the previous section, we restricted ourselves to the simple arithmetic functions, 
so as not to intimidate those who are a little leery of complex mathematics. In this 
section, we add a little to the mathematical complexity, without having to alter the 
program itself one bit. Thus, the mathematically shy can safely skip to the next 
section without feeling they are missing any of the fun. 

We start off by representing some elementary properties of the logarithmic and 
trigonometric functions. The new rules are similar to the "zero and one" rules we 
needed for the arithmetic operators, except here the constants e and . i (e = 2.71828... 
and &pi; = 3.14159...) are important in addition to 0 and 1. We also throw in some rules 
relating logs and exponents, and for sums and differences of logs. The rules assume 
that complex numbers are not allowed. If they were, log (and even x^) would have 
multiple values, and it would be wrong to arbitrarily choose one of these values. 

(setf *simplification-rules* 

(append *simplification-ru1es* (mapcar #*simp-rule *( 
(log 1 = 0) 
(log 0 = undefined) 
(log e =1) 
(sin 0 =0) 
(sin pi = 0) 
(cos 0 =1) 
(cos pi = -1) 
(sin(pi / 2) =1) 
(cos(pi / 2) =0) 
(log (e " x) = x) 
(e ^ (log x) = x) 
((x ^ y) * (x ^ z) = . My + z)) 
((x ^ y) / (x M) = . My - z)) 
(log . + log y = log(x * y)) 
(log X - log y = log(x / y)) 
((sin X) ^ 2 + (cos X) ^ 2 = 1) 
)))) 

Now we would like to go a step further and extend the system to handle differentiation. 
This is a favorite problem, and one which has historical significance: in the 
summer of 1958 John McCarthy decided to investigate differentiation as an interesting 
symbolic computation problem, which was difficult to express in the primitive 
programming languages of the day. This investigation led him to see the importance 
of functional arguments and recursive functions in the field of symbolic computation. 
For example, McCarthy invented what we now call mapcar to express the idea 
that the derivative of a sum is the sum of the derivative function applied to each 
argument. Further work led McCarthy to the publication in October 1958 of MIT 


<a id='page-249'></a>
AI Lab Memo No. 1: "An Algebraic Language for the Manipulation of Symbolic 
Expressions/' which defined the precursor of Lisp. 

In McCarthy's work and in many subsequent texts you can see symbolic differentiation 
programs with a simplification routine tacked on the end to make the output 
more readable. Here, we take the opposite approach: the simplification routine is 
central, and differentiation is handled as just another operator, with its own set of 
simplification rules. We will require a new infix-to-prefix translation rule. While 
we're at it, we'll add a rule for indefinite integration as well, although we won't write 
simplification rules for integration yet. Here are the new notations: 

math infix prefix 

dy/dx d y / d X/ydx Int y d X

And here are the necessary infix-to-prefix rules: 

(defparameter *infix->prefix-rules* 
(mapcar #'expand-pat-match-abbrev 

 (d y x) 
(int y .) 

'(((x+ = y+) (= . y)) 
((- x+) (- .)) 
((+.+) (+x)) 
((X-H + y+) (+ X y)) 

((x+ - y+) (- . y)) 
((d y+ / d x) (d y x))
((Int y+ d x) (int y .))
((x+ * y+) (* . y)) 
((x+ / y+) (/ . y)) 
((x+ ^ y+) r . y))))) 

 New rule 
New rule 

Since the new rule for differentiation occurs before the rule for division, there won't 
be any confusion with a differential being interpreted as a quotient. On the other 
hand, there is a potential problem with integrals that contain d as a variable. The 
user can always avoid the problem by using (d) instead of d inside an integral. 

Now we augment the simplification rules, by copying a differentiation table out 
of a reference book: 

(setf *simplification-rules* 

(append *simplification-rules* (mapcar #*simp-rule '( 
(d . / d . =1) 
(d (u + V) / d . = (d u / d X) + (d V / d X)) 
(d (u - V) / d . = (d u / d X) - (d V / d x)) 
(d (- u) / d X = - (d u / d X)) 
(d (u * V) / d X = u * (d V / d X) + V * (d u / d X)) 
(d (u / V) / d X = (V * (d u / d X) - u * (d V / d X)) 

/ V ^ 2) 


<a id='page-250'></a>

(d (u ^ n) / d X = . * u Mn - 1) * (d u / d X)) 
(d (u V) / d X = V * u Mv - 1) * (d u / d X) 

+ u ^ V * (log u) * (d V / d x)) 
(d (log u) / d X = (d u / d X) / u) 
(d (sin u) / d X = (cos u) * (d u / d x)) 
(d (cos u) / d X = - (sin u) * (d u / d x)) 
(d (e ^ u) / d X = (e ^ u) * (d u / d X)) 
(d u / d X = 0))))) 
We have added a default rule, (d u / d . = 0); this should only apply when the 
expression u is free of the variable . (that is, when u is not a function of x). We could 
use ?1 f to check this, but instead we rely on the fact that differentiation is closed over 
the list of operators described here—as long as we don't introduce any new operators, 
the answer will always be correct. Note that there are two rules for exponentiation, 
one for the case when the exponent is a number, and one when it is not. This was 
not strictly necessary, as the second rule covers both cases, but that was the way the 
rules were written in the table of differentials I consulted, so I left both rules in. 

SIMPLIFIER> (d (x + x) / d x) 
2 
SIMPLIFIER> (d (a * x ^ 2 + b * x + c) / d x) 
((2 * (A * X)) + B) 
SIMPLIFIER> (d ((a * x ^ 2 + b * x + c) / x) / d x) 
((((A * (X ^ 2)) + ((B * X) + O) - (X * ((2 * (A * X)) + B))) 

/ (X ^ 2)) 
SIMPLIFIER> (log ((d (x + .) / d x) / 2)) 
0 
SIMPLIFIER> (log(x + x) - log x) 
(LOG 2) 
SIMPLIFIER> (x cos pi) 
(1 / X) 
SIMPLIFIER> (d (3 * x + (cos x) / x) / d x) 
((((COS X) - (X * (- (SIN X)))) / (X ^ 2)) + 3) 
SIMPLIFIER> (d ((cos x) / x) / d x) 
(((COS X) - (X * (- (SIN X)))) / (X ^ 2)) 
SIMPLIFIER> (d (3 * x ^ 2 + 2 * . + 1) / d x) 
((6 * X) + 2) 
SIMPiIFIER> (sin(x + x) ^ 2 + cos(d . ^ 2 / d x) ^ 2) 
1 
SIMPLIFIER> (sin(x + x) * sin(d . " 2 / d x) + 

cos(2 * X)* cos(x * d 2 * y / d y)) 
1 

The program handles differentiation problems well and is seemingly clever in its use 
of the identity sin^ . -f cos^ . = 1. 


<a id='page-251'></a>
8.5 Limits of Rule-Based Approaches 
In this section we return to some examples that pose problems for the simplifier. 
Here is a simple one: 

SIMPLIFIER> (x + y + y + .) => (X + (Y + (Y + X))) 

We would prefer 2 * (x + y). The problem is that, although we went to great trouble 
to group numbers together, there was no effort to group non-numbers. We could 
write rules of the form: 

(y + (y + x) = (2 * y) + x) 

(y + (x + y) = (2 * y) + x) 

Thesewouldworkfortheexampleathand, but they would not work for (. + y + . 

+ y + .). For that we would need more rules: 
(y + (. + (y + X)) = (2 * y) + X + .) 

(y + (. + (. + y)) = (2 * y) + . + .) 

(y + ((y + .) + .) = (2 * y) + . + .) 

(y + ((. + y) + .) = (2 * y) + . + .) 

.. handle all the cases, we would need an infinite number of rules. The pattern-
matching language is not powerful enough to express this succintly. It might help 
if nested sums (and products) were unnested; that is, if we allowed + to take an 
arbitrary number of arguments instead of just one. Once the arguments are grouped 
together, we could sort them, so that, say, all the ys appear before . and after x. Then 
like terms could be grouped together. We have to be careful, though. Consider these 
examples: 

SIMPLIFIER> (3 * . + 4 * x) 

((3 * X) + (4 * X)) 

SIMPLIFIER> (3 *x + y + x + 4 *x) 

((3 * X) + (Y + (X + (4 * X)))) 

We would want (3 * .) to sort to the same place as . and (4 * .) so that they could 
all be combined to (8 * x). In chapter 15, we develop a new version of the program 
that handles this problem. 


<a id='page-252'></a>

8.6 Integration 
So far, the algebraic manipulations have been straightforward. There is a direct 
algorithm for computing the derivative of every expression. When we consider 
integrals, or antiderivatives,^ the picture is much more complicated. As you may 
recall from freshman calculus, there is a fine art to computing integrals. In this 
section, we try to see how far we can get by encoding just a few of the many tricks 
available to the calculus student. 

The first step is to recognize that entries in the simplification table will not be 
enough. Instead, we will need an algorithm to evaluate or "simplify" integrals. 
We will add a new case to simpl ify-exp to check each operator to see if it has a 
simplification function associated with it. These simplification functions will be 
associated with operators through the functions set-simp-fn and simp-fn. If an 
operator does have a simplification function, then that function will be called instead 
of consulting the simplification rules. The simplification function can elect not to 
handle the expression after all by returning nil, in which case we continue with the 
other simplification methods. 

(defun simp-fn (op) (get op 'simp-fn)) 
(defun set-simp-fn (op fn) (setf (get op 'simp-fn) fn)) 

(defun simplify-exp (exp) 
"Simplify using a rule, or by doing arithmetic, 
or by using the simp function supplied for this operator." 
(cond ((simplify-by-fn exp)) 

((rule-based-translator exp *simplification-rules* 
:rule-if #'exp-lhs :rule-then #'exp-rhs 
raction #*(lambda (bindings response) 

(simplify (sublis bindings response))))) 
((evaluable exp) (eval exp)) 
(t exp))) 

(defun simplify-by-fn (exp) 
"If there is a simplification fn for this exp, 
and if applying it gives a non-null result, 
then simplify the result and return that." 
(let* ((fn (simp-fn (exp-op exp))) 

(result (if fn (funcall fn exp)))) 

(if (null result) 
nil 
(simplify result)))) 

Freshman calculus classes teach a variety of integration techniques. Fortunately, 
one technique—the derivative-divides technique—can be adopted to solve most of the 

^The term antiderivative is more correct, because of branch point problems. 


<a id='page-253'></a>
problems that come up at the freshman calculus level, perhaps 90% of the problems 
given on tests. The basic rule is: 

jf{x)dx =

 Jm^dx. 

As an example, consider / xsin{x^)dx. Using the substitution u = x^, we can 
differentiate to get du/dx = 2x, Then by applying the basic rule, we get: 

&int; x sin{x^2)dx = 1/2 &int; sin(u)du/dx dx = 1/2 &int sin(u) du. 

Assume we have a table of integrals that includes the rule / sin(x) dx = - cos(x). 
Then we can get the final answer: 

--cos{x^). 

Abstracting from this example, the general algorithm for integrating an expression 
y with respect to . is: 

1. Pick a factor of y, calling it f{u). 
2. Compute the derivative du/dx. 
3. Divide y by f{u) . du/dx, calling the quotient k. 
4. If/c is a constant (with respect to x), then the result is A: / f{u)du. 
This algorithm is nondeterrninistic, as there may be many factors of y. In our 
example, f{u) = sin(x^),w = x^, and du/dx = 2x. So k = \, and the answer is 

-^COs(x2). 

The first step in implementing this technique is to make sure that division is done 
correctly. We need to be able to pick out the factors of y, divide expressions, and then 
determine if a quotient is free of x. The function f acton* ze does this. It keeps a list 
of factors and a running product of constant factors, and augments them with each 
call to the local function f ac. 


<a id='page-254'></a>

(defun factorize (exp) 
"Return a list of the factors of exp^n, 
where each factor is of the form . y .)." 
(let ((factors nil) 

(constant 1)) 
(labels 
((fac (X n) 
(cond 
((numberp x) 
(setf constant (* constant (expt . .)))) 

((starts-with . **) 
(fac (exp-lhs x) n) 
(fac (exp-rhs x) n)) 

((starts-with . V) 
(fac (exp-lhs x) n) 
(fac (exp-rhs x) (- n))) 

((and (starts-with . *-) (length=1 (exp-args x))) 
(setf constant (- constant)) 
(fac (exp-lhs x) n)) 

((and (starts-with . '") (numberp (exp-rhs x))) 
(fac (exp-lhs x) (* . (exp-rhs x)))) 
(t (let ((factor (find . factors :key #'exp-lhs 
:test #'equal))) 

(if factor 
(incf (exp-rhs factor) n) 
(push . ,x ,n) factors))))))) 

;; Body of factorize: 
(fac exp 1) 
(case constant 

(0 *{r 0 1))) 
(1 factors) 
(t '{. .constant 1) ..factors)))))) 

factor i ze maps from an expression to a list of factors, but we also need unf actor i ze 
to turn a list back into an expression: 

(defun unfactorize (factors) 
"Convert a list of factors back into prefix form." 
(cond ((null factors) 1) 

((length=1 factors) (first factors)) 
(t .(* .(first factors) .(unfactorize (rest factors)))))) 

The derivative-divides method requires a way of dividing two expressions. We do this 
by factoring each expression and then dividing by cancelling factors. There may be 
cases where, for example, two factors in the numerator could be multiplied together 


<a id='page-255'></a>
to cancela factor in the denominator, but this possibility is not considered. It turns 
out that most problems from freshman calculus do not require such sophistication. 

(defun divide-factors (numer denom) 
"Divide a list of factors by another, producing a third." 
(let ((result (mapcar #'copy-list numer))) 

(dolist (d denom) 
(let ((factor (find (exp-lhs d) result :key #*exp-lhs 
:test #*equal))) 

(if factor 
(decf (exp-rhs factor) (exp-rhs d)) 
(push *r ,(exp-lhs d) .(- (exp-rhs d))) result)))) 

(delete 0 result :key #'exp-rhs))) 

Finally, the predicate free-of returns true if an expression does not have any occurrences 
of a particular variable in it. 

(defun free-of (exp var) 
"True if expression has no occurrence of var." 
(not (find-anywhere var exp))) 

(defun find-anywhere (item tree) 
"Does item occur anywhere in tree? If so. return it." 
(cond ((eql item tree) tree) 

((atom tree) nil) 
((find-anywhere item (first tree))) 
((find-anywhere item (rest tree))))) 

In factorize we made use of the auxiliary function length=1. The function call 
(length=1 x) is faster than (= (length x) 1) because the latter has to compute 
the length of the whole list, while the former merely has to see if the list has a rest 
element or not. 

(defun length=1 (x) 
"Is X a list of length 1?" 
(and (consp x) (null (rest x)))) 

Given these preliminaries, the function i ntegrate is fairly easy. We start with 
some simple cases for integrating sums and constant expressions. Then, we factor 
the expression and split the list of factors into two: a Ust of constant factors, and 
a list of factors containing x. (This is done with partition-if, a combination of 
remove-if and remove-if-not.) Finally, we call deri .-divides, giving it a chance 
with each of the factors. If none of them work, we return an expression indicating 
that the integral is unknown. 


<a id='page-256'></a>

(defun integrate (exp x) 
First try some trivial cases 

(cond 
((free-of exp x) *(* ,exp x)) Int c dx = c*x 
((starts-with exp *+) Int f + g = 

*(+ .(integrate (exp-lhs exp) x) Int f + Int g 
.(integrate (exp-rhs exp) x))) 
((starts-with exp *-) 

(ecase (length (exp-args exp)) 
(1 (integrate (exp-lhs exp) x)) Int -f = - Int f 
(2 '(- .(integrate (exp-lhs exp) x) Int f -g = 

.(integrate (exp-rhs exp) x))))) ; Int f - Int g 
Now move the constant factors to the left of the integral 
((multiple-value-bind (const-factors x-factors) 
(partition-if #'(lambda (factor) (free-of factor x)) 
(factorize exp)) 
(simplify 
.(* .(unfactorize const-factors) 
And try to integrate: 
.(cond ((null x-factors) x) 
((some #'(lambda (factor) 
(deriv-divides factor x-factors x)) 

x-factors)) 
;; <other methods here> 
(t *(int? .(unfactorize x-factors) .x))))))))) 

(defun partition-if (pred list) 
"Return 2 values: elements of list that satisfy pred. 
and elements that don't." 
(let ((yes-list nil) 

(no-list nil)) 
(dolist (item list) 

(if (funcall pred item) 
(push item yes-list) 
(push item no-list))) 

(values (nreverse yes-list) (nreverse no-list)))) 


<a id='page-257'></a>
Note that the place in integrate where other techniques could be added is 
marked. We will only implement the derivative-divides method. It turns out that 
the function is a little more complicated than the simple four-step algorithm outlined 
before: 

(defun deriv-divides (factor factors x) 
(assert (starts-with factor 
(let* ((u (exp-lhs factor)) ; factor = u^n 

(n (exp-rhs factor)) 
(k (divide-factors 
factors (factorize '(* .factor ,(deriv u x)))))) 
(cond ((free-of k x) 

Int k*u"n*du/dx dx = k*Int u"n du 
= k*u^(n+l)/(n+l) for . /= -1 
= k*log(u) for . = -1 

(if (= . -1) 
'(* .(unfactorize k) (log .u)) 
*(/ (* ,(unfactorize k) . ,u .(+ . 1))) 

,(+ . 1)))) 
((and (= . 1) (in-integral-table? u)) 
Int y'*f(y) dx = Int f(y) dy 

(let ((k2 (divide-factors 
factors 
(factorize *(* ,u ,(deriv (exp-lhs u) x)))))) 

(if (free-of k2 x) 
.(* .(integrate-from-table (exp-op u) (exp-lhs u)) 
.(unfactorize k2)))))))) 

There are three cases. In any case, all factors are of the form (" u .), so we separate 
the factor into a base, u, and exponent, n. li u or u"^ evenly divides the original 
expression (here represented as factors), then we have an answer. But we need to 
check the exponent, because / u'^du is u'^~^'^/{n -h 1) forn -1, but it is log(u) for 
. = -1, But there is a third case to consider. The factor may be something like ( 
(sin X 2)) 1), in which case we should consider /(..) = sin(x^). This case is 
handled with the help of an integral table. We don't need a derivative table, because 

we can just use the simplifier for that. 

(defun deriv (y x) (simplify *(d ,y .x))) 

(defun integration-table (rules) 
(dolist (i-rule rules) 
(let ((rule (infix->prefix i-rule))) 
(setf (get (exp-op (exp-lhs (exp-lhs rule))) 'int) 
rule)))) 


<a id='page-258'></a>

(defun in-integral-table? (exp) 
(and (exp-p exp) (get (exp-op exp) 'int))) 

(defun integrate-from-table (op arg) 
(let ((rule (get op 'int))) 
(subst arg (exp-lhs (exp-lhs (exp-lhs rule))) (exp-rhs rule)))) 

(integration-table 

'((Int log(x) d . = . * log(x) - .) 
(Int exp(x) d . = exp(x)) 
(Int sin(x) d X = - cos(x)) 
(Int cos(x) d X = sin(x)) 
(Int tan(x) d . = - log(cos(x))) 
(Int sinh(x) d . = cosh(x)) 
(Int cosh(x) d X = sinh(x)) 
(Int tanh(x) d . = log(cosh(x))) 
)) 

The last step is to install integrate as the simplification function for the operator 
Int. The obvious way to do this is: 

(set-simp-fn 'Int 'integrate) 

Unfortunately, that does not quite work. The problem is that integrate expects 
two arguments, corresponding to the two arguments y and xin ilnt y x). But the 
convention for simplification functions is to pass them a single argument, consisting 
of the whole expression (Int y jc). We could go back and edit simpl Ify-exp to 
change the convention, but instead I choose to make the conversion this way: 

(set-simp-fn 'Int #'(lambda (exp) 
(integrate (exp-lhs exp) (exp-rhs exp)))) 

Here are some examples, taken from chapters 8 and 9 of Calculus (Loomis 1974): 

SIMPLIFIER> (Int . * sin(x 2) d x) 
(1/2 * (- (COS (X ^ 2)))) 
SIMPLIFIER> (Int ((3 * . ^ 3) - 1 / (3 * . ^ 3)) d x) 
((3 * ((X ^ 4) / 4)) - (1/3 * ((X ^ -2) / -2))) 
SIMPLIFIER> (Int (3 * . + 2) ^ -2/3 d x) 
(((3 * X) + 2) 1/3) 
SIMPLIFIER> (Int sin(x) ^ 2 * cos(x) d x) 
(((SIN X) ^ 3) / 3) 
SIMPLIFIER> (Int sin(x) / (1 + cos(x)) d x) 
(-1 * (LOG ((COS X) + 1))) 
SIMPLIFIER> (Int (2 * . + 1) / (x ^ 2 + . - 1) d x) 


<a id='page-259'></a>

(LOG ((X ^ 2) + (X - 1))) 
SIMPLIFIER> (Int 8 * . ^ 2 / (x ^ 3 + 2) ^ 3 d x) 
(8 * ((1/3 * (((X ^ 3) + 2) -2)) / -2)) 

All the answers are correct, although the last one could be made simpler. One quick 
way to simplify such an expression is to factor and unfactor it, and then simplify 
again: 

(set-simp-fn 'Int 
#'(lambda (exp) 
(unfactorize 
(factorize 

(integrate (exp-lhs exp) (exp-rhs exp)))))) 

With this change, we get: 
SIMPLIFIER> (Int 8 * . ^ 2 / (x 3 + 2) ^ 3 d x) 
(-4/3 * (((X ^ 3) + 2) ^ -2)) 

8.7 History and References 
A brief history is given in the introduction to this chapter. An interesting point is that 
the history of Lisp and of symbolic algebraic manipulation are deeply intertwined. 
It is not too gross an exaggeration to say that Lisp was invented by John McCarthy 
to express the symbolic differentiation algorithm. And the development of the first 
high-quality Lisp system, MacLisp, was driven largely by the needs of MACSYMA, 
one of the first large Lisp systems. See McCarthy 1958 for early Lisp history and 
the differentiation algorithm, and Martin and Fateman 1971 and Moses (1975) for 
more details on MACSYMA. A comprehensive book on computer algebra systems 
is Davenport 1988. It covers the MACSYMA and REDUCE systems as well as the 
algorithms behind those systems. 

Because symbolic differentiation is historically important, it is presented in a 
number of text books, from the original Lisp 1.5 Primer (Weissman 1967) and Allen's 
influential Anatomy of Lisp (1978) to recent texts like Brooks 1985, Hennessey 1989, 
and Tanimoto 1990. Many of these books use rules or data-driven programming, 
but each treats differentiation as the main task, with simplification as a separate 
problem. None of them use the approach taken here, where differentiation is just 
another kind of simplification. 

The symbolic integration programs SAINT and SiN are covered in Slagle 1963 and 
Moses 1967, respectively. The mathematical solution to the problem of integration 


<a id='page-260'></a>

in closed term is addressed in Risch 1969, but be warned; this paper is not for the 
mathematically naive, and it has no hints on programming the algorithm. A better 
reference is Davenport et al. 1988. 
In this book, techniques for improving the efficiency of algebraic manipulation 
are covered in sections 9.6 and 10.4. Chapter 15 presents a reimplementation that 
does not use pattern-matching, and is closer to the techniques used in MACSYMA. 
8.8 Exercises 
&#9635; Exercise 8.2 [s] Some notations use the operator ** instead of " to indicate exponentiation. 
Fix i nf ix->pref i . so that either notation is allowed. 

&#9635; Exercise 8.3 [m] Can the system as is deal with imaginary numbers? What are 
some of the difficulties? 
&#9635; Exercise 8.4 [h] There are some simple expressions involving sums that are not 
handled by the i ntegrate function. The function can integrate axx^-\-bxx-\-c 
but not 5 X {a X x^ b X X + c) . Similarly, it can integrate x^ + 2x x^ -\- x^ but not 
(.2 + x)^^ and it can do + + . -h 1 but not (x^ -f 1) . (x + 1). Modify i ntegrate 
so that it expands out products (or small exponents) of sums. You will probably want 
to try the usual techniques first, and do the expansion only when that fails. 

&#9635; Exercise 8.5 [d] Another very general integration technique is called integration 
by parts. It is based on the rule: &int; udv = uv - &int; vdu 
So, for example, given 
X cos xdx/
we can take u = x,dv = cos xdx. Then we can determine . = sin . by integration, 
and come up with the solution: 
JX cos xdx = X sin ^ ~J ^ ^ ~ ^ ^^" ^ 

It is easy to program an integration by parts routine. The hard part is to program 
the control component. Integration by parts involves a recursive call to i ntegrate, 
and of all the possible ways of breaking up the original expression into a u and a dv, 


<a id='page-261'></a>

few, if any, will lead to a successful integration. One simple control rule is to allow 
integration by parts only at the top level, not at the recursive level. Implement this 
approach. 

&#9635; Exercise 8.6 [d] A more complicated approach is to try to decide which ways of 
breaking up the original expression are promising and which are not. Derive some 
heuristics for making this division, and reimplement i ntegrate to include a search 
component, using the search tools of chapter 6. 

Look in a calculus textbook to see how / sin^ xdx is evaluated by two integrations 
by parts and a division. Implement this technique as well. 

&#9635; Exercise 8.7 [m] Write simplification rules for predicate calculus expressions. For 
example, 

(true and . = x) 
(false and . = false) 
(true or X = true) 
(false or X = false) 

&#9635; Exercise 8.8 [m] The simplification rule (. / 0 = undef i ned) is necessary to avoid 
problems with division by zero, but the treatment of undef i ned is inadequate. For 
example, the expression ((0/0) - (0/0)) will simplify to zero, when it should 
simplify to undef i ned. Add rules to propagate undef i ned values and prevent them 
from being simplified away. 

&#9635; Exercise 8.9 [d] Extend the method used to handle undef i ned to handle +i nf i ni ty 
and -i nf ini ty as well. 


## Chapter 9
<a id='page-265'></a>

Efficiency issues 

ALisp programmer knows the value of every thing, 
but the cost of nothing. 

-Alan J. Perils 

Lisp is not inherently less efficient than other 
high-level languages. 

—Richard J. Fateman 

O
O
ne of the reasons Lisp has enjoyed a long history is because it is an ideal language for 
what is now called rapid-prototyping—developing a program quickly, with little regards 
for details. That is what we have done so far in this book: concentrated on getting a 
working algorithm. Unfortunately, when a prototype is to be turned into a production-quality 
program, details can no longer be ignored. Most "real" AI programs deal with large amounts of 
data, and with large search spaces. Thus, efficiency considerations become very important. 

However, this does not mean that writing an efficient program is fundamentaly different 
from writing a working program. Ideally, developing an efficient program should be a three-step 
process. First, develop a working program, using proper abstractions so that the program will be 
easy to change if necessary. Second,instrument the program to determine where it is spending 
most of the time. Third, replace the slow parts with faster versions, while maintaining the 
program's correctness. 


<a id='page-266'></a>

The term efficiency will be used primarily to talk about the speed or run time of a 
program. To a lesser extent, efficiency is also used to refer to the space or amount of 
storage consumed by a program. We will also talk about the cost of a program. This 
is partly a use of the metaphor "time is money," and partly rooted in actual monetary 
costs—if a critical program runs unacceptably slowly, you may need to buy a more 
expensive computer. 

Lisp has been saddled with a reputation as an "inefficient language." Strictly 
speaking, it makes no sense to call a language efficient or inefficient. Rather, it is only 
a particular implementation of the language executing a particular program that can be 
measured for efficiency. So saying Lisp is inefficient is partly a historical claim: some 
past implementations have been inefficient. It is also partly a prediction: there are 
some reasons why future implementations are expected to suffer from inefficiencies. 
These reasons mainly stem from Lisp's flexibility. Lisp allows many decisions to be 
delayed until run time, and that can make the run time take longer. In the past decade, 
the "efficiency gap" between Lisp and "conventional languages" Uke FORTRAN or 
C has narrowed. Here are the reasons—some deserved, some not—behind Lisp's 
reputation for inefficiency: 

* Early implementations were interpreted rather than compiled, which made 
them inherently inefficient. Common Lisp implementations have compilers, 
so this is no longer a problem. While Lisp is (primarily) no longer an interpreted 
language, it is still an interactive language, so it retains its flexibility. 
* Lisp has often been used to write interpreters for embedded languages, thereby 
compounding the problem. Consider this quote from Cooper and Wogrin's 
(1988) book on the rule-based programming language OPS5: 
The efficiency of implementations that compile rules into executable code 
compares favorably to that of programs wntten in most sequential languages 
such as FORTRAN or Pascal Implementations that compile rules 
into data structures to be interpreted, as do many Lisp-based ones, could be 
noticeably slower. 

Here Lisp is guilty by association. The fallacious chain of reasoning is: Lisp has 
been used to write interpreters; interpreters are slow; therefore Lisp is slow. 
While it is true that Lisp makes it very easy to write interpreters, it also makes 
it easy to write compilers. This book is the first that concentrates on using Lisp 
as both the implementation and target language for compilers. 

* Lisp encourages a style with lots of function calls, particularly recursive calls. 
In some older systems, function calls were expensive. But it is now understood 
that a function call can be compiled into a simple branch instruction, and that 

<a id='page-267'></a>

many recursive calls can be made no more expensive than an equivalent iterative 
loop (see chapter 22). It is also possible to instruct a Common Lisp compiler 
to compile certain functions inline, so there is no calling overhead at all. 

On the other hand, many Lisp systems require two fetches instead of one to find 
the code for a function, and thus will be slower. This extra level of indirection 
is the price paid for the freedom of being able to redefine functions without 
reloading the whole program. 

Run-time type-checking is slow. Lisp provides a repertoire of generic functions. 
For example, we can write (+ x y) without bothering to declare if . and y are integers, 
floatingpoint, bignums, complex numbers, rationals, or some combination 
of the above. This is very convenient, but it means that type checks must be 
made at run time, so the generic+will be slower than, say, a 16-bit integer addition 
with no check for overflow. If efficiency is important. Common Lisp allows 
the programmer to include declarations that can eUminate run-time checks. 

In fact, once the proper declarations are added. Lisp can be as fast or faster 
than conventional languages. Fateman (1973) compared the FORTRAN cube 
root routine on the PDP-10 to a MacLisp transliteration. The MacLisp version 
produced almost identical numerical code, but was 18% faster overall, due to 
a superior function-calling sequence.^ The epigraph at the beginning of this 
chapter is from this article. 

Berlin and Weise (1990) show that with a special compilation technique called 
partial evaluation, speeds 7 to 90 times faster than conventionally compiled code 
can be achieved. Of course, partial evaluation could be used in any language, 
but it is very easy to do in Lisp. 

The fact remains that Lisp objects must somehow represent their type, and 
even with declarations, not all of this overhead can be eliminated. Most Lisp 
implementations optimize access to lists and fixnums but pay the price for the 
other, less commonly used data types. 

Lisp automatically manages storage, and so it must periodically stop and collect 
the unused storage, or garbage. In early systems, this was done by periodically 
sweeping through all of memory, resulting in an appreciable pause. Modern 
systems tend to use incremental garbage-collection techniques, so pauses are 
shorter and usually unnoticed by the user (although the pauses may still be too 
long for real-time applications such as controlling a laboratory instrument). 
The problem with automatic garbage collection these days is not that it is 
slow-in fact, the automatic systems do about as well as handcrafted storage 

^One could say that the FORTRAN compiler was "broken." This underscores the problem 

of defining the efficiency of a language-do we judge by the most popular compiler, by the best 
compiler available, or by the best compiler imaginable? 


<a id='page-268'></a>

allocation. The problem is that they make it convenient for the programmer 
to generate a lot of garbage in the first place. Programmers in conventional 
languages, who have to clean up their own garbage, tend to be more careful 
and use static rather than dynamic storage more often. If garbage becomes a 
problem, the Lisp programmer can just adopt these static techniques. 

Lisp systems are big and leave little room for other programs. Most Lisp systems 
are designed to be complete environments, within which the programmer 
does all program development and execution. For this kind of operation, it 
makes sense to have a large language like Common Lisp with a huge set of 
tools. However, it is becoming more common to use Lisp as just one component 
in a computing environment that may include UNIX, X Windows, emacs, 
and other interacting programs. In this kind of heterogeneous environment, 
it would be useful to be able to define and run small Lisp processes that do 
not include megabytes of unused tools. Some recent compilers support this 
option, but it is not widely available yet. 

Lisp is a complicated high-level language, and it can be difficult for the programmer 
to anticipate the costs of various operations. In general, the problem 
is not that an efficient encoding is impossible but that it is difficult to arrive at 
that efficient encoding. In a language like C, the experienced programmer has 
a pretty good idea how each statement will compile into assembly language 
instructions. But in Lisp, very similar statements can compile into widely different 
assembly-level instructions, depending on subtle interactions between 
the declarations given and the capabilities of the compiler. Page 318 gives an 
example where adding a declaration speeds up a trivial function by 40 times. 
Nonexperts do not understand when such declarations are necessary and are 
frustrated by the seeming inconsistencies. With experience, the expert Lisp 
programmer eventually develops a good "efficiency model," and the need for 
such declarations becomes obvious. Recent compilers such as CMU's Python 
provide feedback that eases this learning process. 

In summary. Lisp makes it possible to write programs in a wide variety of styles, 

some efficient, some less so. The programmer who writes Lisp programs in the 

same style as C programs will probably find Lisp to be of comparable speed, perhaps 

slightly slower. The programmer who uses some of the more dynamic features of 

Lisp typically finds that it is much easier to develop a working program. Then, if 

the resulting program is not efficient enough, there will be more time to go back 

and improve critical sections. Deciding which parts of the program use the most 

resources is called instrumentation. It is foolhardy to try to improve the efficiency of 

a program without first checking if the improvement will make a real difference. 
One route to efficiency is to use the Lisp prototype as a specification and reimplement 
that specification in a lower-level language, such as C or C++. Some commercial 


<a id='page-269'></a>

AI vendors are taking this route. An alternative is to use Lisp as the language for both 
the prototype and the final implementation. By adding declarations and making 
minor changes to the original program, it is possible to end up with a Lisp program 
that is similar in efficiency to a C program. 

There are four very general and language-independent techniques for speeding 
up an algorithm: 

* Caching the results of computations for later reuse. 
* Compiling so that less work is done at run time. 
* Delaying the computation of partial results that may never be needed. 
* Indexing a data structure for quicker retrieval. 
This chapter covers each of the four techniques in order. It then addresses the 
important problem of instrumentation. The chapter concludes with a case study of 
the s i mpl i fy program. The techniques outlined here result in a 130-fold speed-up in 
this program. 

Chapter 10 concentrates on lower-level "tricks" for improving efficiency further. 

9.1 Caching Results of Previous Computations: 
Memoization 
We start with a simple mathematical function to demonstrate the advantages of 
caching techniques. Later we will demonstrate more complex examples. 

The Fibonacci sequence is defined as the numbers 1,1,2,3,5,8,... where each 
number is the sum of the two previous numbers. The most straightforward function 
to compute the nth number in this sequence is as follows: 

(defun fib (n) 
"Compute the nth number in the Fibonacci sequence." 
(if (<= . 1) 1 

(+ (fib (- . D) (fib (- . 2))))) 

The problem with this function is that it computes the same thing over and over 
again. To compute (fib 5) means computing (fib 4) and (fib 3), but (fib 4) 
also requires (fib 3), they both require (fib 2), and so on. There are ways to rewrite 
the function to do less computation, but wouldn't it be nice to write the function as 
is, and have it automatically avoid redundant computation? Amazingly, there is 
a way to do just that. The idea is to use the function fib to build a new function 
that remembers previously computed results and uses them, rather than recompute 


<a id='page-270'></a>

them. This process is called memoization. The function memo below is a higher-order 
function that takes a function as input and returns a new function that will compute 
the same results, but not do the same computation twice. 

(defun memo (fn) 
"Return a memo-function of fn." 
(let ((table (make-hash-table))) 

#'(lambda (x) 
(multiple-value-bind (val found-p) 
(gethash . table) 

(if found-p 
val 
(setf (gethash . table) (funcall fn x))))))) 

The expression (memo #'fib) will produce a function that remembers its results 
between calls, so that, for example, if we apply it to 3 twice, the first call will do the 
computation of (f i b 3), but the second will just look up the result in a hash table. 
With f i b traced, it would look like this: 

> (setf memo-fib (memo #'fib)) ^ #<CLOSURE -67300731> 

> (funcall memo-fib 3) 
(1 ENTER FIB: 3) 

(2 ENTER FIB: 2) 
(3 ENTER FIB: 1) 
(3 EXIT FIB: 1) 
(3 ENTER FIB: 0) 
(3 EXIT FIB: 1) 

(2 EXIT FIB: 2) 
(2 ENTER FIB: 1) 
(2 EXIT FIB: 1) 

(1 EXIT FIB: 3) 
3 

> (funcall memo-fib 3) 3 

The second time we call memo -fi b with 3 as the argument, the answer is just retrieved 
rather than recomputed. But the problem is that during the computation of (fib 
3), we still compute (f i b 2) multiple times. It would be better if even the internal, 
recursive calls were memoized, but they are calls to f i b, which is unchanged, not to 
memo -fib . We can solve this problem easily enough with the function memoi ze: 


<a id='page-271'></a>
(defun memoize (fn-name) 
"Replace fn-name's global definition with a memoized version." 
(setf (symbol-function fn-name) (memo (symbol-function fn-name)))) 

When passed a symbol that names a function, memoi ze changes the global definition 
of the function to a memo-function. Thus, any recursive calls will go first to the 
memo-function, rather than to the original function. This is just what we want. In 
the following, we contrast the memoized and unmemoized versions of f i b. First, a 
call to (fi b 5) with f i b traced: 

> (fib 5) 
(1 ENTER FIB: 5) 
(2 ENTER FIB: 4) 
(3 ENTER FIB: 3) 

(4 ENTER FIB: 2) 
(5 ENTER FIB: 1) 
(5 EXIT FIB: 1) 
(5 ENTER FIB: 0) 
(5 EXIT FIB: 1) 

(4 EXIT FIB: 2) 
(4 ENTER FIB: 1) 
(4 EXIT FIB: 1) 

(3 EXIT FIB: 3) 

(3 ENTER FIB: 2) 
(4 ENTER FIB: 1) 
(4 EXIT FIB: 1) 
(4 ENTER FIB: 0) 
(4 EXIT FIB: 1) 

(3 EXIT FIB: 2) 
(2 EXIT FIB: 5) 
(2 ENTER FIB: 3) 

(3 ENTER FIB: 2) 
(4 ENTER FIB: 1) 
(4 EXIT FIB: 1) 
(4 ENTER FIB: 0) 
(4 EXIT FIB: 1) 

(3 EXIT FIB: 2) 
(3 ENTER FIB: 1) 
(3 EXIT FIB: 1) 

(2 EXIT FIB: 3) 
(1 EXIT FIB: 8) 
8 

We see that (fib 5) and (fib 4) are each computed once, but (fi b 3) is computed 
twice, (fib 2)threetimes,and (fib 1) five times. Below we call (memoize 'fib) and 
repeat the calculation. This time, each computation is done only once. Furthermore, 


<a id='page-272'></a>

when the computation of (f i b 5) is repeated, the answer is returned immediately 
with no intermediate computation, and a further call to (f i b 6) can make use of the 
valueofCfib 5). 

> (memoize 'fib) => #<CLOSURE 76626607> 

> (fib 5) 

(1 ENTER FIB: 5) 

(2 ENTER FIB: 4) 

(3 ENTER FIB: 3) 

(4 ENTER FIB: 2) 

(5 ENTER FIB: 1) 

(5 EXIT FIB: 1) 

(5 ENTER FIB: 0) 

(5 EXIT FIB: 1) 

(4 EXIT FIB: 2) 

(3 EXIT FIB: 3) 

(2 EXIT FIB: 5) 

(1 EXIT FIB: 8) 

8 

> (fib 5) ^ 8 

> (fib 6) => 

(1 ENTER FIB: 6) 

(1 EXIT FIB: 13) 

13 

Understanding why this works requires a clear understanding of the distinction 
between functions and function names. The original (defun fib ...) form does two 
things: builds a function and stores it as the symbol -function value of f i b. Within 
that function there are two references to f i b; these are compiled (or interpreted) as 
instructions to fetch the symbol - function of f i b and apply it to the argument. 

What memo i ze does is fetch the original function and transform it with memo to a 
function that, when called, will first look in the table to see if the answer is already 
known. If not, the original function is called, and a new value is placed in the table. 
The trick is that memoi ze takes this new function and makes it the symbol - function 
value of the function name. This means that all the references in the original function 
will now go to the new function, and the table will be properly checked on each 
recursive call. One further complication to memo: the function gethash returns both 
the value found in the table and an indicator of whether the key was present or not. 
We use mul ti pi e - va 1 ue - bi nd to capture both values, so that we can distinguish the 
case when nil is the value of the function stored in the table from the case where 
there is no stored value. 

If you make a change to a memoized function, you need to recompile the original 
definition, and then redo the call to memoize. In developing your program, rather 


<a id='page-273'></a>

than saying (memoize *f), it might be easier to wrap appropriate definitions in a 
memoi ze form as follows: 

(memoize 
(defun f (X) ...) 
) 

Or define a macro that combines defun and memoi ze: 

(defmacro defun-memo (fn args &body body) 
"Define a memoized function." 
*(memoize (defun ,fn ,args . ,body))) 

(defun-memo f (x) ...) 

Both of these approaches rely on the fact that defun returns the name of the function 
defined. 

. (fib n) unmemoized memoized memoized up to 
25 121393 1.1 .010 0 
26 196418 1.8 .001 25 
27 317811 2.9 .001 26 
28 514229 4.7 .001 27 
29 832040 8.2 .001 28 
30 1346269 12.4 .001 29 
31 2178309 20.1 .001 30 
32 3524578 32.4 .001 31 
33 5702887 52.5 .001 32 
34 9227465 81.5 .001 33 
50 2.0el0 &mdash; .014 34 
100 5.7e20 &mdash; .031 50 
200 4.5e41 &mdash; .096 100 
500 2.2el04 &mdash; .270 200 
1000 7.0e208 &mdash; .596 500 
1000 7.0e208 &mdash; .001 1000 
1000 7.0e208 &mdash; .876 0 

Now we show a table giving the values of (f i b .) for certain n, and the time in 
seconds to compute the value, before and after (memoi ze ' f i b). For larger values 
of ., approximations are shown in the table, although f i b actually returns an exact 
integer. With the unmemoized version, I stopped at . = 34, because the times were 
getting too long. For the memoized version, even . = 1000 took under a second. 


<a id='page-274'></a>

Note there are three entries for (f i b 1000). The first entry represents the incremental 
computation when the table contains the memoized values up to 500, the 
second entry shows the time for a table lookup when (fib 1000) is already computed, 
and the third entry is the time for a complete computation starting with an 
empty table. 

It should be noted that there are two general approaches to discussing the efficiency 
of an algorithm. One is to time the algorithm on representative inputs, as we 
did in this table. The other is to analyze the asymptotic complexity of the algorithm. For 
the f i b problem, an asymptotic analysis considers how long it takes to compute (fib 
n) as n approaches infinity. The notation 0(/(n)) is used to describe the complexity. 
For example, the memoized version f i b is an 0(n) algorithm because the computation 
time is bounded by some constant times n, for any value of n. The unmemoized 
version, it turns out, is O (1.7^), meaning computing f i b of n+1 can take up to 1.7 times 
as long as f i b of n. In simpler terms, the memoized version has linear complexity, 
while the unmemoized version has exponential complexity. Exercise 9.4 ([page 308](chapter9.md#page-308)) 
describes where the 1.7 comes from, and gives a tighter bound on the complexity. 

The version of memo presented above is inflexible in several ways. First, it only 
works for functions of one argument. Second, it only returns a stored value for 
arguments that are eql, because that is how hash tables work by default. For some 
applications we want to retrieve the stored value for arguments that are equa 1. Third, 
there is no way to delete entries from the hash table. In many applications there are 
times when it would be good to clear the hash table, either because it has grown too 
large or because we have finished a set of related problems and are moving on to a 
new problem. 

The versions of memo and memoi ze below handle these three problems. They are 
compatible with the previous version but add three new keywords for the extensions. 
The name keyword stores the hash table on the property list of that name, so it can 
be accessed by cl ear-memoi ze. The test kejword tells what kind of hash table to 
create: eq, eql, or equal. Finally, the key keyword tells which arguments of the 
function to index under. The default is the first argument (to be compatible with the 
previous version), but any combination of the arguments can be used. If you want 
to use all the arguments, specify 1 dent i ty as the key. Note that if the key is a Ust of 
arguments, then you will have to use equal hash tables. 

(defun memo (fn name key test) 

"Return a memo-function of fn. " 

(let ((table (make-hash-table :test test))) 

(setf (get name 'memo) table) 

#'(lambda (&rest args) 

(let ((k (funcall key args))) 

(multiple-value-bind (val found-p) 

(gethash k table) 

(if found-p val 


<a id='page-275'></a>
(setf (gethash k table) (apply fn args)))))))) 

(defun memoize (fn-name &key (key #*first) (test #'eql)) 
"Replace fn-name's global definition with a memoized version." 
(setf (symbol-function fn-name) 

(memo (symbol-function fn-name) fn-name key test))) 

(defun clear-memoize (fn-name) 
"Clear the hash table from a memo function." 
(let ((table (get fn-name 'memo))) 

(when table (clrhash table)))) 

9.2 Compiling One Language into Another 
In chapter 2 we defined a new language—the language of grammar rules—which was 
processed by an interpreter designed especially for that language. An interpreter is 
a program that looks at some data structure representing a "program" or sequence 
of rules of some sort and interprets or evaluates those rules. This is in contrast to a 
compiler, which translates some set of rules in one language into a program in another 
language. 

The function generate was an interpreter for the "language" defined by the set of 
grammar rules. Interpreting these rules is straightforward, but the process is somewhat 
inefficient, in that generate must continually search through the *gramma r* to 
find the appropriate rule, then count the length of the right-hand side, and so on. 

A compiler for this rule-language would take each rule and translate it into a function. 
These functions could then call each other with no need to search through the 
*grammar*. We implement this approach with the function compi 1 e - rul e. It makes 
use of the auxiliary functions one-of and rule-lhs and rule-rhs from [page 40](chapter2.md#page-40), 
repeated here: 

(defun rule-lhs (rule) 
"The left-hand side of a rule." 
(first rule)) 

(defun rule-rhs (rule) 
"The right-hand side of a rule." 
(rest (rest rule))) 

(defun one-of (set) 
"Pick one element of set, and make a list of it." 
(list (random-elt set))) 


<a id='page-276'></a>

(defun random-elt (choices) 
"Choose an element from a list at random." 
(elt choices (random (length choices)))) 

The function compile-rule turns a rule into a function definition by building up 
Lisp code that implements all the actions that generate would take in interpreting 
the rule. There are three cases. If every element of the right-hand side is an atom, 
then the rule is a lexical rule, which compiles into a call to one-of to pick a word at 
random. If there is only one element of the right-hand side, then bui 1 d - code is called 
to generate code for it. Usually, this will bea call to append to build up a list. Finally, 
if there are several elements in the right-hand side, they are each turned into code 
by build-code; are given a number by build-cases; and then a case statement is 
constructed to choose one of the cases. 

(defun compile-rule (rule) 
"Translate a grammar rule into a LISP function definition." 
(let ((rhs (rule-rhs rule))) 

'(defun ,(rule-lhs rule) () 

.(cond ((every #*atom rhs) *(one-of '.rhs)) 
((length=1 rhs) (build-code (first rhs))) 
(t '(case (random .(length rhs)) 

.(build-cases 0 rhs))))))) 

(defun build-cases (number choices) 
"Return a list of case-clauses" 
(when choices 

(cons (list number (build-code (first choices))) 
(build-cases (+ number 1) (rest choices))))) 

(defun build-code (choice) 
"Append together multiple constituents" 
(cond ((null choice) nil) 

((atom choice) (list choice)) 
((length=1 choice) choice) 
(t '(append .(mapcar #'build-code choice))))) 

(defun length=1 (x) 
"Is X a list of length 1?" 
(and (consp x) (null (rest x)))) 

The Lisp code built by compile-rule must be compiled or interpreted to make it 
available to the Lisp system. We can do that with one of the following forms. 
Normally we would want to call compi1 e, but during debugging it may be easier 
not to. 


<a id='page-277'></a>
(dolist (rule ^grammar*) (eval (compile-rule rule))) 
(dolist (rule *grammar*) (compile (eval (compile-rule rule)))) 

One frequent way to use compilation is to define a macro that expands into the code 
generated by the compiler. That way, we just type in calls to the macro and don't 
have to worry about making sure all the latest rules have been compiled. We might 
implement this as follows: 

(defmacro defrule (&rest rule) 
"Define a grammar rule" 
(compile-rule rule)) 

(defrule Sentence -> (NP VP)) 

(defrule NP -> (Art Noun)) 

(defrule VP -> (Verb NP)) 

(defrule Art -> the a) 

(defrule Noun -> man ball woman table) 

(defrule Verb -> hit took saw liked) 

Actually, the choice of using one big list of rules (like *g r ammar *) versus using individual 
macros to define rules is independent of the choice of compiler versus interpreter. 
Wecould justas easily definedef rule simply to push the ruleonto*grammar*. Macros 
like def rul e are useful when you want to define rules in different places, perhaps in 
several separate files. The def parameter method is appropriate when all the rules 
can be defined in one place. 

We can see the Lisp code generated by compi 1 e - rul e in two ways: by passing it 
a rule directly: 

> (compile-rule '(Sentence -> (NP VP))) 
(DEFUN SENTENCE () 
(APPEND (NP) (VP))) 

> (compile-rule '(Noun -> man ball woman table)) 
(DEFUN NOUN () 
(ONE-OF '(MAN BALL WOMAN TABLE))) 

or by macroexpanding a def rul e expression. The compiler was designed to produce 
the same code we were writing in our first approach to the generation problem (see 
[page 35](chapter2.md#page-35)). 


<a id='page-278'></a>

> (macroexpand '(defrule Adj* -> () Adj (AdJ Adj*))) 
(DEFUN ADJ* () 

(CASE (RANDOM 3) 
(0 NIL) 
(1 (ADJ)) 
(2 (APPEND (ADJ) (ADJ*))))) 

Interpreters are usually easier to write than compilers, although in this case, even 
the compiler was not too difficult. Interpreters are also inherently more flexible than 
compilers, because they put off making decisions until the last possible moment. 
For example, our compiler considers the right-hand side of a rule to be a list of words 
only if every element is an atom. In all other cases, the elements are treated as 
nonterminals. This could cause problems if we extended the definition of Noun to 
include the compound noun "chow chow": 

(defrule Noun -> man ball woman table (chow chow)) 

The rule would expand into the following code: 

(DEFUN NOUN () 

(CASE (RANDOM 5) 
(0 (MAN)) 
(1 (BALD) 
(2 (WOMAN)) 
(3 (TABLE)) 
(4 (APPEND (CHOW) (CHOW))))) 

The problem is that ma. and ball and all the others are suddenly treated as functions, 
not as literal words. So we would get a run-time error notifying us of undefined 
functions. The equivalent rule would cause no trouble for the interpreter, which waits 
until it actually needs to generate a symbol to decide if it is a word or a nonterminal. 
Thus, the semantics of rules are different for the interpreter and the compiler, and 
we as program implementors have to be very careful about how we specify the actual 
meaning of a rule. In fact, this was probably a bug in the interpreter version, since 
it effectively prohibits words like "noun" and "sentence" from occurring as words if 
they are also the names of categories. One possible resolution of the conflict is to 
say that an element of a right-hand side represents a word if it is an atom, and a list 
of categories if it is a list. If we did indeed settle on that convention, then we could 
modify both the interpreter and the compiler to comply with the convention. Another 
possibility would be to represent words as strings, and categories as symbols. 

The flip side of losing run-time flexibility is gaining compile-time diagnostics. For 
example, it turns out that on the Common Lisp system I am currently using, I get 
some useful error messages when I try to compile the buggy version of Noun: 


<a id='page-279'></a>
> (defrule Noun -> man ball woman table (chow chow)) 
The following functions were referenced but don't seem defined: 

CHOW referenced by NOUN 

TABLE referenced by NOUN 

WOMAN referenced by NOUN 

BALL referenced by NOUN 

MAN referenced by NOUN 
NOUN 

Another problem with the compilation scheme outlined here is the possibility of name 
clashes. Under the interpretation scheme, the only names used were the function 
generate and the variable ^grammar*. With compilation, every left-hand side of a 
rule becomes the name of a function. The grammar writer has to make sure he or 
she is not using the name of an existing Lisp function, and hence redefining it. Even 
worse, if more than one grammar is being developed at the same time, they cannot 
have any functions in common. If they do, the user will have to recompile with 
every switch from one grammar to another. This may make it difficult to compare 
grammars. The best away around this problem is to use the Common Lisp idea of 
packages, but for small exercises name clashes can be avoided easily enough, so we 
will not explore packages until section 24.1. 

The major advantage of a compiler is speed of execution, when that makes a 
difference. For identical grammars running in one particular implementation of 
Common Lisp on one machine, our interpreter generates about 75 sentences per 
second, while the compiled approach turns out about 200. Thus, it is more than twice 
as fast, but the difference is negligible unless we need to generate many thousands of 
sentences. In section 9.6 we will see another compiler with an even greater speed-up. 

The need to optimize the code produced by your macros and compilers ultimately 
depends on the quality of the underlying Lisp compiler. For example, consider the 
following code: 

> (defun fl (n 1) 
(let ((11 (first D) 
(12 (second 1))) 
(expt (* 1 (+ . 0)) 
(- 4 (length (list 11 12)))))) 
Fl 

> (defun f2 (n 1) (* . n)) F2 

> (disassemble 'fl ) 
6 PUSH ARG 10 ; . 
7 MOVEM PDL-PUSH 
8 * PDL-POP 
9 RETURN PDL-POP 
Fl 


<a id='page-280'></a>

> (disassemble 'f2) 

6 PUSH ARGO ; . 

7 MOVEM PDL-PUSH 

8 * PDL-POP 

9 RETURN PDL-POP 

F2 

This particular Lisp compiler generates the exact same code for f 1 and f 2. Both 
fimctions square the argument n, and the four machine instructions say, "Take the 
0th argument, make a copy of it, multiply those two numbers, and return the result." 
It's clear the compiler has some knowledge of the basic Lisp functions. In the case 
of f 1, it was smart enough to get rid of the local variables 11 and 12 (and their 
initialization), as well as the calls to first, second, length, and 1 i st and most of the 
arithmetic. The compiler could do this because it has knowledge about the functions 
length and 1 i st and the arithmetic functions. Some of this knowledge might be in 
the form of simplification rules. 

As a user of this compiler, there's no need for me to write clever macros or 
compilers that generate streamlined code as seen in f 2; I can blindly generate code 
with possible inefficiencies Uke those in f 1, and assume that the Lisp compiler 
will cover up for my laziness. With another compiler that didn't know about such 
optimizations, I would have to be more careful about the code I generate. 

9.3 Delaying Computation 
Back on [page 45](chapter2.md#page-45), we saw a program to generate all strings derivable from a grammar. 
One drawback of this program was that some grammars produce an infinite number 
of strings, so the program would not terminate on those grammars. 

It turns out that we often want to deal with infinite sets. Of course, we can't 
enumerate all the elements of an infinite set, but we should be able to represent the 
set and pick elements out one at a time. In other words, we want to be able to specify 
how a set (or other object) is constructed, but delay the actual construction, perhaps 
doing it incrementally over time. This soimds like a job for closures: we can specify 
the set constructor as a function, and then call the function some time later. We will 
implement this approach with the sjmtax used in Scheme—the macro del ay builds a 
closure to be computed later, and the function force calls that function and caches 
away the value. We use structures of type del ay to implement this. A delay structure 
has two fields: the value and the function. Initially, the value field is undefined, and 
the function field holds the closure that will compute the value. The first time the 
delay is forced, the function is called, and its result is stored in the value field. The 
function field is then set to nil to indicate that there is no need to call the function 
again. The function force checks if the fimction needs to be called, and returns the 


<a id='page-281'></a>

value. If force is passed an argument that is not a delay, it just returns the argument. 

(defstruct delay (value nil) (function nil)) 

(defmacro delay (&rest body) 
"A computation that can be executed later by FORCE." 
*(make-delay :function #'(lambda () . .body))) 

(defun force (x) 
"Find the value of x. by computing if it is a delay." 
(if (not (delay-p x)) 

. 
(progn 
(when (delay-function x) 
(setf (delay-value x) 
(funcall (delay-function x))) 
(setf (delay-function x) nil)) 
(delay-value x)))) 

Here's an example of the use of del ay. The list . is constructed using a combination 
of normal evaluation and delayed evaluation. Thus, the 1 is printed when . is created, 
but the 2 is not: 

> (setf X (list (print 1) (delay (print 2)))) 

1 

(1 #S(DELAY .-FUNCTION (LAMBDA () (PRINT 2)))) 

The second element is evaluated (and printed) when it is forced. But then forcing it 
again just retrieves the cached value, rather than calling the function again: 

> (force (second x)) 

2 

2 

> X => (1 #S(DELAY :VALUE 2)) 

> (force (second x)) 2 

Now let's see how delays can be used to build infinite sets. An infinite set will be 
considered a special case of what we will call a pipe: a list with a first component 
that has been computed, and a rest component that is either a normal list or a 
delayed value. Pipes have also been called delayed Usts, generated lists, and (most 
commonly) streams. We will use the term pipe because stream already has a meaning 
in Common Lisp. The bookArtificial Intelligence Programming (Charniak et al. 1987) 


<a id='page-282'></a>

also calls these structures pipes, reserving streams for delayed structures that do not 
cache computed results. 

To distinguish pipes from lists, we will use the accessors head and tai 1 instead 
of first and rest. We will also use empty-pipe instead of ni 1, make-pipe instead 
of cons, and pipe-el t instead of el t. Note that make-pipe is a macro that delays 
evaluation of the tail. 

(defmacro make-pipe (head tail) 
"Create a pipe by evaluating head and delaying tail." 
'(cons .head (delay .tail))) 

(defconstant empty-pipe nil) 

(defun head (pipe) (first pipe)) 
(defun tail (pipe)(force (rest pipe))) 

(defun pipe-elt (pipe i) 
"The i-th element of a pipe. 0-based" 
(if (= i 0) 

(head pipe) 
(pipe-elt (tail pipe) (-i 1)))) 

Here's a function that can be used to make a large or infinite sequence of integers 
with delayed evaluation: 

(defun integers (&optional (start 0) end) 
"A pipe of integers from START to END. 
If END is nil. this is an infinite pipe." 
(if (or (null end) (<= start end)) 

(make-pipe start (integers (+ start 1) end)) 
nil)) 

And here is an example of its use. The pipe c represents the numbers from 0 to infinity. 
When it is created, only the zeroth element, 0, is evaluated. The computation 
of the other elements is delayed. 

> (setf c (integers 0)) ^ (0 . #S(DELAY :FUNCTION #<CLOSURE -77435477>)) 

> (pipe-elt c 0) =.> 0 

Calling pi pe - el t to look at the third element causes the first through third elements 
to be evaluated. The numbers 0 to 3 are cached in the correct positions, and further 
elements remain unevaluated. Another call to pi pe-el t with a larger index would 
force them by evaluating the delayed function. 


<a id='page-283'></a>
> (pipe-elt c 3) ^ 3 

> c => 

(0 . #S(DELAY 
:VALUE 
(1 . #S(DELAY 
:VALUE 
(2 . #S(DELAY 
:VALUE 
(3 . #S(DELAY 
:FUNCTION 
#<CLOSURE -77432724>))))))) 
While this seems to work fine, there is a heavy price to pay. Every delayed value must 
be stored in a two-element structure, where one of the elements is a closure. Thus, 
there is some storage wasted. There is also some time wasted, as ta . or pi pe-elt 
must traverse the structures. 

An alternate representation for pipes is as(value. closure) pairs, where the closure 
values are stored into the actual cons cells as they are computed. Previously we 
needed structures of type del ay to distinguish a delayed from a nondelayed object, 
but in a pipe we know the rest can be only one of three things: nil, a list, or a delayed 
value. Thus, we can use the closures directly instead of using del ay structures, if we 
have some way of distinguishing closures from lists. Compiled closures are atoms, so 
they can always be distinguished from lists. But sometimes closures are implemented 
as lists beginning with 1 ambda or some other implementation-dependent symbol.^ 
The built-in function functionp is defined to be true of such lists, as well as of all 
symbols and all objects returned by compi 1 e. But using functionp means that we 
can not have a pipe that includes the symbol 1 ambda as an element, because it will be 
confused for a closure: 

> (functionp (last '(theta iota kappa lambda))) ^ . 

If we consistently use compiled functions, then we could eliminate the problem by 
testing with the built-in predicate compi 1 ed-function-p. The following definitions 
do not make this assumption: 

(defmacro make-pipe (head tail) 
"Create a pipe by evaluating head and delaying tail." 
'(cons .head #'(lambda () .tail))) 

^In KCL, the symbol 1 ambda -cl osure is used, and in Allegro, it is excl:. 1 exi cal - cl osure. 


<a id='page-284'></a>

(defun tail (pipe) 
"Return tail of pipe or list, and destructively update 
the tail if it is a function." 
(if (functionp (rest pipe)) 

(setf (rest pipe) (funcall (rest pipe))) 
(rest pipe))) 

Everything else remains the same. If we recompile integers (because it uses the 
macro make -pi pe), we see the following behavior. First, creation of the infinite pipe 
c is similar: 

> (setf c (integers 0)) (0 , #<CLOSURE 77350123>) 

> (pipe-elt c 0) => 0 

Accessing an element of the pipe forces evaluation of all the intervening elements, 
and as before leaves subsequent elements unevaluated: 

> (pipe-elt c 5) => 5 

> c => (0 1 2 3 4 5 . #<CLOSURE 77351636> 

Pipes can also be used for finite lists. Here we see a pipe of length 11: 

> (setf i (integers 0 10)) => (0 . #<CLOSURE 77375357>) 

> (pipe-elt i 10) ^ 10 

> (pipe-elt i 11) => NIL 

> i ^ (0 1 2 3 4 5 6 7 8 9 10) 

Clearly, this version wastes less space and is much neater about cleaning up after 
itself. In fact, a completely evaluated pipe turns itself into a list! This efficiency was 
gained at the sacrifice of a general principle of program design. Usually we strive 
to build more complicated abstractions, like pipes, out of simpler ones, like delays. 
But in this case, part of the functionality that delays were providing was duplicated 
by the cons cells that make up pipes, so the more efficient implementation of pipes 
does not use delays at all. 

Here are some more utility functions on pipes: 

(defun enumerate (pipe &key count key (result pipe)) 
"Go through all (or count) elements of pipe, 
possibly applying the KEY function. (Try PRINT.)" 

Returns RESULT, which defaults to the pipe itself, 
(if (or (eq pipe empty-pipe) (eql count 0)) 


<a id='page-285'></a>
result 

(progn 
(unless (null key) (funcall key (head pipe))) 
(enumerate (tail pipe) :count (if count (- count 1)) 

:key key :result result)))) 

(defun filter (pred pipe) 
"Keep only items in pipe satisfying pred." 
(if (funcall pred (head pipe)) 

(make-pipe (head pipe) 
(filter pred (tail pipe))) 
(filter pred (tail pipe)))) 

And here's an application of pipes: generating prime numbers using the sieve of 
Eratosthenes algorithm: 

(defun sieve (pipe) 
(make-pipe (head pipe) 
(filter #'(lambda (x) (/= (mod . (headpipe)) 0)) 
(sieve (tail pipe))))) 

(defvar *primes* (sieve (integers 2))) 

> *primes* ^ (2 . #<CLOSURE 3075345>) 

> (enumerate *primes* icount 10) => 
(2 3 5 7 11 13 17 19 23 29 31 . #<CLOSURE 5224472> 

Finally, let's return to the problem of generating all strings in a grammar. First we're 
going to need some more utility functions: 

(defun map-pipe (fn pipe) 
"Map fn over pipe, delaying all but the first fn call." 
(if (eq pipe empty-pipe) 

empty-pipe 
(make-pipe (funcall fn (head pipe)) 
(map-pipe fn (tail pipe))))) 

(defun append-pipes (x y) 
"Return a pipe that appends the elements of . and y." 
(if (eq X empty-pipe) 

y 
(make-pipe (head x) 
(append-pipes (tail x) y)))) 


<a id='page-286'></a>

(defun mappend-pipe (fn pipe) 
"Lazily map fn over pipe, appending results." 
(if (eq pipe empty-pipe) 

empty-pipe 
(let ((X (funcall fn (head pipe)))) 
(make-pipe (head x) 
(append-pipes (tail x) 
(mappend-pipe 
fn (tail pipe))))))) 

Now we can rewrite generate-all and combine-all to use pipes instead of lists. 
Everything else is the same as on [page 45](chapter2.md#page-45). 

(defun generate-all (phrase) 
"Generate a random sentence or phrase" 
(if (listp phrase) 

(if (null phrase) 
(list nil) 
(combine-all-pipes 

(generate-all (first phrase)) 
(generate-all (rest phrase)))) 
(let ((choices (rule-rhs (assoc phrase *grammar*)))) 

(if choices 
(mappend-pipe #*generate-all choices) 
(list (list phrase)))))) 

(defun combine-all-pipes (xpipe ypipe) 
"Return a pipe of pipes formed by appending a y to an x" 
;; In other words, form the cartesian product, 
(mappend-pipe 

#'(lambda (y) 
(map-pipe #'(lambda (x) (append-pipes . y)) 
xpipe)) 
ypipe)) 

With these definitions, here's the pipe of all sentences from *grammar2* (from 
[page 43](chapter2.md#page-43)): 

> (setf ss (generate-all 'sentence)) 
((THE . #<CLOSURE 27265720>) . #<CLOSURE 27266035> 


<a id='page-287'></a>

> (enumerate ss rcount 5) =i> 

((THE . #<CLOSURE 27265720>) 

(A . #<CLOSURE 27273143> 
(THE . #<CLOSURE 27402545>) 
(A . #<CLOSURE 27404344>) 
(THE . #<CLOSURE 27404527>) 
(A . #<CLOSURE 27405473> . #<CLOSURE 27405600>) 
> (enumerate ss .-count 5 :key #'enumerate) 

((THE MAN HIT THE MAN) 
(A MAN HIT THE MAN) 
(THE BIG MAN HIT THE MAN) 
(A BIG MAN HIT THE MAN) 
(THE LITTLE MAN HIT THE MAN) 
(THE . #<CLOSURE 27423236>) . #<CL0SURE 27423343> 

> (enumerate (pipe-elt ss 200)) 

(THE ADIABATIC GREEN BLUE MAN HIT THE MAN) 

While we were able to represent the infinite set of sentences and enumerate instances 
of it, we still haven't solved all the problems. For one, this enumeration will never 
get to a sentence that does not have "hit the man" as the verb phrase. We will see 
longer and longer lists of adjectives, but no other change. Another problem is that 
left-recursive rules will still cause infinite loops. For example, if the expansion for 
Adj*hadbeen (Adj* -> (Adj*Adj) ()) instead of (Adj* -> () (Adj Adj*)), 
then the enumeration would never terminate, because pipes need to generate a first 
element. 

We have used delays and pipes for two main purposes: to put off until later 
computations that may not be needed at all, and to have an expHcit representation of 
large or infinite sets. It should be mentioned that the language Prolog has a different 
solution to the first problem (but not the second). As we shall see in chapter 11, Prolog 
generates solutions one at a time, automatically keeping track of possible backtrack 
points. Where pipes allow us to represent an infinite number of alternatives in the 
data, Prolog allows us to represent those alternatives in the program itself. 

&#9635; Exercise 9.1 [h] When given a function f and a pipe p. mappend-pipe returns a 
new pipe that will eventually enumerate all of ( f (first .)), then all of ( f (second 
.)), and so on. This is deemed "unfair" if ( f (first .)) has an infinite number of 
elements. Define a function that will fairly interleave elements, so that all of them are 
eventually enumerated. Show that the function works by changing generate -a 11 to 
work with it. 


<a id='page-288'></a>

9.4 Indexing Data 
Lisp makes it very easy to use lists as the universal data structure. A list can represent 
a set or an ordered sequence, and a list with sublists can represent a tree or graph. 
For rapid prototyping, it is often easiest to represent data in lists, but for efficiency 
this is not always the best idea. To find an element in a list of length . will take n/2 
steps on average. This is true for a simple list, an association list, or a property list. 
If . can be large, it is worth looking at other data structures, such as hash tables, 
vectors, property lists, and trees. 

Picking the right data structure and algorithm is as important in Lisp as it is in 
any other programming language. Even though Lisp offers a wide variety of data 
structures, it is often worthwhile to spend some effort on building just the right data 
structure for frequently used data. For example. Lisp's hash tables are very general 
and thus can be inefficient. You may want to build your own hash tables if, for 
example, you never need to delete elements, thus making open hashing an attractive 
possibility. We will see an example of efficient indexing in section 9.6 ([page 297](chapter9.md#page-297)). 

9.5 Instrumentation: Deciding What 
to Optimize 
Because Lisp is such a good rapid-prototyping language, we can expect to get a 
working implementation quickly. Before we go about trying to improve the efficiency 
of the implementation, it is a good idea to see what parts are used most often. 
Improving little-used features is a waste of time. 

The minimal support we need is to count the number of calls to selected functions, 
and then print out the totals. This is called profiling the functions.^ For each function 
to be profiled, we change the definition so that it increments a counter and then calls 
the original function. 

Most Lisp systems have some built-in profiling mechanism. If your system has 
one, by all means use it. The code in this section is provided for those who lack such 
a feature, and as an example of how functions can be manipulated. The following is 
a simple profiling facility. For each profiled function, it keeps a count of the number 
of times it is called under the prof i 1 e - count property of the function's name. 

^The terms metering and monitoring are sometimes used instead of profiling. 


<a id='page-289'></a>
(defun profilel (fn-name) 

"Make the function count how often it is called" 
First save away the old, unprofiled function 
Then make the name be a new function that increments 
a counter and then calls the original function 

(let ((fn (symbol-function fn-name))) 
(setf (get fn-name 'unprofiled-fn) fn) 
(setf (get fn-name 'profile-count) 0) 
(setf (symbol-function fn-name) 

(profiled-fn fn-name fn)) 
fn-name)) 

(defun unprofilel (fn-name) 
"Make the function stop counting how often it is called." 
(setf (symbol-function fn-name) (get fn-name 'unprofiled-fn)) 
fn-name) 

(defun profiled-fn (fn-name fn) 
"Return a function that increments the count." 
#'(lambda (&rest args) 

(incf (get fn-name 'profile-count)) 

(apply fn args))) 

(defun profile-count (fn-name) (get fn-name 'profile-count)) 

(defun profile-report (fn-names &optional (key #'profile-count)) 
"Report profiling statistics on given functions." 
(loop for name in (sort fn-names #'> :key key) do 

(format t "~&~7D ~A" (profile-count name) name))) 

That's all we need for the bare-bones functionality. However, there are a few ways 
we could improve this. First, it would be nice to have macros that, like trace and 
untrace, allow the user to profile multiple functions at once and keep track of what 
has been profiled. Second, it can be helpful to see the length of time spent in each 
function, as well as the number of calls. 

Also, it is important to avoid profiling a function twice, since that would double 
the number of calls reported without alerting the user of any trouble. Suppose we 
entered the following sequence of commands: 

(defun f (X) (g x)) 
(profilel 'f) 
(profilel 'f) 

Then the definition of f would be roughly: 


<a id='page-290'></a>

(lambda (&rest args) 
(incf (get 'f 'profile-count)) 
(apply #'(lambda (&rest args) 

(incf (get 'f 'profile-count)) 
(apply #'(lambda (x) (g x)) 
args)) 
args)) 

The result is that any call to f will eventually call the original f, but only after 
incrementing the count twice. 

Another consideration is what happens when a profiled function is redefined by 
the user. The only way we could ensure that a redefined function would continue 
profiling would be to change the definition of the macro defun to look for functions 
that should be profiled. Changing system functions like defun is a risky prospect, 
and in Common Lisp the Language, 2d edition, it is explicitly disallowed. Instead, 
we'll do the next best thing: ensure that the next call to prof i 1 e will reprofile any 
functions that have been redefined. We do this by keeping track of both the original 
unprofiled function and the profiled function. We also keep a list of all functions 
that are currently profiled. 

In addition, we will count the amount of time spent in each function. However, 
the user is cautioned not to trust the timing figures too much. First, they include the 
overhead cost of the profiling facility. This can be significant, particularly because 
the facility conses, and thus can force garbage collections that would not otherwise 
have been done. Second, the resolution of the system clock may not be fine enough 
to make accurate timings. For functions that take about 1/10 of a second or more, the 
figures will be reliable, but for quick functions they may not be. 

Here is the basic code for prof i 1 e and unprof i 1 e: 

(defvar *profiled-functions* nil 
"Function names that are currently profiled") 

(defmacro profile (&rest fn-names) 
"Profile fn-names. With no args, list profiled functions." 
'(mapcar #'profilel 

(setf *profiled-functions* 
(union *profiled-functions* ',fn-names)))) 

(defmacro unprofile (&rest fn-names) 
"Stop profiling fn-names. With no args, stop all profiling." 
'(progn 

(mapcar #'unprofilel 
,(if fn-names ",fn-names '*profiled-functions*)) 
(setf *profiled-functions* 
.(if (null fn-names) 
nil 


<a id='page-291'></a>
'(set-difference *profiled-functions* 
*,fn-names))))) 

The idiom *',fn-names deserves comment, since it is common but can be confusing 
at first. It may be easier to understand when written in the equivalent form 
' (quote ,fn-names). As always, the backquote builds a structure with both constant 
and evaluated components. In this case, the quote is constant and the variable 
fn-names is evaluated. In MacLisp, the function kwote was defined to serve this 
purpose: 

(defun kwote (x) (list 'quote x)) 

Now we need to change prof i1 el and unprof i1 el to do the additional bookkeeping: 
For prof i1 el, there are two cases. If the user does a prof i 1 el on the same function 
name twice in a row, then on the second time we will notice that the current function 
is the same as the functioned stored under the profiled-fn property, so nothing 
more needs to be done. Otherwise, we create the profiled function, store it as the 
current definition of the name under the prof i1 ed-f . property, save the unprofiled 
function, and initialize the counts. 

(defun profilel (fn-name) 

"Make the function count how often it is called" 
First save away the old, unprofiled function 
Then make the name be a new function that increments 
a counter and then calls the original function 

(let ((fn (symbol-function fn-name))) 
(unless (eq fn (get fn-name 'profiled-fn)) 
(let ((new-fn (profiled-fn fn-name fn))) 

(setf (symbol-function fn-name) new-fn 
(get fn-name 'profiled-fn) new-fn 
(get fn-name 'unprofiled-fn) fn 
(get fn-name 'profile-time) 0 
(get fn-name 'profile-count) 0)))) 

fn-name) 

(defun unprofilel (fn-name) 
"Make the function stop counting how often it is called." 
(setf (get fn-name 'profile-time) 0) 
(setf (get fn-name 'profile-count) 0) 
(when (eq (symbol-function fn-name) (get fn-name 'profiled-fn)) 

;; normal case: restore unprofiled version 
(setf (symbol-function fn-name) 
(get fn-name 'unprofiled-fn))) 
fn-name) 


<a id='page-292'></a>

Now we look into the question of timing. There is a built-in Common Lisp function, 
get-internal -real -time, that returns the elapsed time since the Lisp session 
started. Because this can quickly become a bignum, some implementations 
provide another timing function that wraps around rather than increasing forever, 
but which may have a higher resolution than get-internal - real - time. For example, 
on TI Explorer Lisp Machines, get-internal-real-time measures 1/60second 
intervals, while time:microsecond-time measures l/l,000,000-second intervals, 
but the value returned wraps around to zero every hour or so. The function 
time:microsecond-time-difference is used to compare two of these numbers 
with compensation for wraparound, as long as no more than one wraparound 
has occurred. 

In the code below, I use the conditional read macro characters #+ and #- to define 
the right behavior on both Explorer and non-Explorer machines. We have seeen 
that # is a special character to the reader that takes different action depending on 
the following character. For example, #' f . is read as (function f .). The character 
sequence #+ is defined so that ^+feature expression reads as expression if thefeature is 
defined in the current implementation, and as nothing at all if it is not. The sequence 
#- acts in just the opposite way. For example, on a TI Explorer, we would get the 
following: 

> '(hi #+TI t #+Symbolics s #-Explorer e #-Mac m) ^ (HI . .) 

The conditional read macro characters are used in the following definitions: 

(defun get-fast-time () 
"Return the elapsed time. This may wrap around; 
use FAST-TIME-DIFFERENCE to compare." 
#+Explorer (time:microsecond-time) ; do this on an Explorer 
#-Explorer (get-internal-real-time)) ; do this on a non-Explorer 

(defun fast-time-difference (end start) 
"Subtract two time points." 
#+Explorer (time:microsecond-time-difference end start) 
#-Explorer (- end start)) 

(defun fast-time->seconds (time) 
"Convert a fast-time interval into seconds." 
#+Explorer (/ time 1000000.0) 
#-Explorer (/ time internal-time-units-per-second)) 

The next step is to update prof i1 ed - f . to keep track of the timing data. The simplest 
way to do this would be to set a variable, say start, to the time when a function is 
entered, run the function, and then increment the function's time by the difference between 
the current time and start. The problem with this approach is that every func



<a id='page-293'></a>
tion in the call stack gets credit for the time of each called function. Suppose the function 
f calls itself recursively five times, with each call and return taking place a second 
apart, so that the whole computation takes nine seconds. Then f will be charged nine 
seconds for the outer call, seven seconds for the next call, and so on, for a total of 
25 seconds, even though in reality it only took nine seconds for all of them together. 

A better algorithm would be to charge each function only for the time since the 
last call or return. Then f would only be charged the nine seconds. The variable 
*prof i le-call -stack* is used to holdastack of functionname/entry time pairs. This 
stack is manipulated by prof i 1 e-enter and prof i 1 e-exi t to get the right timings. 

The functions that are used on each call to a profiled function are declared inline. 
In most cases, a call to a function compiles into machine instructions that set up the 
argument list and branch to the location of the function's definition. With an i nl i ne 
function, the body of the function is compiled in line at the place of the function 
call. Thus, there is no overhead for setting up the argument list and branching to the 
definition. An i nl i ne declaration can appear anywhere any other declaration can 
appear. In this case, the function proel aim is used to register a global declaration. 
Inline declarations are discussed in more depth on [page 317](chapter10.md#page-317). 

(proclaim '(inline profile-enter profile-exit inc-profile-time)) 

(defun profiled-fn (fn-name fn) 
"Return a function that increments the count, and times." 
#'(lambda (&rest args) 

(profile-enter fn-name) 

(multiple-value-progl 
(apply fn args) 
(profile-exit fn-name)))) 

(defvar *profile-call-stack* nil) 

(defun profile-enter (fn-name) 
(incf (get fn-name 'profile-count)) 
(unless (null *profile-call-stack*) 

Time charged against the calling function: 
(inc-profile-time (first *profile-call-stack*) 

(car (first *profile-call-stack*)))) 
;; Put a new entry on the stack 
(push (cons fn-name (get-fast-time)) 

*profi le-call-stack*)) 

(defun profile-exit (fn-name) 
Time charged against the current function: 
(inc-profile-time (pop *profile-call-stack*) 
fn-name) 
Change the top entry to reflect current time 
(unless (null *profile-call-stack*) 
(setf (cdr (first *profile-call-stack*)) 
(get-fast-time)))) 


<a id='page-294'></a>

(defun inc-profile-time (entry fn-name) 
(incf (get fn-name 'profile-time) 
(fast-time-difference (get-fast-time) (cdr entry)))) 

Finally, we need to update prof i 1 e- report to print the timing data as well as the 
counts. Note that the default f .- names is a copy of the global list. That is because we 
pass f .- names to sort, which is a destructive function. We don't want the global list 
to be modified as a result of this sort. 

(defun profile-report (&optional 
(fn-names (copy-list *profiled-functions*)) 
(key #'profile-count)) 

"Report profiling statistics on given functions." 
(let ((total-time (reduce #'+ (mapcar #'profile-time fn-names)))) 
(unless (null key) 
(setf fn-names (sort fn-names #'> :key key))) 
(format t "~&Total elapsed time: ~d seconds." 

(fast-time->seconds total-time)) 
(format t "~& Count Sees Time% Name") 
(loop for name in fn-names do 

(format t "~&~7D ~6,2F  ~3d% ~A" 
(profile-count name) 
(fast-time->seconds (profile-time name)) 
(round (/ (profile-time name) total-time) .01) 
name)))) 

(defun profile-time (fn-name) (get fn-name 'profile-time)) 

These functions can be used by calling profile, then doing some representative com


putation, then calling prof i 1 e - report, and finally unprof i 1 e. It can be convenient 

to provide a single macro for doing all of these at once: 

(defmacro with-profiling (fn-names &rest body) 

'(progn 
(unprofile . ,fn-names) 
(profile . .fn-names) 
(setf *profile-call-stack* nil) 
(unwind-protect 

(progn . .body) 
(profile-report',fn-names) 
(unprofile . .fn-names)))) 

Note the use of unwi nd - protect to produce the report and call unprof i 1 e even if the 
computation is aborted, unwind-protect is a special form that takes any number 
of arguments. It evaluates the first argument, and if all goes well it then evaluates 


<a id='page-295'></a>
the other arguments and returns the first one, just like progl. But if an error occurs 
during the evaluation of the first argument and computation is aborted, then the 
subsequent arguments (called cleanup forms) are evaluated anyway. 

9.6 A Case Study in Efficiency: The 
SIMPLIFY Program 
Suppose we wanted to speed up the simplify program of chapter 8. This section 
shows how a combination of general techniques—memoizing, indexing, and 
compiling—can be used to speed up the program by a factor of 130. Chapter 15 will 
show another approach: replace the algorithm with an entirely different one. 

The first step to a faster program is defining a benchmark, a test suite representing 
a typical work load. The following is a short list of test problems (and their answers) 
that are typical of the s impli f y task. 

(defvar nest-data* (mapcar #'infix->prefix 

'((d (a*x^2 + b*x + c)/dx) 
(d ((a * . 2 + b * . + c) / x) / d x) 
(d ((a * . ^ 3 + b * . ^ 2 + c * . + d) / . ^ 5) / d x) 
((sin (X + X)) * (sin (2 * x)) + (cos (d (x ^ 2) / d x)) ^ 1) 
(d (3 * X + (cos X) / X) / d X)))) 
(defvar ^^answers* (mapcar #'simplify *test-data*)) 

The function test-it runs through the test data, making sure that each answer is 
correct and optionally printing profiling data. 

(defun test-it (&optional (with-profiling t)) 
"Time a test run, and make sure the answers are correct." 
(let ((answers 

(if with-profiling 
(with-profiling (simplify simplify-exp pat-match 
match-variable variable-p) 
(mapcar #*simplify nest-data*)) 

(time (mapcar #'simplify *test-data*))))) 
(mapc #*assert-equal answers *answers*) 
t)) 

(defun assert-equal (x y) 
"If X is not equal to y, complain." 
(assert (equal . y) (. y) 

"Expected "a to be equal to ~a" . y)) 

Here are the results of (test - i t) with and without profiling: 


<a id='page-296'></a>

> (test-it nil) 
Evaluation of (MAPCAR #'SIMPLIFY *TEST-DATA*) took 6.612 seconds. 

> (test-it t) 
Total elapsed time: 22.819614 seconds. 

Count Sees Time% Name 
51690 11.57 51% PAT-MATCH 
37908 8.75 38% VARIABLE-P 
1393 0.32 1% MATCH-VARIABLE 
906 0.20 1% SIMPLIFY 
274 1.98 9% SIMPLIFY-EXP 

Running the test takes 6.6 seconds normally, although the time triples when the 
profiling overhead is added in. It should be clear that to speed things up, we have 
to either speed up or cut down on the number of calls to pat-match or vari abl e-p, 
since together they account for 89% of the calls (and 89% of the time as well). We 
will look at three methods for achieving both those goals. 

Memoization 

Consider the rule that transforms (x + x) into (2 * .). Once this is done, we have 
to simplify the result, which involves resimplifying the components. If x were some 
complex expression, this could be time-consuming, and it will certainly be wasteful, 
because . is already simplified and cannot change. We have seen this type of problem 
before, and the solution is memoization: make simpl i fy remember the work it has 
done, rather than repeating the work. We can just say: 

(memoize 'simplify :test #'equal) 

Two questions are unclear: what kind of hash table to use, and whether we should 
clear the hash table between problems. The simplifier was timed for all four combinations 
of eq or equal hash tables and resetting or nonresetting between problems. 
The fastest result was equal hashing and nonresetting. Note that with eq hashing, 
the resetting version was faster, presumably because it couldn't take advantage of 
the common subexpressions between examples (since they aren't eq). 

hashing resetting time 
none — 6.6 
equal yes 3.8 
equal no 3.0 
eq yes 7.0 
eq no 10.2 


<a id='page-297'></a>
This approach makes the function simpl i fy remember the work it has done, in 
a hash table. If the overhead of hash table maintenance becomes too large, there is 
an alternative: make the data remember what simplify has done. This approach was 
taken in MACSYMA: it represented operators as lists rather than as atoms. Thus, instead 
of (* 2 X), MACSYMA would use ((*) 2 .). The simplification function would 
destructively insert a marker into the operator list. Thus, the result of simplifying 2x 
would be ((* s i mp) 2 .). Then, when the simplifier was called recursively on this 
expression, it would notice the s i mp marker and return the expression as is. 

The idea of associating memoization information with the data instead of with the 
function will be more efficient unless there are many functions that all want to place 
their marks on the same data. The data-oriented approach has two drawbacks: it 
doesn't identify structures that are equal but not eq, and, because it requires explicitly 
altering the data, it requires every other operation that manipulates the data to know 
about the markers. The beauty of the hash table approach is that it is transparent; no 
code needs to know that memoization is taking place. 

Indexing 

We currently go through the entire list of rules one at a time, checking each rule. This 
is inefficient because most of the rules could be trivially ruled out—if only they were 
indexed properly. The simplest indexing scheme would be to have a separate list 
of rules indexed under each operator. Instead of having simpl ify-exp check each 
member of *s i mpl i f i cat i on - rul es*, it could look only at the smaller list of rules for 
the appropriate operator. Here's how: 

(defun simplify-exp (exp) 
"Simplify using a rule, or by doing arithmetic, 
or by using the simp function supplied for this operator. 
This version indexes simplification rules under the operator." 
(cond ((simplify-by-fn exp)) 

((rule-based-translator exp (rules-for (exp-op exp)) 
:rule-if #'exp-lhs :rule-then #'exp-rhs 
:action #'(lambda (bindings response) 

(simplify (sublis bindings response))))) 
((evaluable exp) (eval exp)) 
(t exp))) 

(defvar *rules-for* (make-hash-table :test #*eq)) 

(defun main-op (rule) (exp-op (exp-lhs rule))) 


<a id='page-298'></a>

(defun index-rules (rules) 
"Index all the rules under the main op." 
(clrhash *rules-for*) 
(dolist (rule rules) 

:; nconc instead of push to preserve the order of rules 
(setf (gethash (main-op rule) *rules-for*) 
(nconc (gethash (main-op rule) *rules-for*) 
(list rule))))) 

(defun rules-for (op) (gethash op *rules-for*)) 

(i ndex-rules *s i mpli fi cati on-rul es*) 

Timing the memoized, indexed version gets us to .98 seconds, down from 6.6 seconds 
for the original code and 3 seconds for the memoized code. If this hadn't helped, we 
could have considered more sophisticated indexing schemes. Instead, we move on 
to consider other means of gaining efficiency. 

&#9635; Exercise 9.2 [m] The list of rules for each operator is stored in a hash table with 
the operator as key. An alternative would be to store the rules on the property list 
of each operator, assuming operators must be symbols. Implement this alternative, 
and time it against the hash table approach. Remember that you need some way of 
clearing the old rules—trivial with a hash table, but not automatic with property lists. 

Compilation 

You can look at simpl i fy-exp as an interpreter for the simplification rule language. 
One proven technique for improving efficiency is to replace the interpreter with a 
compiler. Forexample, the rule (x + . = 2 * .) could be compiled into something 
like: 

(lambda (exp) 
(if (and (eq (exp-op exp) '+) (equal (exp-lhs exp) (exp-rhs exp))) 
(make-exp :op '* :lhs 2 :rhs (exp-rhs exp)))) 

This eliminates the need for consing up and passing around variable bindings, and 
should be faster than the general matching procedure. When used in conjunction 
with indexing, the individual rules can be simpler, because we already know we have 
the right operator. For example, with the above rule indexed under "->-", it could now 
be compiled as: 


<a id='page-299'></a>
(lambda (exp) 
(if (equal (exp-lhs exp) (exp-rhs exp)) 
(make-exp :op '* :lhs 2 :rhs (exp-lhs exp)))) 

It is important to note that when these functions return nil, it means that they 
have failed to simplify the expression, and we have to consider another means of 
simplification. 

Another possibility is to compile a set of rules all at the same time, so that the 
indexing is in effect part of the compiled code. As an example, I show here a small set 
of rules and a possible compilation of the rule set. The generated function assumes 
that . is not an atom. This is appropriate because we are replacing simpl 1 fy-exp, 
not simpl ify. Also, we will return nil to indicate that . is already simplified. I 
have chosen a slightly different format for the code; the main difference is the let 
to introduce variable names for subexpressions. This is useful especially for deeply 
nested patterns. The other difference is that I explicitly build up the answer with a 
call to 1 i St, rather than make-exp. This is normally considered bad style, but since 
this is code generated by a compiler, I wanted it to be as efficient as possible. If the 
representation of the exp data type changed, we could simply change the compiler; a 
much easier task than hunting down all the references spread throughout a human-
written program. The comments following were not generated by the compiler. 

(x * 1 = x) 
(1 * . = x) 
(x * 0 = 0) 
(0 * . = 0) 
(X * X = . ^ 2) 

(lambda (x) 
(let ((xT (exp-lhs x)) 
(xr (exp-rhs x))) 
(or (if (eql xr *1) 
xl) 
(if (eql xl *1) 
xr) 
(if (eql xr *0) 
.) 
(if (eql xl .) 
.) 
(if (equal xr xl) 
(list Xl '2))))) 

; (x 1 = X) 
; (1 ' X = X) 
; (X ' 0 = 0) 
; (0 ' X = 0) 
: (X * X = X ^ 2) 

I chose this format for the code because I imagined (and later show) that it would be 
fairly easy to write the compiler for it. 


<a id='page-300'></a>

The Single-Rule Compiler 

Here I show the complete single-rule compiler, to be followed by the indexed-rule-set 
compiler. The single-rule compiler works like this: 

> (compile-rule '(= (+ . x) (* 2 x))) 
(LAMBDA (X) 
(IF (OP? X '+) 
(LET ((XL (EXP-LHS X)) 
(XR (EXP-RHS X))) 
(IF (EQUAL XR XL) 
(SIMPLIFY-EXP (LIST '* '2 XL)))))) 

Given a rule, it generates code that first tests the pattern and then builds the right-
hand side of the rule if the pattern matches. As the code is generated, correspondences 
are built between variables in the pattern, like x, and variables in the generated 
code, like xl. These are kept in the association Ust *bi ndi ngs*. The matching can be 
broken down into four cases: variables that haven't been seen before, variables that 
have been seen before, atoms, and lists. For example, the first time we run across 
. in the rule above, no test is generated, since anything can match x. But the entry 

(x . xl) is added to the *bi ndi ngs* Hst to mark the equivalence. When the second . 
is encountered, the test (equal xr xl) is generated. 
Organizing the compiler is a little tricky, because we have to do three things at 
once: return the generated code, keep track of the *b i ndi ngs*, andkeep track of what 
to do "next"—that is, when a test succeeds, we need to generate more code, either 
to test further, or to build the result. This code needs to know about the bindings, 
so it can't be done before the first part of the test, but it also needs to know where it 
should be placed in the overall code, so it would be messy to do it after the first part 
of the test. The answer is to pass in a function that will tell us what code to generate 
later. This way, it gets done at the right time, and ends up in the right place as well. 
Such a function is often called a continuation, because it tells us where to continue 
computing. In our compiler, the variable consequent is a continuation function. 

The compiler is called compi 1 e - rul e. It takes a rule as an argument and returns 
a lambda expression that implements the rule. 

(defvar *bindings* nil 
"A list of bindings used by the rule compiler.") 

(defun compile-rule (rule) 
"Compile a single rule." 
(let ((*bindings* nil)) 

'(lambda (x) 
,(compile-exp 'x (exp-lhs rule) ; . is the lambda parameter 
(delay (build-exp (exp-rhs rule) 


<a id='page-301'></a>
^bindings*)))))) 

All the work is done by compi 1 e-exp, which takes three arguments: a variable that 
will represent the input in the generated code, a pattern that the input should be 
matched against, and a continuation for generating the code if the test passes. There 
are five cases: (1) If the pattern is a variable in the list of bindings, then we generate 
an equality test. (2) If the pattern is a variable that we have not seen before, then 
we add it to the binding list, generate no test (because anything matches a variable) 
and then generate the consequent code. (3) If the pattern is an atom, then the match 
succeeds only if the input is eql to that atom. (4) If the pattern is a conditional like 
(?i s . numberp), then we generate the test (numberp .). Other such patterns could 
be included here but have not been, since they have not been used. Finally, (5) if the 
pattern is a list, we check that it has the right operator and arguments. 

(defun compile-exp (var pattern consequent) 
"Compile code that tests the expression, and does consequent 
if it matches. Assumes bindings in *bindings*." 
(cond ((get-binding pattern *bindings*) 

Test a previously bound variable 
*(if (equal ,var ,(lookup pattern *bindings*)) 
,(force consequent))) 

((variable-p pattern) 
;; Add a new bindings; do type checking if needed, 
(push (cons pattern var) *bindings*) 
(force consequent)) 

((atom pattern) 
Match a literal atom 
*(if (eql ,var pattern) 
,(force consequent))) 
((starts-with pattern '?is) 
(push (cons (second pattern) var) *bindings*) 

'(if(,(third pattern) ,var) 
,(force consequent))) 
;; So, far, only the ?is pattern is covered, because 
;; it is the only one used in simplification rules. 
;; Other patterns could be compiled by adding code here. 

Or we could switch to a data-driven approach, 
(t Check the operator and arguments 
'(if (op? ,var *,(exp-op pattern)) 
,(compile-args var pattern consequent))))) 

The function compi 1 e - a rgs is used to check the arguments to a pattern. It generates 
a let form binding one or two new variables (for a unary or binary expression), and 
then calls compi 1 e-exp to generate code that actually makes the tests. It just passes 
along the continuation, consequent, to compi 1 e-exp. 


<a id='page-302'></a>

(defun compile-args (var pattern consequent) 
"Compile code that checks the arg or args, and does consequent 
if the arg(s) match." 

First make up variable names for the arg(s). 
(let ((L (symbol var 'D) 
(R (symbol var 'R))) 
(if (exp-rhs pattern) 
;; two arg case 

'(let ((,L (exp-lhs ,var)) 
(,R (exp-rhs ,var))) 
,(compile-exp L (exp-lhs pattern) 
(delay 
(compile-exp R (exp-rhs pattern) 
consequent)))) 
one arg case 

'(let ((,L (exp-lhs ,var))) 
,(compile-exp L (exp-lhs pattern) consequent))))) 
The remaining functions are simpler, bui 1 d-exp generates code to build the right-
hand side of a rule, op? tests if its first argument is an expression with a given 
operator, and symbol constructs a new symbol. Also given is new-symbol, although 
it is not used in this program. 

(defun build-exp (exp bindings) 
"Compile code that will build the exp, given the bindings." 
(cond ((assoc exp bindings) (rest (assoc exp bindings))) 

((variable-p exp) 
(error "Variable ~a occurred on right-hand side,~ 

but not left." exp)) 
((atom exp) ",exp) 
(t (let ((new-exp (mapcar #*(lambda (x) 

(build-exp . bindings)) 
exp))) 

'(simplify-exp (list .,new-exp)))))) 
(defun op? (exp op) 
"Does the exp have the given op as its operator?" 
(and (exp-p exp) (eq (exp-op exp) op))) 

(defun symbol (&rest args) 
"Concatenate symbols or strings to form an interned symbol" 
(intern (format nil "-{-a^}" args))) 

(defun new-symbol (&rest args) 
"Concatenate symbols or strings to form an uninterned symbol" 
(make-symbol (format nil "'"{^a"}" args))) 


<a id='page-303'></a>
Here are some examples of the compiler: 

> (compile-rule '(= (log (^ e x)) x)) 
(LAMBDA (X) 
(IF (OP? X 'LOG) 
(LET ((XL (EXP-LHS X))) 
(IF (OP? XL 
(LET ((XLL (EXP-LHS XL)) 
(XLR (EXP-RHS XL))) 
(IF (EQL XLL .) 
XLR)))))) 

> (compile-rule (simp-rule '(n * (m * x) = (n * m) * x))) 
(LAMBDA (X) 
(IF (OP? X .*) 
(LET ((XL (EXP-LHS X)) 
(XR (EXP-RHS X))) 
(IF (NUMBERP XL) 
(IF (OP? XR '*) 
(LET ((XRL (EXP-LHS XR)) 
(XRR (EXP-RHS XR))) 
(IF (NUMBERP XRL) 
(SIMPLIFY-EXP 

(LIST .* 
(SIMPLIFY-EXP (LIST '* XL XRL)) 
XRR))))))))) 

The Rule-Set Compiler 

The next step is to combine the code generated by this single-rule compiler to generate 
more compact code for sets of rules. We'll divide up the complete set of rules into 
subsets based on the main operator (as we did with the rules-for function), and 
generate one big function for each operator. We need to preserve the order of the 
rules, so only certain optimizations are possible, but if we make the assumption 
that no function has side effects (a safe assumption in this application), we can 
still do pretty well. We'll use the simp-fn facility to install the one big function for 
each operator. 

The function compi1 e - rul e- set takes an operator, finds all the rules for that operator, 
and compiles each rule individually. (It uses compi1 e -i ndexed -rule rather than 
compi 1 e -rul e, because it assumes we have already done the indexing for the main operator.) 
After each rule has been compiled, they are combined with combi ne- rul es, 
which merges similar parts of rules and concatenates the different parts. The result 
is wrapped in a 1 ambda expression and compiled as the final simplification function 
for the operator. 


<a id='page-304'></a>

(defun compile-rule-set (op) 
"Compile all rules indexed under a given main op. 
and make them into the simp-fn for that op." 
(set-simp-fn op 

(compile nil 
'(lambda (x) 
.(reduce #'combine-rules 
(mapcar #*compile-indexed-rule 
(rules-for op))))))) 

(defun compile-indexed-rule (rule) . 
"Compile one rule into lambda-less code, 
assuming indexing of main op." 
(let ((*bindings* nil)) 

(compile-args 
'x (exp-lhs rule) 

(delay (build-exp (exp-rhs rule) ^bindings*))))) 

Here are two examples of what compi 1 e - i ndexed - rul e generates: 
> (compile-indexed-rule '(= (log 1) 0)) 
(LET ((XL (EXP-LHS X))) 
(IF (EQL XL .) 
.)) 

> (compile-indexed-rule *(= (log (" e x)) x)) 
(LET ((XL (EXP-LHS X))) 
(IF (OP? XL *n 
(LET ((XLL (EXP-LHS XL)) 
(XLR (EXP-RHS XL))) 
(IF (EQL XLL .) 
XLR)))) 

Thenextstepis to combine several of these rules into one. The function comb i ne- rul es 
takes two rules and merges them together as much as possible. 

(defun combine-rules (a b) 

"Combine the code for two rules into one, maintaining order." 
In the default case, we generate the code (or a b), 
but we try to be cleverer and share common code, 
on the assumption that there are no side-effects, 

(cond ((and distp a) distp b) 
(= (length a) (length b) 3) 
(equal (first a) (first b)) 
(equal (second a) (second b))) 

;; a=(f . y), b=(f . .) => (f . (combine-rules y .)) 
This can apply when f=IF or f=LET 


<a id='page-305'></a>
(list (first a) (second a) 

(combine-rules((matching-ifs a b) 

'(if .(second a) 
.(combine-rules.(combine-rules

((starts-with a Or) 

 (third a) (third b)))) 

 (third a) (third b)) 
(fourth a) (fourth b)))) 

a=(or ... (if . y)). b=(if . .) => 
(or ... (if . (combine-rules y .))) 

else 
a=(or ...) b => (or ... b) 
(if (matching-ifs (lastl a) b) 
(append (butlast a) 
(list (combine-rules(append a (list b)))) 
(t a. b => (or a b) 
'(or .a .b)))) 

(defun matching-ifs (a b) 

 (lastl a) b))) 

"Are a and b if statements with the same predicate?" 
(and (starts-with a 'if) (starts-with b 'if) 
(equal (second a) (second b)))) 

(defun lastl (list) 
"Return the last element (not last cons cell) of list" 
(first (last list))) 

Here is what combi ne- rul es does with the two rules generated above: 

> (combine-rules 
'(let ((xl (exp-lhs x))) (if (eql xl .) .)) 
'(let ((xl (exp-lhs x))) 

(if (op? xl '^) 
(let ((xll (exp-lhs xD) 
(xlr (exp-rhs xl))) 
(if (eql xll 'e) xlr))))) 
(LET ((XL (EXP-LHS X))) 
(OR (IF (EQL XL .) .) 
(IF (OP? XL "^) 
(LET ((XLL (EXP-LHS XL)) 
(XLR (EXP-RHS XL))) 
(IF (EQL XLL .) XLR))))) 

Now we run the compiler by calling compi 1 e-all - rul es-indexed and show the 
combined compiled simplification function for 1 og. The comments were entered by 
hand to show what simplification rules are compiled where. 


<a id='page-306'></a>

(defun compile-all-rules-indexed (rules) 
"Compile a separate fn for each operator, and store it 
as the simp-fn of the operator." 
(index-rules rules) 
(let ((all-ops (delete-duplicates (mapcar #*main-op rules)))) 

(mapc #'compile-rule-set all-ops))) 

> (compile-all-rules-indexed *simplification-rules*) 
(SIN COS LOG ^ * / - + D) 

> (simp-fn 'log) 
(LAMBDA (X) 
(LET ((XL (EXP-LHS X))) 
(OR (IF (EQL XL .) 
.) logl = 0 
(IF (EQL XL .) 
'UNDEFINED) log 0 -undefined 
(IF (EQL XL '.) 
.) loge = l 
(IF (OP? XL '^) 
(LET ((XLL (EXP-LHS XL)) 
(XLR (EXP-RHS XL))) 
(IF (EQL XLL .) 
XLR)))))) lloge"" = X 

If we want to bypass the rule-based simplifier altogether, we can change si mp1 i fy- exp 
once again to eliminate the check for rules: 

(defun simplify-exp (exp) 
"Simplify by doing arithmetic, or by using the simp function 
supplied for this operator. Do not use rules of any kind." 
(cond ((simplify-by-fn exp)) 

((evaluable exp) (eval exp)) 
(t exp))) 

At last, we are in a position to run the benchmark test on the new compiled code; the 
function test -it runs in about .15 seconds with memoization and .05 without. Why 
would memoization, which helped before, now hurt us? Probably because there is a 
lot of overhead in accessing the hash table, and that overhead is only worth it when 
there is a lot of other computation to do. 

We've seen a great improvement since the original code, as the following table 
summarizes. Overall, the various efficiency improvements have resulted in a 130fold 
speed-up—we can do now in a minute what used to take two hours. Of course, 
one must keep in mind that the statistics are only good for this one particular set of 


<a id='page-307'></a>
test data on this one machine. It is an open question what performance you will get 
on other problems and on other machines. 
The following table summarizes the execution time and number of function calls 
on the test data: 

original memo memo+index memo+comp comp 
run time (sees) 6.6 3.0 .98 .15 .05 
speed-up — 2 7 44 130 
calls 
pat-match 51690 20003 5159 0 0 
variable-p 37908 14694 4798 0 0 
match-vari able 1393 551 551 0 0 
simplify 906 408 408 545 906 
simplify-exp 274 118 118 118 274 

9.7 History and References 
The idea of memoization was introduced by Donald Michie 1968. He proposed 
using a list of values rather than a hash table, so the savings was not as great. In 
mathematics, the field of dynamic programming is really just the study of how to 
compute values in the proper order so that partial results will already be cached away 
when needed. 

A large part of academic computer science covers compilation; Aho and Ullman 
1972 is just one example. The technique of compiling embedded languages (such as 
the language of pattern-matching rules) is one that has achieved much more attention 
in the Lisp community than in the rest of computer science. See Emanuelson and 
Haraldsson 1980, for an example. 

Choosing the right data structure, indexing it properly, and defining algorithms 
to operate on it is another important branch of computer science; Sedgewick 1988 is 
one example, but there are many worthy texts. 

Delaying computation by packaging it up in a 1 ambda expression is an idea that 
goes back to Algol's use of thunks—a mechanism to implement call-by-name parameters, 
essentially by passing functions of no arguments. The name thunk comes from 
the fact that these functions can be compiled: the system does not have to think 
about them at run time, because the compiler has already thunk about them. Peter 
Ingerman 1961 describes thunks in detail. Abelson and Sussman 1985 cover delays 
nicely. The idea of eliminating unneeded computation is so attractive that entire languages 
have built around the concept of lazy evaluation—don't evaluate an expression 
until its value is needed. See Hughes 1985 or Field and Harrison 1988. 


<a id='page-308'></a>

9.8 Exercises 
&#9635; Exercise 9.3 [d] In this chapter we presented a compiler for s i mp1 i fy. It is not too 
much harder to extend this compiler to handle the full power of pat-match. Instead 
of looking at expressions only, allow trees with variables in any position. Extend and 
generalize the definitions of compi 1 e -rul e and compi 1 e - rul e-set so that they can 
be used as a general tool for any application program that uses pat-match and/or 
rule-based -trans1 ator. Make sure that the compiler is data-driven, so that the 
programmer who adds a new kind of pattern to pat-match can also instruct the 
compiler how to deal with it. One hard part will be accounting for segment variables. 
It is worth spending a considerable amount of effort at compile time to make this 
efficient at run time. 

&#9635; Exercise 9.4 [m] Define the time to compute (fib n) without memoization as T<sub>n</sub>. 
Write a formula to express T<sub>n</sub>. Given that T<sub>25</sub> &asymp; 1.1 seconds, predict T<sub>100</sub>.

&#9635; Exercise 9.5 [m] Consider a version of the game of Nim played as follows: there is 
a pile of . tokens. Two players alternate removing tokens from the pile; on each turn 
a player must take either one, two, or three tokens. Whoever takes the last token 
wins. Write a program that, given n, returns the number of tokens to take to insure 
a win, if possible. Analyze the execution times for your program, with and without 
memoization. 

&#9635; Exercise 9.6 [m] A more complicated Nim-like game is known as Grundy's game. 
The game starts with a single pile of . tokens. Each player must choose one pile and 
split it into two uneven piles. The first player to be unable to move loses. Write a 
program to play Grundy's game, and see how memoization helps. 

&#9635; Exercise 9.7 [h] This exercise describes a more challenging one-person game. In 
this game the player rolls a six-sided die eight times. The player forms four two-digit 
decimal numbers such that the total of the four numbers is as high as possible, but 
not higher than 170. A total of 171 or more gets scored as zero. 

The game would be deterministic and completely boring if not for the requirement 
that after each roll the player must immediately place the digit in either the ones or 
tens column of one of the four numbers. 

Here is a sample game. The player first rolls a 3 and places it in the ones column 
of the first number, then rolls a 4 and places it in the tens column, and so on. On the 
last roll the player rolls a 6 and ends up with a total of 180. Since this is over the limit 
of 170, the player's final score is 0. 


<a id='page-309'></a>

roll 3 4 6 6 3 5 3 6 

1st num. -3 43 43 43 43 43 43 43 

2nd num. -6 -6 36 36 36 36 

-

-

3rd num. -6 -6 -6 36 36 

-

4th num. -5 -5 65 
total 03 43 49 55 85 90 120 0 

Write a function that allows you to play a game or a series of games. The function 
should take as argument a function representing a strategy for playing the game. 

&#9635; Exercise 9.8 [h] Define a good strategy for the dice game described above. (Hint: 
my strategy scores an average of 143.7.) 

&#9635; Exercise 9.9 [m] One problem with playing games involving random numbers is 
the possibility that a player can cheat by figuring out what random is going to do next. 
Read the definition of the function random and describe how a player could cheat. 
Then describe a countermeasure. 

&#9635; Exercise 9.10 [m] On [page 292](chapter9.md#page-292) we saw the use of the read-time conditionals, and 
# -, where #+ is the read-time equivalent of when, and #- is the read-time equivalent 
of unless. Unfortunately, there is no read-time equivalent of case. Implement one. 

&#9635; Exercise 9.11 [h] Write a compiler for ELIZA that compiles all the rules at once into 
a single function. How much naore efficient is the compiled version? 

&#9635; Exercise 9.12 [d] Write some rules to simplify Lisp code. Some of the algebraic 
simplification rules will still be valid, but new ones will be needed to simplify nonalgebraic 
functions and special forms. (Since ni1 is a valid expression in this domain, 
you will have to deal with the semipredicate problem.) Here are some example rules 
(using prefix notation): 

= (+ . 0) .) 
= 'nil nil) 
= (car (cons . y)) .) 
= (cdr (cons . y)) y) 
= (if t . y) .) 
= (if nil X y) y) 
= (length nil) 0) 
= (expt y (?if X numberp)) (expt (expt y (/ . 2)) 2)) 


<a id='page-310'></a>

&#9635; Exercise 9.13 [m] Consider the following two versions of the sieve of Eratosthenes 
algorithm. The second explicitly binds a local variable. Is this worth it? 

(defun sieve (pipe) 
(make-pipe (head pipe) 
(filter #*(lambda (x)(/= (mod . (headpipe)) 0)) 
(sieve (tail pipe))))) 

(defun sieve (pipe) 
(let ((first-num (head pipe))) 
(make-pipe first-num 
(filter #'(lambda (x) (/= (mod . first-num) 0)) 
(sieve (tail pipe)))))) 

9.9 Answers 
Answer 9.4 Let Fn denote (fib .). Then the time to compute Fn, Tn, is a small 
constant for . < 1, and is roughly equal to Tn-\ plus Tn-i for larger n. Thus, Tn is 
roughly proportional to Fn'. 

T<sub>n</sub> = F<sub>n</sub> T<sub>i</sub> / F<sub>i</sub> 

We could use some small value of Ti to calculate Tioo if we knew Fioo- Fortunately, 
we can use the equation: 

where &phi; = ^J{5))/2 &asymp; 1.618. This equation was derived by de Moivre in 1718 
(see Knuth, Donald E. Fundamental Algorithms, pp. 78-83), but the number . has a 
long interesting history. Euclid called it the "extreme and mean ratio," because the 
ratio of A to . is the ratio of A -h J5 to A if A/JB is .. In the Renaissance it was called 
the "divine proportion," and in the last century it has been known as the "golden 
ratio," because a rectangle with sides in this ratio can be divided into two smaller 
rectangles that both have the same ratio between sides. It is said to be a pleasing 
proportion when employed in paintings and architecture. Putting history aside, 
given T25 &asymp; 1.1sec we can now calculate: 

T<sub>100</sub> &asymp; &phi;<sup>100</sup> 1.1sec/&phi;<sup>25</sup> &asymp; 5 x10<sup>15</sup>sec 

which is roughly 150 million years. We can also see that the timing data in the table 
fits the equation fairly well. However, we would expect some additional time for 
larger numbers because it takes longer to add and garbage collect bignums than 
fixnums. 


<a id='page-311'></a>

Answer 9.5 First we'll define the notion of a forced win. This occurs either when 
there are three or fewer tokens left or when you can make a move that gives your 
opponent a possible loss. A possible loss is any position that is not a forced win. If 
you play perfectly, then a possible loss for your opponent will in fact be a win for you, 
since there are no ties. See the functions wi . and 1 oss below. Now your strategy 
should be to win the game outright if there are three or fewer tokens, or otherwise 
to choose the largest number resulting in a possible loss for your opponent. If there 
is no such move available to you, take only one, on the grounds that your opponent 
is more likely to make a mistake with a larger pile to contend with. This strategy is 
embodied in the function nim below. 

(defun win (n) 
"Is a pile of . tokens a win for the player to move?" 
(or (<= . 3) 

(loss (- . D) 
(loss (- . 2)) 
(loss (- . 3)))) 

(defun loss (n) (not (win n))) 

(defun nim (n) 
"Play Nim: a player must take 1-3; taking the last one wins." 
(cond ((<= . 3) n) ; an immediate win 
(doss (- . 3)) 3) ; an eventual win 
(doss (- . 2)) 2) ; an eventual win 
(doss (- . 1)) 1) ; an eventual win 
(t 1))) ; a loss; the 1 is arbitrary 

(memoize doss) 

From this we are able to produce a table of execution times (in seconds), with and 
without memoization. Only 1 oss need be memoized. (Why?) Do you have a good 
explanation of the times for the unmemoized version? What happens if you change 
the order of the loss clauses in wi . and/or . i m? 

Answer 9.6 We start by defining a function, moves, which generates all possible 
moves from a given position. This is done by considering each pile of . tokens within 
a set of piles s. Any pile bigger than two tokens can be split. We take care to eliminate 
duplicate positions by sorting each set of piles, and then removing the duplicates. 

(defun moves (s) 
"Return a list of all possible moves in Grundy's game" 
;; S is a list of integers giving the sizes of the piles 
(remove-duplicates 

(loop for . in s append (make-moves . s)) 

:test #'equal)) 


<a id='page-312'></a>

(defun make-moves (n s) 
(when (>= . 2) 
(let ((s/n (remove . s icount 1))) 
(loop for i from 1 to (- (ceiling . 2) 1) 
collect (sort* (list* i (-ni) s/n) 
#'>)))) 

(defun sort* (seq pred &key key) 
"Sort without altering the sequence" 
(sort (copy-seq seq) pred :key key)) 

This time a loss is defined as a position from which you have no moves, or one from 
which your opponent can force a win no matter what you do. A winning position 
is one that is not a loss, and the strategy is to pick a move that is a loss for your 
opponent, or if you can't, just to play anything (here we arbitrarily pick the first move 
generated). 

(defun loss (s) 
(let ((choices (moves s))) 
(or (null choices) 
(every #'win choices)))) 

(defun win (s) (not (loss s))) 

(defun grundy (s) 
(let ((choices (moves s))) 
(or (find-if #'loss choices) 
(first choices)))) 

Answer 9.7 The answer assumes that a strategy function takes four arguments: 
the current die roll, the score so far, the number of remaining positions in the tens 
column, and the number of remaining positions in the ones column. The strategy 
function should return 1 or 10. 

(defun play-games (&optional (n-games 10) (player 'make-move)) 
"A driver for a simple dice game. In this game the player 
rolls a six-sided die eight times. The player forms four 
two-digit decimal numbers such that the total of the four 
numbers is as high as possible, but not higher than 170. 
A total of 171 or more gets scored as zero. After each die 
is rolled, the player must decide where to put it. 
This function returns the player's average score." 
(/ (loop repeat n-games summing (play-game player 0 4 4)) 

(float n-games))) 


<a id='page-313'></a>

(defun play-game (player &optional (total 0) (tens 4) (ones 4)) 

(cond ((or (> total 170) <tens 0) (< ones 0)) 0) 
((and (= tens 0) (= ones 0)) total) 
(t (let ((die (roll-die))) 

(case (funcall player die total tens ones) 
(1 (play-game player {+ total die) 
tens (- ones 1))) 
(10 (play-game player (+ total (* 10 die)) 
(- tens 1) ones)) 

(t 0)))))) 

(defun roll-die () (+ 1 (random 6))) 

So, the expression (play-games 5 #'make-move) would play five games with a 
strategy called make-move. This returns only the average score of the games; if you 
want to see each move as it is played, use this function: 

(defun show (player) 
"Return a player that prints out each move it makes." 
#'(lambda (die total tens ones) 

(when (= total 0) (fresh-line)) 

(let ((move (funcall player die total tens ones))) 
(incf total (* die move)) 
(format t "~2d->~3d I ~@[*~]" (* move die) total (> total 170)) 
move))) 

and call (pi ay-games 5 (show #'make-moves)). 

Answer 9.9 The expression (random 6 (make-random-state)) returns the next 
number that rol 1 -di e will return. To guard against this, we can make rol 1 -di e use 
a random state that is not accessible through a global variable: 

(let ((state (make-random-state t))) 
(defun roll-die () (+ 1 (random 6 state)))) 

Answer 9.10 Because this has to do with read-time evaluation, it must be implemented 
as a macro or read macro. Here's one way to do it: 

(defmacro read-time-case (first-case &rest other-cases) 
"Do the first case, where normally cases are 
specified with #+ or possibly #- marks." 
(declare (ignore other-cases)) 
first-case) 


<a id='page-314'></a>

A fanciful example, resurrecting a number of obsolete Lisps, follows: 

(defun get-fast-time 0 

(read-time-case 
#+Explorer (ti me:mi crosecond-ti me) 
#+Franz (sysitime) 
#+(or PSL UCI) (time) 

#+YKT (currenttime) 
#+MTS (status 39) 
#+Interlisp (clock 1) 
#+Lispl.5 (tempus-fugit) 
otherwise 
(get-internal-real-time)) ) 

Answer 9.13 Yes. Computing (head pipe) may be a trivial computation, but it 
will be done many times. Binding the local variable makes sure that it is only done 
once. In general, things that you expect to be done multiple times should be moved 
out of delayed functions, while things that may not be done at all should be moved 
inside a delay. 


## Chapter 10
<a id='page-315'></a>

### Low-Level Efficiency Issues 

> There are only two qualities in the world: efficiency 
and inefficiency; and only two sorts of people: the 
efficient and the inefficient 
>
> —George Bernard Shaw, 
> *John Bull's Other Island* (1904) 

The efficiency techniques of the previous chapter all involved fairly significant changes 
to an algorithm. But what happens when you already are using the best imaginable 
algorithms, and performance is still a problem? One answer is to find what parts of the 
program are used most frequently and make micro-optimizations to those parts. This chapter 
covers the following six optimization techniques. If your programs all run quickly enough, then 
feel free to skip this chapter. But if you would like your programs to run faster, the techniques 
described here can lead to speed-ups of 40 times or more. 


<a id='page-316'></a>

* Use declarations. 
* Avoid generic functions. 
* Avoid complex argument lists. 
* Provide compiler macros. 
* Avoid unnecessary consing. 
* Use the right data structure. 

### 10.1 Use Declarations 
On general-purpose computers running Lisp, much time is spent on type-checking. 
You can gain efficiency at the cost of robustness by declaring, or promising, that 
certain variables will always be of a given type. For example, consider the following 
function to compute the sum of the squares of a sequence of numbers: 

```lisp
(defun sum-squares (seq) 
  (let ((sum 0)) 
    (dotimes (i (length seq)) 
      (incf sum (square (elt seq i)))) 
    sum)) 

(defun square (x) (* . x)) 
```

If this function will only be used to sum vectors of fixnums, we can make it a lot faster 
by adding declarations: 

```lisp
(defun sum-squares (vect) 
  (declare (type (simple-array fixnum *) vect) 
           (inline square) (optimize speed (safety 0))) 

  (let ((sum 0)) 
    (declare (fixnum sum)) 
    (dotimes (i (length vect)) 
      (declare (fixnum i)) 
    (incf sum (the fixnum (square (svref vect i))))))) 
  sum)) 
```

The fixnum declarations let the compiler use integer arithmetic directly, rather than 
checking the type of each addend. The `(the fixnum ... )` special form is a promise 
that the argument is a fixnum. The `(optimize speed (safety 0))` declaration tells 
the compiler to make the function run as fast as possible, at the possible expense of 
<a id='page-317'></a>
making the code less safe (by ignoring type checks and so on). Other quantities that 
can be optimized are `compilation-speed`, `space` and in ANSI Common Lisp only, 
`debug` (ease of debugging). Quantities can be given a number from 0 to 3 indicating 
how important they are; 3 is most important and is the default if the number is left out. 

The `(inline square)` declaration allows the compiler to generate the multiplication 
specified by `square` right in the loop, without explicitly making a function 
call to square. The compiler will create a local variable for `(svref vect i)` and will 
not execute the reference twice—inline functions do not have any of the problems 
associated with macros as discussed on [page 853](chapter24.md#page-853). However, there is one drawback: 
when you redefine an inline function, you may need to recompile all the functions 
that call it. 

You should declare a function `inline` when it is short and the function-calling 
overhead will thus be a significant part of the total execution time. You should not 
declare a function `inline` when the function is recursive, when its definition is likely 
to change, or when the function's definition is long and it is called from many places. 

In the example at hand, declaring the function `inline` saves the overhead of 
a function call. In some cases, further optimizations are possible. Consider the 
predicate `starts-with`: 

```lisp
(defun starts-with (list x) 
  "Is this a list whose first element is x?" 
  (and (consp list) (eql (first list) x))) 
```

Suppose we have a code fragment like the following: 

```lisp
(if (consp list) (starts-with list x) ...) 
```

If `starts-with` is declared `inline` this will expand to: 

```lisp
(if (consp list) (and (consp list) (eql (first list) x)) ...) 
```

which many compilers will simplify to: 

```lisp
(if (consp list) (eql (first list) x) ...) 
```

Very few compilers do this kind of simplification across functions without the hint 
provided by `inline`. 

Besides eliminating run-time type checks, declarations also allow the compiler 
to choose the most efficient representation of data objects. Many compilers support 
both *boxed* and *unboxed* representations of data objects. A boxed representation 
includes enough information to determine the type of the object. An unboxed 
representation is just the "raw bits" that the computer can deal with directly. Consider 
<a id='page-318'></a>
the following function, which is used to clear a 1024x1024 array of floating point 
numbers, setting each one to zero: 

```lisp
(defun clear-m-array (array) 
  (declare (optimize (speed 3) (safety 0))) 
  (declare (type (simple-array single-float (1024 1024)) array)) 
  (dotimes (i 1024) 
    (dotimes (j 1024) 
      (setf (aref array i j) 0.0)))) 
```

In Allegro Common Lisp on a Sun SPARCstation, this compiles into quite good code, 
comparable to that produced by the C compiler for an equivalent C program. If the 
declarations are omitted, however, the performance is about 40 times worse. 

The problem is that without the declarations, it is not safe to store the raw floating 
point representation of `0.0` in each location of the array. Instead, the program 
has to box the `0.0`, allocating storage for a typed pointer to the raw bits. This 
is done inside the nested loops, so the result is that each call to the version of 
`clear-m-array` without declarations calls the floating-point-boxing function 1048567 
times, allocating a megaword of storage. Needless to say, this is to be avoided. 

Not all compilers heed all declarations; you should check before wasting time 
with declarations your compiler may ignore. The function `disassemble` can be used 
to show what a function compiles into. For example, consider the trivial function to 
add two numbers together. Here it is with and without declarations: 

```lisp
(defun f (x y) 
  (declare (fixnum . y) (optimize (safety 0) (speed 3))) 
  (the fixnum (+ . y))) 
(defun g (x y) (+ . y)) 
```

Here is the disassembled code for f from Allegro Common Lisp for a Motorola 
68000-series processor: 

```
> (disassemble 'f) 
;; disassembling #<Function f @ #x83ef79> 
;; formals: x y 
;; code vector @ #x83ef44 
0:      link    a6.#0 
4:      move.l  a2.-(a7) 
6:      move.l  a5,-(a7) 
8:      move.l  7(a2),a5 
12:     move.l  8(a6),d4 y 
16:     add.l   12(a6),d4 ; X 
20:     move.l  #l,d1 
```
<a id='page-319'></a>
```
22:     move.l  -8(a6),a5 
26:     unlk    a6 
28:     rtd     #8 
```

This may look intimidating at first glance, but you don't have to be an expert at 68000 
assembler to gain some appreciation of what is going on here. The instructions 
labeled 0-8 (labels are in the leftmost column) comprise the typical function preamble 
for the 68000. They do subroutine linkage and store the new function object and 
constant vector into registers. Since `f` uses no constants, instructions 6, 8, and 22 
are really unnecessary and could be omitted. Instructions 0,4, and 26 could also be 
omitted if you don't care about seeing this function in a stack trace during debugging. 
More recent versions of the compiler will omit these instructions. 

The heart of function `f` is the two-instruction sequence 12-16. Instruction 12 
retrieves `y`, and 16 adds `y` to `x`, leaving the result in `d4`, which is the "result" register. 
Instruction 20 sets `d1`, the "number of values returned" register, to 1. 

Contrast this to the code for `g`, which has no declarations and is compiled at 
default speed and safety settings: 

```
> (disassemble 'g) 
;; disassembling #<Function g @ #x83dbd1> 
;; formals: x y 
;; code vector @ #x83db64 
0:      add.l   #8.31(a2) 
4:      sub.w   #2,dl 
6:      beq.s   12 
8:      jmp     16(a4) ; wnaerr 
12:     link    a6.#0 
16:     move.l  a2,-(a7) 
18:     move.l  a5,-(a7) 
20:     move.l  7(a2),a5 
24:     tst.b   -208(a4) ; signal-hit 
28:     beq.s   34 
30:     jsr     872(a4) ; process-sig 
34:     move.l  8(a6),d4 ; y 
38:     move.l  12(a6),d0 ; X 
42:     or.l    d4,d0 
44:     and.b   #7.d0 
48:     bne.s   62 
50:     add.l   12(a6),d4 ; X 
54:     bvc.s   76 
56:     jsr     696(a4) ; add-overflow 
60:     bra.s   76 
62:     move.l  12(a6),-(a7) ; . 
66:     move.l  d4.-(a7) 
68:     move.l  #2.dl 
```
<a id='page-320'></a>

```
70:     move.l  -304(a4),a 0 ; +-2op 
74:     jsr     (a4) 
76:     move.l  #1,d1 
78:     move.l  -8(a6),a5 
82:     unlk    a6 
84:     rtd     #8 
```

See how much more work is done. The first four instructions ensure that the right 
number of arguments have been passed to `g`. If not, there is a jump to `wnaerr` (wrong-number-
of-arguments-error). Instructions 12-20 have the argument loading code 
that was at 0-8 in `f`. At 24-30 there is a check for asynchronous signals, such as the 
user hitting the abort key. After `x` and `y` are loaded, there is a type check (42-48). If 
the arguments are not both fixnums, then the code at instructions 62-74 sets up a 
call to `+_2op`, which handles type coercion and non-fixnum addition. If all goes well, 
we don't have to call this routine, and do the addition at instruction 50 instead. But 
even then we are not done—just because the two arguments were fixnums does not 
mean the result will be. Instructions 54-56 check and branch to an overflow routine 
if needed. Finally, instructions 76-84 return the final value, just as in `f`. 

Some low-quality compilers ignore declarations altogether. Other compilers 
don't need certain declarations, because they can rely on special instructions in the 
underlying architecture. On a Lisp Machine, both `f` and `g` compile into the same 
code: 

```
6 PUSH      ARG|0     ; X 
7 +         ARG|1     ; Y 
8 RETURN    PDL-POP 
```

The Lisp Machine has a microcoded `+` instruction that simultaneously does a fixnum 
add and checks for non-fixnum arguments, branching to a subroutine if either argument 
is not a fixnum. The hardware does the work that the compiler has to do on a 
conventional processor. This makes the Lisp Machine compiler simpler, so compiling 
a function is faster. However, on modern pipelined computers with instruction 
caches, there is little or no advantage to microcoding. The current trend is away from 
microcode toward reduced instruction set computers (RISC). 

On most computers, the following declarations are most likely to be helpful: 

* `fixnum` and `float`. Numbers declared as fixnums or floating-point numbers 
can be handled directly by the host computer's arithmetic instructions. On 
some systems, `float` by itself is not enough; you have to say `single-float` 
or `double-float`. Other numeric declarations will probably be ignored. For 
example, declaring a variable as `integer` does not help the compiler much, 
because bignums are integers. The code to add bignums is too complex to put 
<a id='page-321'></a>
inline, so the compiler will branch to a general-purpose routine (like `+_2op` in 
Allegro), the same routine it would use if no declarations were given. 

* `list` and `array`. Many Lisp systems provide separate functions for the list- and 
array- versions of commonly used sequence functions. For example, `(delete 
X (the list l))` compiles into `(sys: delete-list-eql x 1)` on a TI Explorer 
Lisp Machine. Another function, `sys:delete-vector`, is used for arrays, and 
the generic function `delete` is used only when the compiler can't tell what type 
the sequence is. So if you know that the argument to a generic function is either 
a `list` or an `array`, then declare it as such. 
* `simple-vector` and `simple-array`. Simple vectors and arrays are those that 
do not share structure with other arrays, do not have fill pointers, and are 
not adjustable. In many implementations it is faster to `aref` a `simple-vector` 
than a `vector`. It is certainly much faster than taking an `elt` of a sequence of 
unknown type. Declare your arrays to be simple (if they in fact are). 
* `(array` *type*`)`. It is often important to specialize the type of array elements. For 
example, an `(array short-float)` may take only half the storage of a general 
array, and such a declaration will usually allow computations to be done using 
the CPU's native floating-point instructions, rather than converting into and 
out of Common Lisp's representation of floating points. This is very important 
because the conversion normally requires allocating storage, but the direct 
computation does not. The specifiers `(simple-array type)` and `(vector type)` 
should be used instead of `(array type)` when appropriate. A very common 
mistake is to declare `(simple-vector type)`. This is an error because Common 
Lisp expects `(simple-vector size)`—don't ask me why. 
* `(array * dimensions)`. The full form of an `array` or `simple-array` type specifier 
is `(array type dimensions)`. So, for example, `(array bit (* *))` is a two-
dimensional bit array, and `(array bit (1024 1024))` is a 1024 &times; 1024 bit array. 
It is very important to specify the number of dimensions when known, and less 
important to specify the exact size, although with multidimensional arrays, 
declaring the size is more important. The format for a `vector` type specifier is 
`(vector type size)`. 
Note that several of these declarations can apply all at once. For example, in 

```lisp
(position #\. (the simple-string file-name)) 
```

the variable `filename` has been declared to be a vector, a simple array, and a sequence 
of type `string-char`. All three of these declarations are helpful. The type 
`simple-string` is an abbreviation for `(simple-array string-char)`. 


<a id='page-322'></a>

This guide applies to most Common Lisp systems, but you should look in the 
implementation notes for your particular system for more advice on how to fine-tune 
your code. 

### 10.2 Avoid Generic Functions 
Common Lisp provides functions with great generality, but someone must pay the 
price for this generality. For example, if you write `(elt x 0)`, different machine 
instruction will be executed depending on if `x` is a list, string, or vector. Without 
declarations, checks will have to be done at runtime. You can either provide declarations, 
as in `(elt (the list x) O)`, or use a more specific function, such as `(first x)` 
in the case of lists, `(char x 0)` for strings, `(aref x 0)` for vectors, and `(svref x 0)` 
for simple vectors. Of course, generic functions are useful—I wrote `random-elt` 
as shown following to work on lists, when I could have written the more efficient 
`random-mem` instead. The choice paid off when I wanted a function to choose a random 
character from a string—`random-elt` does the job unchanged, while `random-mem` 
does not. 
``` lisp
(defun random-elt (s) (elt s (random (length s)))) 
(defun random-mem (1) (nth (random (length (the list 1))) 1)) 
```

This example was simple, but in more complicated cases you can make your sequence 
functions more efficient by having them explicitly check if their arguments are lists 
or vectors. See the definition of `map-into` on [page 857](chapter24.md#page-857). 

### 10.3 Avoid Complex Argument Lists 
Functions with keyword arguments suffer a large degree of overhead. This may also 
be true for optional and rest arguments, although usually to a lesser degree. Let's 
look at some simple examples: 

```lisp
(defun reg (a b c d) (list a b c d)) 
(defun rst (a b c &rest d) (list* a b c d)) 
(defun opt (&optional a b (c 1) (d (sqrt a))) (list a b c d)) 
(defun key (&key a b (c 1) (d (sqrt a))) (list a b c d)) 
```

We can see what these compile into for the TI Explorer, but remember that your 
compiler may be quite different. 


<a id='page-323'></a>

```
> (disassemble 'reg) 
   8 PUSH           ARG|0    ; A 
   9 PUSH           ARG|I    ; B 
  10 PUSH           ARG|2    ; C 
  11 PUSH           ARG|3    ; D 
  12 TAIL-REC CALL-4  FEF|3  ; #'LIST 

> (disassemble 'rst) 
   8 PUSH           ARG|0    ; A 
   9 PUSH           ARG|1    ; B 
  10 PUSH           ARG|2    ; C 
  11 PUSH           LOCAL|0  ; D 
  12 RETURN CALL-4  FEF|3    ; #'LIST* 
```

With the regular argument list, we just push the four variables on the argument stack 
and branch to the list function. (Chapter 22 explains why a tail-recursive call is just 
a branch statement.) 

With a rest argument, things are almost as easy. It turns out that on this machine, 
the microcode for the calling sequence automatically handles rest arguments, storing 
them in local variable 0. Let's compare with optional arguments: 

```lisp
(defun opt (&optional a b (c 1) (d (sqrt a))) (list a b c d)) 

> (disassemble 'opt) 
24 DISPATCH       FEF|5     ; [0=>25;1=>25;2=>25;3=>27;ELSE=>30]
25 PUSH-NUMBER    1 
26 POP            ARG|2     ; C 
27 PUSH           ARG|0     ; A 
28 PUSH CALL-1    FEF|3     ; #'SQRT 
29 POP            ARG|3     ; D 
30 PUSH           ARG|0     ; A 
31 PUSH           ARG|1     ; B 
32 PUSH           ARG|2     ; C 
33 PUSH           ARG|3     ; D 
34 TAIL-REC CALL-4 FEFI4    ; #'LIST 
```

Although this assembly language may be harder to read, it turns out that optional 
arguments are handled very efficiently. The calling sequence stores the number of 
optional arguments on top of the stack, and the `DISPATCH` instruction uses this to 
index into a table stored at location `FEF|5` (an offset five words from the start of 
the function). The result is that in one instruction the function branches to just the 
right place to initialize any unspecified arguments. Thus, a function with optional 
arguments that are all supplied takes only one more instruction (the dispatch) than 
the "regular" case. Unfortunately, keyword arguments don't fare as well: 

```lisp
(defun key (&key a b (c 1) (d (sqrt a))) (list a b c d)) 
```


<a id='page-324'></a>

```lisp
> (disassemble 'key) 
14 PUSH-NUMBER      1 
15 POP              LOCAL|3   ; C 
16 PUSH             FEF|3     ; SYS::KEYWORD-GARBAGE 
17 POP              LOCAL|4 
18 TEST             LOCAL|0 
19 BR-NULL      24 
20 PUSH             FEF|4     ; '(:A :B :C :D) 
21 SET-NIL          PDL-PUSH 
22 PUSH-LOC         LOCAL|1   ; A
23 (AUX) %STORE-KEY-WORD-ARGS 
24 PUSH             LOCAL|1   ; A 
25 PUSH             LOCAL|2   ; B 
26 PUSH             LOCAL|3   ; C 
27 PUSH             LOCAL|4 
28 EQ               FEF|3     ; SYS::KEYWORD-GARBAGE 
29 BR-NULL      33 
30 PUSH             LOCAL|1   ; A 
31 PUSH CALL-1      FEF|5     ; #'SQRT 
32 RETURN CALL-4    FEF|6     ; #'LIST 
33 PUSH             LOCAL|4 
34 RETURN CALL-4    FEF|6     ; #'LIST 
```

It is not important to be able to read all this assembly language. The point is that there 
is considerable overhead, even though this architecture has a specific instruction 
`(%STORE-KEY-WORD-ARGS)` to help deal with keyword arguments. 

Now let's look at the results on another system, the Allegro compiler for the 
68000. First, here's the assembly code for `reg`, to give you an idea of the minimal 
calling sequence:[1](#fn-10-1)

```
> (disassemble 'reg) 
;; disassembling #<Function reg @ #x83db59> 
;; formals: a b c d 
;; code vector @ #x83db1c 
0:      link    a6,#0 
4:      move.l  a2.-(a7) 
6:      move.l  a5.-(a7) 
8:      move.l  7(a2),a5 
12:     move.l  20(a6),-(a7)    ; a 
16:     move.l  16(a6).-(a7)    ; b 
20:     move.l  12(a6),-(a7)    ; c 
24:     move.l  8(a6).-(a7)     ; d 
28:     move.l  #4.dl 
30:     jsr     848(a4)         ; list 
```

[fn-10-1] These are all done with safety 0 and speed 3. 

<a id='page-325'></a>

```
34:     move.l  -8(a6).a5 
38:     unlk    a6 
40:     rtd     #10 
```

Now we see that &rest arguments take a lot more code in this system: 

### WIP

> (disassemble 'rst) 
;; disassembling #<Function rst @ #x83de89> 
;; formals: a D c &rest d 
11 code vector @ #x83de34 
0: sub.w #3,dl 
2: bge.s 8 

4: jmp 16(a4) ; wnaerr 
8: move.l (a7)+.al 
10 move.l d3.-(a7) ; nil 
12 sub.w #l,dl 
14 blt.s 38 
16 move.l al,-52(a4) ; c-protected-retaddr 
20 jsr 40(a4) ; cons 
24 move.l d4,-(a7) 
26 dbra dl,20 
30 move.l -52(a4).al ; C-protected-retaddr 
34 clr.l -52(a4) ; C-protected-retaddr 
38 move.l al.-(a7) 
40 link a6.#0 
44 move.l a2.-(a7) 
46 move.l a5,-(a7) 
48 move.l 7(a2).a5 
52 move.l -332(a4),a0 ; list* 
56 move.l -8(a6),a5 
60 unlk a6 
62 move.l #4,dl 
64 jmp (a4) 
The loop from 20-26 builds up the &rest list one cons at a time. Part of the difficulty 
is that cons could initiate a garbage collection at any time, so the list has to be built 
in a place that the garbage collector will know about. The function with optional 
arguments is even worse, taking 34 instructions (104 bytes), and keywords are worst 
of all, weighing in at 71 instructions (178 bytes), and including a loop. The overhead 
for optional arguments is proportional to the number of optional arguments, while 
for keywords it is proportional to the product of the number of parameters allowed 
and the number of arguments actually supplied. 

A good guideline to follow is to use keyword arguments primarily as an interface 
to infrequently used functions, and to provide versions of these functions without 
keywords that can be used in places where efficiency is important. Consider: 


<a id='page-326'></a>

(proclaim '(inline key)) 
(defun key (&key a b (c 1) (d (sqrt a))) (*no-key abed)) 
(defun *no-key (abed) (list abed)) 

Here the function key is used as an interface to the function no - key, which does the 
real work. The inline proclamation should allow the compiler to compile a call to key 
as a call to no - key with the appropriate arguments: 

> (disassemble #'(lambda (x y) (key :b . :a y))) 

10 PUSH ARG II Y 
11 PUSH ARG 10 X 
12 PUSH-NUMBER 1 
13 PUSH ARG II Y 
14 PUSH CALL-1 FEFI3 #'SQRT 
15 TAIL-REC CALL-4 FEFI4 #*NO-KEY 

The overhead only comes into play when the keywords are not known at compile 
time. In the following example, the compiler is forced to call key, not no- key, because 
it doesn't know what the keyword k will be at run time: 

> (disassemble #*(lambda (k . y) (key k . :a y))) 
10 PUSH ARGIO . 
11 PUSH ARG 11 . 
12 PUSH FEFI3 ':. 
13 PUSH ARG 12 . 
14 TAIL-REC CALL-4 FEFI4 #... 

Of course, in this simple example I could have replaced no-key with 1 i st, but in 
general there will be some more complex processing. If I had proclaimed no-key 
inline as well, then I would get the following: 

> (disassemble #'(lambda (x y) (key :b . :a y))) 

10 PUSH ARG 11 ; Y 

11 PUSH ARG 10 ; . 

12 PUSH-NUMBER 1 

13 PUSH ARG II ; Y 

14 PUSH CALL-1 FEFI3 ; #'SQRT 

15 TAIL-REC CALL-4 FEFI4 ; #'LIST 

If you like, you can define a macro to automatically define the interface to the keyword-
less function: 


<a id='page-327'></a>

(defmacro defun* (fn-name arg-list &rest body) 
"Define two functions, one an interface to a &keyword-less 
version. Proclaim the interface function inline." 
(if (and (member '&key arg-list) 

(not (member *&rest arg-list))) 
(let ((no-key-fn-name (symbol fn-name '*no-key)) 
(args (mapcar #'first-or-self 

(set-difference 
arg-list 
1ambda-list-keywords)))) 

'(progn 
(proclaim '(inline ,fn-name)) 
(defun ,no-key-fn-name ,args 

..body) 
(defun ,fn-name ,arg-list 
(,no-key-fn-name ..args)))) 
'(defun ,fn-name ,arg-list 
..body))) 

> (macroexpand '(defun* key (&key a b (c 1) (d (sqrt a))) 
(list a b c d))) 

(PROGN (PROCLAIM '(INLINE KEY)) 
(DEFUN KEY*NO-KEY (A . C D) (LIST A . C D)) 
(DEFUN KEY (&KEY A . (C 1) (D (SQRT A))) 

(KEY*NO-KEY A . C D))) 

> (macroexpand '(defun* reg (abed) (list abed))) 
(DEFUN REG (A . C D) (LIST A . C D)) 

There is one disadvantage to this approach: a user who wants to declare key inHne 
or not inline does not get the expected result. The user has to know that key is 
implemented with key*no- key, and declare key*no- key inline. 

An alternative is just to proclaim the function that uses &key to be inline. Rob 
MacLachlan provides an example. In CMU Lisp, the function member has the following 
definition, which is proclaimed inline: 

(defun member (item list &key (key #'identity) 
(test #'eql testp)(test-not nil notp)) 
(do ((list list (cdr list))) 
((null list) nil) 
(let ((car (car list))) 
(if (cond 
(testp 
(funcall test item 
(funcall key car))) 
(notp 
(not 


<a id='page-328'></a>

(funcall test-not item 
(funcall key car)))) 
(t 
(funcall test item 
(funcall key car)))) 
(return list))))) 

A call like (member ch 1 :key #'first-letter rtest #'cha r=) expands into the 
equivalent of the following code. Unfortunately, not all compilers are this clever with 
inline declarations. 

(do ((list list (cdr list))) 
((null list) nil) 
(let ((car (car list))) 
(if (char= ch (first-letter car)) 
(return list)))) 

This chapter is concerned with efficiency and so has taken a stand against the use 
of keyword parameters in frequently used functions. But when maintainability 
is considered, keyword parameters look much better. When a program is being 
developed, and it is not clear if a function will eventually need additional arguments, 
keyword parameters may be the best choice. 

10.4 Avoid Unnecessary Consing 
The cons function may appear to execute quite quickly, but like all functions that 
allocate new storage, it has a hidden cost. When large amounts of storage are 
used, eventually the system must spend time garbage collecting. We have not 
mentioned it earlier, but there are actually two relevant measures of the amount of 
space consumed by a program: the amount of storage allocated, and the amount of 
storage retained. The difference is storage that is used temporarily but eventually 
freed. Lisp guarantees that unused space will eventually be reclaimed by the garbage 
collector. This happens automatically—the programmer need not and indeed can not 
explicitly free storage. The problem is that the efficiency of garbage collection can 
vary widely. Garbage collection is particularly worrisome for real-time systems, 
because it can happen at any time. 

The antidote to garbage woes is to avoid unnecessary copying of objects in often-
used code. Try using destructive operations, like nreverse, delete, and nconc, 
rather than their nondestructive counterparts, (like reverse, remove, and append) 
whenever it is safe to do so. Or use vectors instead of lists, and reuse values rather 
than creating copies. As usual, this gain in efficiency may lead to errors that can 


<a id='page-329'></a>
be difficult to debug. However, the most common kind of unnecessary copying 
can be eliminated by simple reorganization of your code. Consider the following 
version of f 1 a tten, which returns a list of all the atoms in its input, preserving order. 
Unlike the version in chapter 5, this version returns a single list of atoms, with no 
embedded lists. 

(defun flatten (input) 
"Return a flat list of the atoms in the input. 
Ex: (flatten '((a) (b (c) d))) => (a b c d)." 
(cond ((null input) nil) 

((atom input) (list input)) 
(t (append (flatten (first input)) 
(flatten (rest input)))))) 

This definition is quite simple, and it is easy to see that it is correct. However, each 
call to append requires copying the first argument, so this version can cons O(n^) cells 
on an input with . atoms. The problem with this approach is that it computes the 
list of atoms in the first and rest of each subcomponent of the input. But the first 
sublist by itself is not part of the final answer—that's why we have to call append. We 
could avoid generating garbage by replacing append with nconc, but even then we 
would still be wasting time, because nconc would have to scan through each sublist 
to find its end. 

The version below makes use of an accumulator to keep track of the atoms that 
have been collected in the rest, and to add the atoms in the first one at a time with 
cons, rather than building up unnecessary sublists and appending them. This way 
no garbage is generated, and no subcomponent is traversed more than once. 

(defun flatten (input &optional accumulator) 
"Return a flat list of the atoms in the input. 
Ex: (flatten '((a) (b (c) d))) => (a b c d)." 
(cond ((null input) accumulator) 

((atom input) (cons input accumulator)) 

(t (flatten (first input) 
(flatten (rest input) accumulator))))) 

The version with the accumulator may be a little harder to understand, but it is far 
more efficient than the original version. Experienced Lisp programmers become 
quite skilled at replacing calls to append with accumulators. 

Some of the early Lisp machines had unreliable garbage-collection, so users 
just turned garbage collection off, used the machine for a few days, and rebooted 
when they ran out of space. With a large virtual memory system this is a feasible 
approach, because virtual memory is a cheap resource. The problem is that real 
memory is still an expensive resource. When each page contains mostly garbage 


<a id='page-330'></a>

and only a little live data, the system will spend a lot of time paging data in and out. 
Compacting garbage-collection algorithms can relocate live data, packing it into a 
minimum number of pages. 

Some garbage-collection algorithms have been optimized to deal particularly well 
with just this case. If your system has an ephemeral or generational garbage collector, 
you need not be so concerned with short-lived objects. Instead, it will be the medium-
aged objects that cause problems. The other problem with such systems arises when 
an object in an old generation is changed to point to an object in a newer generation. 
This is to be avoided, and it may be that reverse is actually faster than nreverse in 
such cases. To decide what works best on your particular system, design some test 
cases and time them. 

As an example of efficient use of storage, here is a version of pat-match that 
eliminates (almost) all consing. The original version of pat-match, as used in ELIZA 
([page 180](chapter6.md#page-180)), used an association list of variable/value pairs to represent the binding 
list. This version uses two sequences: a sequence of variables and a sequence of 
values. The sequences are implemented as vectors instead of lists. In general, vectors 
take half as much space as lists to store the same information, since half of every list 
is just pointing to the next element. 

In this case, the savings are much more substantial than just half. Instead of 
building up small binding lists for each partial match and adding to them when the 
match is extended, we will allocate a sufficiently large vector of variables and values 
just once, and use them over and over for each partial match, and even for each 
invocation of pat-match. To do this, we need to know how many variables we are 
currently using. We could initialize a counter variable to zero and increment it each 
time we found a new variable in the pattern. The only difficulty would be when the 
counter variable exceeds the size of the vector. We could just give up and print an 
error message, but there are more user-friendly alternatives. For example, we could 
allocate a larger vector for the variables, copy over the existing ones, and then add in 
the new one. 

It turns out that Common Lisp has a built-in facility to do just this. When a 
vector is created, it can be given a fill pointer. This is a counter variable, but one that 
is conceptually stored inside the vector. Vectors with fill pointers act like a cross 
between a vector and a stack. You can push new elements onto the stack with the 
functions vector -push or vector-push-extend. The latter will automatically allocate 
a larger vector and copy over elements if necessary. You can remove elements with 
vector - pop, or you can explicitly look at the fill pointer with f .1 - poi . te r, or change 
it with a setf. Here are some examples (with *print-array* set to t so we can see 
the results): 

> (setf a (make-array 5 :fiH-pointer 0)) ^ #() 

> (vector-push 1 a) 0 


<a id='page-331'></a>
> (vector-push 2 a) =.> 1 

> a => #(1 2) 

> (vector-pop a) => 2 

> a #(1) 

> (dotimes (i 10) (vector-push-extend 'x a)) NIL 

>a=:^#(lXXXXXXXXXX) 

> (fill-pointer a) => 11 

> (setf (fill-pointer a) 1) 1 

> a => #(1) 

> (find *x a) => NIL NIL ; FIND can't find past the fill pointer 

> (aref a 2) => X ;But AREF can see beyond the fill pointer 

Using vectors with fill pointers in pat-match, the total storage for binding lists is 
just twice the number of variables in the largest pattern. I have arbitrarily picked 
10 as the maximum number of variables, but even this is not a hard limit, because 
vector -push-extend can increase it. In any case, the total storage is small, fixed 
in size, and amortized over all calls to pat-match. These are just the features that 
indicate a responsible use of storage. 

However, there is a grave danger with this approach: the value returned must 
be managed carefully. The new pat-match returns the value of success when it 
matches, success is bound to a cons of the variable and value vectors. These can be 
freely manipulated by the calling routine, but only up until the next call to pa t - ma tch. 
At that time, the contents of the two vectors can change. Therefore, if any calling 
function needs to hang on to the returned value after another call to pat-match, it 
should make a copy of the returned value. So it is not quite right to say that this 
version of pat-match eliminates all consing. It will cons when vector-push-extend 
runs out of space, or when the user needs to make a copy of a returned value. 

Here is the new definition of pat-match. It is implemented by closing the defi


nition of pat-match and its two auxilliary functions inside a let that establishes the 

bindings of vars, val s, and success, but that is not crucial. Those three variables 

could have been implemented as global variables instead. Note that it does not sup


port segment variables, or any of the other options implemented in the pat-match 

of chapter 6. 

(let* ((vars (make-array 10 :fill-pointer 0 ladjustable t)) 
(vals (make-array 10 :fill-pointer 0 :adjustable t)) 
(success (cons vars vals))) 


<a id='page-332'></a>

(defun efficient-pat-match (pattern input) 
"Match pattern against input." 
(setf (fill-pointer vars) 0) 
(setf (fill-pointer vals) 0) 
(pat-match-1 pattern input)) 

(defun pat-match-1 (pattern input) 

(cond ((variable-p pattern) (match-var pattern input)) 
((eql pattern input) success) 
((and (consp pattern) (consp input)) 

(and (pat-match-1 (first pattern) (first input)) 
(pat-match-1 (rest pattern) (rest input)))) 
(t fail))) 

(defun match-var (var input) 
"Match a single variable against input." 
(let ((i (position var vars))) 

(cond ((null i) 
(vector-push-extend var vars) 
(vector-push-extend input vals) 
success) 

((equal input (aref vals i)) success) 

(t fail))))) 

An example of its use: 

> (efficient-pat-match '(Tx + ?x = ?y . ?z) 
'(2 + 2 = (3 + 1) is true)) 
(#(?X ?Y 11) . #(2 (3 + 1) (IS TRUE))) 

Extensible vectors with fill pointers are convenient, and much more efficient than 
consing up lists. However, there is some overhead involved in using them, and for 
those sections of code that must be most efficient, it is best to stick with simple 
vectors. The following version of ef f i cient-pat-match explicitly manages the size 
of the vectors and explicitly replaces them with new ones when the size is exceeded: 

(let* ((current-size 0) 
(max-size 1) 
(vars (make-array max-size)) 
(vals (make-array max-size)) 
(success (cons vars vals))) 

(declare (simple-vector vars vals) 
(fixnum current-size max-size)) 


<a id='page-333'></a>

(defun efficient-pat-match (pattern input) 
"Match pattern against input." 
(setf current-size 0) 
(pat-match-1 pattern input)) 

pat-match-1 is unchanged 

(defun match-var (var input) 
"Match a single variable against input." 
(let ((i (position var vars))) 

(cond 
((null i) 
(when (= current-size max-size) 
Make new vectors when we run out of space 

(setf max-size (* 2 max-size) 
vars (replace (make-array max-size) vars) 
vals (replace (make-array max-size) vals) 
success (cons vars vals))) 

;; Store var and its value in vectors 
(setf (aref vars current-size) var) 
(setf (aref vals current-size) input) 
(incf current-size) 
success) 

((equal input (aref vals i)) success) 

(t fail))))) 

In conclusion, replacing lists with vectors can often save garbage. But when you 
must use lists, it pays to use a version of cons that avoids consing when possible. The 
following is such a version: 

(proclaim '(inline reuse-cons)) 

(defun reuse-cons (x y x-y) 
"Return (cons . y), or just x-y if it is equal to (cons . y). " 
(if (and (eql . (car x-y)) (eql y (cdr x-y))) 

x-y 

(cons X y))) 

The trick is based on the definition of subst in Steele's Common Lisp the Language. 
Here is a definition for a version of remove that uses reuse- cons: 


<a id='page-334'></a>

(defun remq (item list) 
"Like REMOVE, but uses EQ, and only works on lists. " 
(cond ((null list) nil) 

((eq item (first list)) (remq item (rest list))) 

(t (reuse-cons (first list) 
(remq item (rest list)) 
list)))) 

Avoid Consing: Unique Lists 

Of course, reuse - cons only works when you have candidate cons cells around. That 
is, (reuse-cons a b c) only saves space when c is (or might be) equal to (cons a b). 
For some applications, it is useful to have a version of cons that returns a unique cons 
cell without needing c as a hint. We will call this version ucons for "unique cons." 
ucons maintains a double hash table: *uni q- cons - tabl e* is a hash table whose keys 
are the cars of cons cells. The value for each car is another hash table whose keys 
are the cdrs of cons cells. The value of each cdr in this second table is the original 
cons cell. So two different cons cells with the same ca r and cdr will retrieve the same 
value. Here is an implementation of ucons: 

(defvar *uniq-cons-table* (make-hash-table :test #'eq)) 

(defun ucons (x y) 
"Return a cons s.t. (eq (ucons . y) (ucons . y)) is true." 
(let ((car-table (or (gethash . *uniq-cons-table*) 

(setf (gethash . *uniq-cons-table*) 
(make-hash-table :test #'eq))))) 
(or (gethash y car-table) 
(setf (gethash y car-table) (cons . y))))) 

ucons, unlike cons, is a true function: it will always return the same value, given 
the same arguments, where "same" is measured by eq. However, if ucons is given 
arguments that are equal but not eq, it will not return a unique result. For that 
we need the function unique. It has the property that (unique x) is eq to (unique 

y) whenever . and y are equal. unique uses a hash table for atoms in addition to 
the double hash table for conses. This is necessary because strings and arrays can 
be equal without being eq. Besides unique, we also define ul ist and uappend for 
convenience. 
(defvar *uniq-atom-table* (make-hash-table .-test #'equal)) 


<a id='page-335'></a>
(defun unique (exp) 
"Return a canonical representation that is EQUAL to exp. 
such that (equal . y) implies (eq (unique x) (unique y))." 
(typecase exp 

(symbol exp) 
(fixnum exp) Remove if fixnums are not eq in your Lisp 
(atom (or (gethash exp *uniq-atom-table*) 

(setf (gethash exp *uniq-atom-table*) exp))) 
(cons (unique-cons (car exp) (cdr exp))))) 

(defun unique-cons (x y) 
"Return a cons s.t. (eq (ucons . y) (uconswhenever (equal . x2) and (equal y y2) are(ucons (unique x) (unique y))) 

(defun ulist (&rest args) 
"A uniquified list." 
(unique args)) 

(defun uappend (x y) 
"A unique list equal to (append . y). " 
(if (null X) 
(unique y) 

 x2 y2)) is true 
true." 

(ucons (first x) (uappend (rest x) y)))) 

The above code works, but it can be improved. The problem is that when uni que is 
applied to a tree, it always traverses the tree all the way to the leaves. The function 
unique-cons is like ucons, except that unique-cons assumes its arguments are not 
yet unique. We can modify uni que- cons so that it first checks to see if its arguments 
are unique, by looking in the appropriate hash tables: 

(defun unique-cons (x y) 
"Return a cons s.t. (eq (ucons . y) (ucons x2 y2)) is true 
whenever (equal . x2) and (equal y y2) are true." 
(let ((ux) (uy)) : unique . and y 

(let ((car-table 

(or (gethash . *uniq-cons-table*) 
(gethash (setf ux (unique x)) *uniq-cons-table*) 
(setf (gethash ux *uniq-cons-table*) 

(make-hash-table :test #*eq))))) 

(or (gethash y car-table) 
(gethash (setf uy (unique y)) car-table) 
(setf (gethash uy car-table) 

(cons ux uy)))))) 

Another advantage of uni que is that it can help in indexing. If lists are unique, 
then they can be stored in an eq hash table instead of a equal hash table. This can 


<a id='page-336'></a>

lead to significant savings v^hen the list structures are large. An eq hash table for 
lists is almost as good as a property list on symbols. 

Avoid Consing: Multiple Values 

Parameters and multiple values can also be used to pass around values, rather than 
building up lists. For example, instead of: 

(defstruct point "A point in 3-D cartesian space." . y z) 

(defun scale-point (k pt) 

"Multiply a point by a constant, K." 

(make-point :x (* k (point-x pt)) 

:y (* k (point-y pt)) 

:z (* k (point-z pt)))) 

one could use the following approach, which doesn't generate structures: 

(defun scale-point (k . y z) 

"Multiply the point (x,y,z) by a constant, K." 

(values (* k x) (* k y) (* k z))) 

Avoid Consing: Resources 

Sometimes it pays to manage explicitly the storage of instances of some data type. A 
pool of these instances may be called a resource. Explicit management of a resource 
is appropriate when: (1) instances are frequently created, and are needed only 
temporarily; (2) it is easy/possible to be sure when instances are no longer needed; 
and (3) instances are fairly large structures or take a long time to initialize, so that it 
is worth reusing them instead of creating new ones. Condition (2) is the crucial one: 
If you deallocate an instance that is still being used, that instance will mysteriously 
be altered when it is reallocated. Conversely, if you fail to deallocate unneeded 
instances, then you are wasting valuable memory space. (The memory management 
scheme is said to leak in this case.) 

The beauty of using Lisp's built-in memory management is that it is guaranteed 
never to leak and never to deallocate structures that are in use. This eliminates two 
potential bug sources. The penalty you pay for this guarantee is some inefficiency of 
the general-purpose memory management as compared to a custom user-supplied 
management scheme. But beware: modern garbage-collection techniques are highly 
optimized. In particular, the so-called generation scavenging or ephemeral garbage 
collectors look more often at recently allocated storage, on the grounds that recently 
made objects are more likely to become garbage. If you hold on to garbage in your 
own data structures, you may end up with worse performance. 


<a id='page-337'></a>

With all these warnings in mind, here is some code to manage resources: 

(defmacro defresource (name &key constructor (initial-copies 0) 
(size (max initial-copies 10))) 

(let ((resource (symbol name '-resource)) 
(deallocate (symbol 'deallocate- name)) 
(allocate (symbol 'allocate- name))) 

'(let ((.resource (make-array .size ifill-pointer 0))) 

(defun .allocate () 
"Get an element from the resource pool, or make one." 
(if (= (fill-pointer .resource) 0) 

.constructor 
(vector-pop .resource))) 

(defun .deallocate (.name) 
"Place a no-longer-needed element back in the pool." 
(vector-push-extend .name .resource)) 

.(if (> initial-copies 0) 
'(mapc #'.deallocate (loop repeat .initial-copies 
collect (.allocate)))) 
'.name))) 

Let's say we had some structure called a buffer which we were constantly making 
instances of and then discarding. Furthermore, suppose that buffers are fairly 
complex objects to build, that we know we'll need at least 10 of them at a time, and 
that we probably won't ever need more than 100 at a time. We might use the buffer 
resource as follows: 

(defresource buffer :constructor (make-buffer) 
:size 100 :initial-copies 10) 

This expands into the following code: 

(let ((buffer-resource (make-array 100 :fil 1-pointer 0))) 

(defun allocate-buffer () 
"Get an element from the resource pool, or make one." 
(if (= (fil1-pointer buffer-resource) 0) 

(make-buffer) 
(vector-pop buffer-resource))) 

(defun deallocate-buffer (buffer) 
"Place a no-longer-needed element back in the pool." 
(vector-push-extend buffer buffer-resource)) 

(mapc #'deanocate-buffer 
(loop repeat 10 collect (allocate-buffer))) 
'buffer) 


<a id='page-338'></a>

We could then use: 

(let ((b (allocate-buffer))) 

(process b) 

(deallocate-buffer b))) 

The important thing to remember is that this works only if the buffer b really can 
be deallocated. If the function process stored away a pointer to b somewhere, 
then it would be a mistake to deallocate b, because a subsequent allocation could 
unpredictably alter the stored buffer. Of course, if process stored a copy of b, then 
everything is alright. This pattern of allocation and deallocation is so common that 
we can provide a macro for it: 

(defmacro with-resource ((var resource &optional protect) &rest body) 
"Execute body with VAR bound to an instance of RESOURCE." 
(let ((allocate (symbol 'allocate- resource)) 

(deallocate (symbol 'deallocate- resource))) 
(if protect 

'(let ((.var nil)) 
(unwind-protect 
(progn (setf ,var (.allocate)) .body) 
(unless (null .var) (.deallocate .var)))) 

'(let ((.var (.allocate))) 
.body 
(.deallocate .var))))) 
The macro allows for an optional argument that sets up an unwi nd - protect environment, 
so that the buffer gets deallocated even when the body is abnormally exited. 
The following expansions should make this clearer: 

> (macroexpand '(with-resource (b buffer) 
"..." (process b) "...")) 
(let ((b (allocate-buffer))) 

. It 

(process b) 

11 II 

(deallocate-buffer b)) 

> (macroexpand '(with-resource (b buffer t) 
"..." (process b) "...")) 
(let ((b nil)) 
(unwind-protect 
(progn (setf b (allocate-buffer)) 


<a id='page-339'></a>
(process b) 
"...") 
(unless (null b) 
(deallocate-buffer b)))) 

An alternative to full resources is to just save a single data object. Such an approach 
is simpler because there is no need to index into a vector of objects, but it is sufficient 
for some applications, such as a tail-recursive function call that only uses one object 
at a time. 

Another possibility is to make the system slower but safer by having the 
deal 1 ocate function check that its argument is indeed an object of the correct type. 

Keep in mind that using resources may put you at odds with the Lisp system's own 
storage management scheme. In particular, you should be concerned with paging 
performance on virtual memory systems. A common problem is to have only a few 
live objects on each page, thus forcing the system to do a lot of paging to get any work 
done. Compacting garbage collectors can collect live objects onto the same page, but 
using resources may interfere with this. 

10.5 Use the Right Data Structures 
It is important to implement key data types with the most efficient implementation. 
This can vary from machine to machine, but there are a few techniques that are 
universal. Here we consider three case studies. 

The Right Data Structure: Variables 

As an example, consider the implementation of pattern-matching variables. We saw 
from the instrumentation of s i mp1 if y that variable-p was one of the most frequently 
used functions. In compiling the matching expressions, I did away with all calls to 
vari abl e-p, but let's suppose we had an application that required run-time use of 
variables. The specification of the data type vari abl e will include two operators, 
the recognizer vari abl e-p, and the constructor make-vari abl e, which gives a new, 
previously unused variable. (This was not needed in the pattern matchers shown so 
far, but will be needed for unification with backward chaining.) One implementation 
of variables is as symbols that begin with the character #\?: 

(defun variable-p (x) 
"Is X a variable (a symbol beginning with *?')?" 
(and (symbolp x) (equal (elt (symbol-name x) 0) #\?))) 


<a id='page-340'></a>

(defun make-variable () "Generate a new variable" (gentemp "?")) 

We could try to speed things up by changing the implementation of variables to be 
keywords and making the functions inline: 

(proclaim '(inline variable-p make-variable)) 
(defun variable-p (x) "Is . a variable?" (keywordp x)) 
(defun make-variable () (gentemp "X" #.(find-package "KEYWORD"))) 

(The reader character sequence #. means to evaluate at read time, rather than at 
execution time.) On my machine, this implementation is pretty fast, and I accepted 
it as a viable compromise. However, other implementations were also considered. 
One was to have variables as structures, and provide a read macro and print function: 

(defstruct (variable (iprint-function print-variable)) name) 

(defvar *vars* (make-hash-table)) 

(set-macro-character #\? 
#'(lambda (stream char) 

Find an old var, or make a new one with the given name 
(declare (ignore char)) 
(let ((name (read stream t nil t))) 

(or (gethash name *vars*) 
(setf (gethash name *vars*) (make-variable mame name)))))) 

(defun print-variable (var stream depth) 
(declare (ignore depth)) 
(format stream "?~a" (var-name var))) 

It turned out that, on all three Lisps tested, structures were slower than keywords 
or symbols. Another alternative is to have the ? read macro return a cons whose 
first is, say, : var. This requires a special output routine to translate back to the ? 
notation. Yet another alternative, which turned out to be the fastest of all, was to 
implement variables as negative integers. Of course, this means that the user cannot 
use negative integers elsewhere in patterns, but that turned out to be acceptable for 
the application at hand. The moral is to know which features are done well in your 
particular implementation and to go out of your way to use them in critical situations, 
but to stick with the most straightforward implementation in noncritical sections. 

Lisp makes it easy to rely on lists, but one must avoid the temptation to overuse 
lists; to use them where another data structure is more appropriate. For example, if 
you need to access elements of a sequence in arbitrary order, then a vector is more 
appropriate than list. If the sequence can grow, use an adjustable vector. Consider 
the problem of maintaining information about a set of people, and searching that set. 
A naive implementation might look like this: 


<a id='page-341'></a>
(defvar *people* nil "Will hold a list of people") 

(defstruct person name address id-number) 

(defun person-with-id (id) 
(find id *people* :key #'person-id-number)) 

In a traditional language like C, the natural solution is to include in the person 
structure a pointer to the next person, and to write a loop to follow these pointers. 
Of course, we can do that in Lisp too: 

(defstruct person name address id-number next) 

(defun person-with-id (id) 

(loop for person = *people* then (person-next person) 

until (null person) 

do (when (eql id (person-id-number person)) 

(RETURN person)))) 

This solution takes less space and is probably faster, because it requires less memory 
accesses: one for each person rather than one for each person plus one for each 
cons cell. So there is a small price to pay for using lists. But Lisp programmers feel 
that price is worth it, because of the convenience and ease of coding and debugging 
afforded by general-purpose functions like f i nd. 

In any case, if there are going to be a large number of people, the list is definitely 
the wrong data structure. Fortunately, Lisp makes it easy to switch to more efficient 
data structures, for example: 

(defun person-with-id (id) 
(gethash id *people*)) 

The Right Data Structure: Queues 

Aqueue is a data structure where one can add elements at the rear and remove them 
from the front. This is almost like a stack, except that in a stack, elements are both 
added and removed at the same end. 

Lists can be used to implement stacks, but there is a problem in using lists to 
implement queues: adding an element to the rear requires traversing the entire list. 
So collecting . elements would be O(n^) instead of 0{n). 

An alternative implementation of queues is as a cons of two pointers: one to the 
list of elements of the queue (the contents), and one to the last cons cell in the list. 
Initially, both pointers would be nil. This implementation in fact existed in BBN Lisp 
and UCI Lisp under the function name tconc: 


<a id='page-342'></a>

;;; A queue is a (contents . last) pair 

(defun tconc (item q) 
"Insert item at the end of the queue." 
(setf (cdr q) 

(if (null (cdr q)) 
(setf (car q) (cons item nil)) 
(setf (rest (cdr q)) 

(cons item nil))))) 

The tconc implementation has the disadvantage that adding the first element to 
the contents is different from adding subsequent elements, so an i f statement is 
required to decide which action to take. The definition of queues given below avoids 
this disadvantage with a clever trick. First, the order of the two fields is reversed. 
The car of the cons cell is the last element, and the cdr is the contents. Second, the 
empty queue is a cons cell where the cdr (the contents field) is nil, and the car (the 
last field) is the cons itself. In the definitions below, we change the name tconc to 
the more standard enqueue, and provide the other queue functions as well: 

;;; A queue is a (last . contents) pair 

(proclaim '(inline queue-contents make-queue enqueue dequeue 
front empty-queue-p queue-nconc)) 

(defun queue-contents (q) (cdr q)) 

(defun make-queue () 
"Build a new queue, with no elements." 
(let ((q (cons nil nil))) 

(setf (car q) q))) 

(defun enqueue (item q) 
"Insert item at the end of the queue." 
(setf (car q) 

(setf (rest (car q)) 
(cons item nil))) 
q) 

(defun dequeue (q) 
"Remove an item from the front of the queue." 
(pop (cdr q)) 

(if (null (cdr q)) (setf (car q) q)) 
q) 

(defun front (q) (first (queue-contents q))) 

(defun empty-queue-p (q) (null (queue-contents q))) 


<a id='page-343'></a>

(defun queue-nconc (q list) 
"Add the elements of LIST to the end of the queue." 
(setf (car q) 

(last (setf (rest (car q)) list)))) 

The Right Data Structure: Tables 

A table is a data structure to which one can insert a key and associate it with a value, 
and later use the key to look up the value. Tables may have other operations, like 
counting the number of keys, clearing out all keys, or mapping a function over each 
key/value pair. 

Lisp provides a wide variety of choices to implement tables. An association list 
is perhaps the simplest: it is just a list of key/value pairs. It is appropriate for small 
tables, up to a few dozen pairs. The hash table is designed to be efficient for large 
tables, but may have significant overhead for small ones. If the keys are symbols, 
property lists can be used. If the keys are integers in a narrow range (or can be 
mapped into them), then a vector may be the most efficient choice. 

Here we implement an alternative data structure, the trie. A trie implements a 
table for keys that are composed of a finite sequence of components. For example, 
if we were implementing a dictionary as a trie, each key would be a word, and 
each letter of the word would be a component. The value of the key would be the 
word's definition. At the top of the dictionary trie is a multiway branch, one for each 
possible first letter. Each second-level node has a branch for every possible second 
letter, and so on. To find an n-letter word requires . reads. This kind of organization 
is especially good when the information is stored on secondary storage, because a 
single read can bring in a node with all its possible branches. 

If the keys can be arbitrary list structures, rather than a simple sequence of letters, 
we need to regularize the keys, transforming them into a simple sequence. One way 
to do that makes use of the fact that any tree can be written as a linear sequence 
of atoms and cons operations, in prefix form. Thus, we would make the following 
transformation: 

(a (b c) d) = 
(cons a (cons (cons b (cons c nil)) (cons d nil))) = 
(cons a cons cons b cons c nil cons d nil) 

In the implementation of tries below, this transformation is done on the fly: The four 
user-level functions are make-trie to create a new trie, put-trie and get-trie to 
add and retrieve key/value pairs, and del ete-tri e to remove them. 

Notice that we use a distinguished value to mark deleted elements, and that 
get-trie returns two values: the actual value found, and a flag saying if anything 


<a id='page-344'></a>

was found or not. This is consistent with the interface to gethash and find, and 
allows us to store null values in the trie. It is an inobtrusive choice, because the 
programmer who decides not to store null values can just ignore the second value, 
and everything will work properly. 

(defstruct trie (value nil) (arcs nil)) 
(defconstant trie-deleted "deleted") 

(defun put-trie (key trie value) 
"Set the value of key in trie." 
(setf (trie-value (find-trie key t trie)) value)) 

(defun get-trie (key trie) 
"Return the value for a key in a trie, and t/nil if found." 
(let* ((key-trie (find-trie key nil trie)) 

(val (if key-trie (trie-value key-trie)))) 

(if (or (null key-trie) (eq val trie-deleted)) 
(values nil nil) 
(values val t)))) 

(defun delete-trie (key trie) 
"Remove a key from a trie." 
(put-trie key trie trie-deleted)) 

(defun find-trie (key extend? trie) 
"Find the trie node for this key. 
If EXTEND? is true, make a new node if need be." 
(cond ((null trie) nil) 

((atom key) 
(follow-arc key extend? trie)) 

(t (find-trie 
(cdr key) extend? 
(find-trie 

(car key) extend? 
(find-trie 
"." extend? trie)))))) 

(defun follow-arc (component extend? trie) 
"Find the trie node for this component of the key. 
If EXTEND? is true, make a new node if need be." 
(let ((arc (assoc component (trie-arcs trie)))) 

(cond ((not (null arc)) (cdr arc)) 
((not extend?) nil) 
(t (let ((new-trie (make-trie))) 

(push (cons component new-trie) 
(trie-arcs trie)) 
new-trie))))) 


<a id='page-345'></a>
There are a few subtleties in the implementation. First, we test for deleted entries 

with an eq comparison to a distinguished marker, the string tri e-deleted. No other 

object will be eq to this string except tri e-del eted itself, so this is a good test. We 

also use a distinguished marker, the string ".", to mark cons cells. Components are 

implicitly compared against this marker with an eql test by the assoc in fol 1 ow- arc. 

Maintaining the identity of this string is crucial; if, for example, you recompiled 

the definition of f i nd-tri e (without changing the definition at all), then you could 

no longer find keys that were indexed in an existing trie, because the "." used by 

fi nd-tri e would be a different one from the "." in the existing trie. 

Artificial Intelligence Programming (Charniak et al. 1987) discusses variations on 
the trie, particularly in the indexing scheme. If we always use proper lists (no non-null 
cdrs), then a more efficient encoding is possible. As usual, the best type of indexing 
depends on the data to be indexed. It should be noted that Charniak et al. call the trie 
a discrimination net. In general, that term refers to any tree with tests at the nodes. 

A trie is, of course, a kind of tree, but there are cases where it pays to convert a trie 
into a dag—di directed acyclic graph. A dag is a tree where some of the subtrees are 
shared. Imagine you have a spelUng corrector program with a list of some 50,000 or 
so words. You could put them into a trie, each word with the value t. But there would 
be many subtrees repeated in this trie. For example, given a word list containing look, 
looks, looked, and looking as well as show, shows, showed, and showing, there would 
be repetition of the subtree containing -s, -ed and -ing. After the trie is built, we 
could pass the whole trie to un i que, and it would collapse the shared subtrees, saving 
storage. Of course, you can no longer add or delete keys from the dag without risking 
unintended side effects. 

This process was carried out for a 56,000 word list. The trie took up 3.2Mbytes, 
while the dag was 1.1 Mbytes. This was still deemed unacceptable, so a more compact 
encoding of the dag was created, using a .2Mbytes vector. Encoding the same word 
list in a hash table took twice this space, even with a special format for encoding 
suffixes. 

Tries work best when neither the indexing key nor the retrieval key contains 
variables. They work reasonably well when the variables are near the end of the 
sequence. Consider looking up the pattern "yel 1 o?" in the dictionary, where the " ?" 
character indicates a match of any letter. Following the branches for "yel 1 o" leads 
quickly to the only possible match, "yel 1 ow". In contrast, fetching with the pattern 
" ??11 ow" is much less efficient. The table lookup function would have to search all 
26 top-level branches, and for each of those consider all possible second letters, and 
for each of those consider the path " 11 ow". Quite a bit of searching is required before 
arriving at the complete set of matches: bellow, billow, fallow, fellow, follow, hallow, 
hollow, mallow, mellow, pillow, sallow, tallow, wallow, willow, and yellow. 

We will return to the problem of discrimination nets with variables in section 14.8, 
[page 472](chapter14.md#page-472). 


<a id='page-346'></a>

10.6 Exercises 
&#9635; Exercise 10.1 [h] Define tlie macro deftable, such that (def table person assoc) 
will act much like a def struct—it will define a set of functions for manipulating a 
table of people: get-person, put-person, cl ear-person, and map-person. The table 
should be implemented as an association list. Later on, you can change the representation 
of the table simply by changing the form to (def tabl e person hash), without 
having to change anything else in your code. Other implementation options include 
property lists and vectors, def table should also take three keyword arguments: 
i nl i ne, si ze and test. Here is a possible macroexpansion: 

> (macroexpand '(deftableperson hash .-inline t :size 100)) = 

(progn 
(proclaim '(inline get-person put-person map-person)) 
(defparameter *person-table* 

(make-hash-table :test #'eql :size 100)) 
(defun get-person (x &optional default) 
(gethash . *person-table* default)) 
(defun put-person (x value) 

(setf (gethash . *person-table*) value)) 
(defun clear-person () (clrhash *person-table*)) 
(defun map-person (fn) (maphash fn *person-table*)) 
(defsetf get-person put-person) 
'person) 

&#9635; Exercise 10.2 [m] We can use the : type option to defstruct to define structures 
implemented as lists. However, often we have a two-field structure that we would 
like to implement as a cons cell rather than a two-element list, thereby cutting storage 
in half. Since defstruct does not allow this, define a new macro that does. 

&#9635; Exercise 10.3 [m] Use reuse - cons to write a version of f 1 atten (see [page 329](chapter10.md#page-329)) that 
shares as much of its input with its output as possible. 

&#9635; Exercise 10.4 [h] Consider the data type set. A set has two main operations: adjoin 
an element and test for membership. It is convenient to also add a map-over-elements 
operation. With these primitive operations it is possible to build up more complex 
operations like union and intersection. 
As mentioned in section 3.9, Common Lisp provides several implementations 
of sets. The simplest uses lists as the underlying representation, and provides the 


<a id='page-347'></a>

functions ad j oi ., member, uni on, i ntersecti on, and set-di f f erence. Another uses 
bit vectors, and a similar one uses integers viewed as bit sequences. Analyze the 
time complexity of each implementation for each operation. 

Next, show how sorted lists can be used to implement sets, and compare the 
operations on sorted lists to their counterparts on unsorted lists. 

10.7 Answers 
Answer 10.2 

(defmacro def-cons-struct (cons car cdr &optional inline?) 
"Define aliases for cons, car and cdr." 
'(progn (proclaim '(.(if inline? 'inline 'notinline) 

.car .cdr .cons)) 
(defun .car (x) (car x)) 
(defun .cdr (x) (cdr x)) 
(defsetf .car (x) (val) '(setf (car .x) .val)) 
(defsetf .cdr (x) (val) '(setf (cdr .x) .val)) 
(defun .cons (x y) (cons . y)))) 

Answer 10.3 

(defun flatten (exp &optional (so-far nil) last-cons) 
"Return a flat list of the atoms in the input. 
Ex: (flatten '((a) (b (c) d))) => (a b c d)." 
(cond ((null exp) so-far) 

((atom exp) (reuse-cons exp so-far last-cons)) 

(t (flatten (first exp) 
(flatten (rest exp) so-far exp) 
exp)))) 


## Chapter 11
<a id='page-348'></a>

Logic Programming 

Alanguage that doesn't affect the way you think 
about programming is not worth knowing. 

—Alan Perlis 

L
L
isp is the major language for AI work, but it is by no means the only one. The other 
strong contender is Prolog, whose name derives from "programming in logic."^ The idea 
behind logic programming is that the programmer should state the relationships that 
describe a problem and its solution. These relationships act as constraints on the algorithms 
that can solve the problem, but the system itself, rather than the programmer, is responsible for 
the details of the algorithm. The tension between the "programming" and "logic" will be covered 
in chapter 14, but for now it is safe to say that Prolog is an approximation to the ideal goal of logic 
programming. Prolog has arrived at a comfortable niche between a traditional programming 
language and a logical specification language. It relies on three important ideas: 

^Actually, programmation en logique, since it was invented bya French group (see [page 382](chapter11.md#page-382)). 


<a id='page-349'></a>

* Prolog encourages the use of a single uniform data base. Good compilers provide 
efficient access to this data base, reducing the need for vectors, hash tables, 
property lists, and other data structures that the Lisp programmer must deal 
with in detail. Because it is based on the idea of a data base, Prolog is relational, 
while Lisp (and most languages) are functional. In Prolog we would represent 
a fact like "the population of San Francisco is 750,000" as a relation. In Lisp, 
we would be inclined to write a function, population, which takes a city as 
input and returns a number. Relations are more flexible; they can be used not 
only to find the population of San Francisco but also, say, to find the cities with 
populations over 500,000. 
* Prolog provides logic variables instead of "normal" variables. A logic variable is 
bound by unification rather than by assignment. Once bound, a logic variable 
can never change. Thus, they are more like the variables of mathematics. The 
existence of logic variables and unification allow the logic programmer to state 
equations that constrain the problem (as in mathematics), without having to 
state an order of evaluation (as with assignment statements). 
* Prolog provides automatic backtracking. In Lisp each function call returns a single 
value (unless the programmer makes special arrangements to have it return 
multiple values, or a list of values). In Prolog, each query leads to a search for 
relations in the data base that satisfy the query. If there are several, they are 
considered one at a time. If a query involves multiple relations, as in "what city 
has a population over 500,000 and is a state capital?," Prolog will go through 
the popul ati on relation to find a city with a population over 500,000. For each 
one it finds, it then checks the capi tal relation to see if the city is a capital. If 
it is, Prolog prints the city; otherwise it backtracks, trying to find another city 
in the population relation. So Prolog frees the programmer from worrying 
about both how data is stored and how it is searched. For some problems, the 
naive automatic search will be too inefficient, and the programmer will have to 
restate the problem. But the ideal is that Prolog programs state constraints on 
the solution, without spelling out in detail how the solutions are achieved. 
This chapter serves two purposes: it alerts the reader to the possibility of writing 
certain programs in Prolog rather than Lisp, and it presents implementations of the 
three important Prolog ideas, so that they may be used (independently or together) 
within Lisp programs. Prolog represents an interesting, different way of looking 
at the programming process. For that reason it is worth knowing. In subsequent 
chapters we will see several useful applications of the Prolog approach. 


<a id='page-350'></a>

11.1 Idea 1: A Uniform Data Base 
The first important Prolog idea should be familiar to readers of this book: manipulating 
a stored data base of assertions. In Prolog the assertions are called clauses, 
and they can be divided into two types: facts, which state a relationship that holds 
between some objects, and rules, which are used to state contingent facts. Here 
are representations of two facts about the population of San Francisco and the capital 
of California. The relations are population and capital, and the objects that 
participate in these relations are SF, 750000, Sacramento, and CA: 

(population SF 750000) 

(capital Sacramento CA) 

We are using Lisp syntax, because we want a Prolog interpreter that can be imbedded 
in Lisp. The actual Prolog notation would be popul ation(sf,750000). Here are 
some facts pertaining to the 1 i kes relation: 

(likes Kim Robin) 
(likes Sandy Lee) 
(likes Sandy Kim) 
(likes Robin cats) 

These facts could be interpreted as meaning that Kim likes Robin, Sandy likes both 
Lee and Kim, and Robin likes cats. We need some way of telling Lisp that these are 
to be interpreted as Prolog facts, not a Lisp function call. We will use the macro <- to 
mark facts. Think of this as an assignment arrow which adds a fact to the data base: 

(<- (likes Kim Robin)) 
(<- (likes Sandy Lee)) 
(<- (likes Sandy Kim)) 
(<- (likes Robin cats)) 

One of the major differences between Prolog and Lisp hinges on the difference 
between relations and functions. In Lisp, we would define a function 1 i kes, so 
that (likes 'Sandy) would return the list (Lee Kim). If we wanted to access the 
information the other way, we would define another function, say, 1 i kers-of, so 
that (1i ker s - of ' Lee) returns (Sandy). In Prolog, we have a single 1 i kes relation 
instead of multiple functions. This single relation can be used as if it were multiple 
functions by posing different queries. For example, the query (1i kes Sandy ?who) 
succeeds with ?who bound to Lee or Kim, and the query (1i kes ?who Lee) succeeds 
with ?who bound to Sandy. 


<a id='page-351'></a>

The second type of clause in a Prolog data base is the rule. Rules state contingent 

facts. For example, we can represent the rule that Sandy likes anyone who likes cats 

as follows: 

(<- (likes Sandy ?x) (likes ?x cats)) 

This can be read in two ways. Viewed as a logical assertion, it is read, "For any x, 
Sandy likes . if . likes cats." This is a declarative interpretation. Viewed as a piece 
of a Prolog program, it is read, "If you ever want to show that Sandy likes some x, 
one way to do it is to show that . likes cats." This is a procedural interpretation. 
It is called a backward-chaining interpretation, because one reasons backward from 
the goal (Sandy likes x) to the premises (x likes cats). The symbol <- is appropriate 
for both interpretations: it is an arrow indicating logical implication, and it points 
backwards to indicate backward chaining. 

It is possible to give more than one procedural interpretation to a declarative form. 
(We did that in chapter 1, where grammar rules were used to generate both strings 
of words and parse trees.) The rule above could have been interpreted procedurally 
as "If you ever find out that some . likes cats, then conclude that Sandy likes x." This 
would be forward chaining: reasoning from a premise to a conclusion. It turns out 
that Prolog does backward chaining exclusively. Many expert systems use forward 
chaining exclusively, and some systems use a mixture of the two. 

The leftmost expression in a clause is called the head, and the remaining ones are 
called the body. In this view, a fact is just a rule that has no body; that is, a fact is true 
no matter what. In general, then, the form of a clause is: 

(<-head body...) 

A clause asserts that the head is true only if all the goals in the body are true. For 
example, the following clause says that Kim likes anyone who likes both Lee and 
Kim: 

(<- (likes Kim ?x) (likes ?x Lee) (likes ?x Kim)) 

This can be read as: 

For any X, deduce that Km likes . 
if it can be proved that X likes lee and . likes Kim. 


<a id='page-352'></a>

11.2 Idea 2: Unification of Logic Variables 
Unification is a straightforward extension of the idea of pattern matching. The 
pattern-matching functions we have seen so far have always matched a pattern 
(an expression containing variables) against a constant expression (one with no 
variables). In unification, two patterns, each of which can contain variables, are 
matched against each other. Here's an example of the difference between pattern 
matching and unification: 

> (pat-match '(Tx + ?y) '(2 + D) ^ ((?Y . 1) (?X . 2)) 

> (unify '(?x + 1) '(2 + ?y)) => ((?Y . 1) (?X . 2)) 

Within the unification framework, variables (such as ?x and ?y above) are called logic 
variables. Like normal variables, a logic variable can be assigned a value, or it can 
be unbound. The difference is that a logic variable can never be altered. Once it is 
assigned a value, it keeps that value. Any attempt to unify it with a different value 
leads to failure. It is possible to unify a variable with the same value more than once, 
just as it was possible to do a pattern match of (?x + ?x) with (2 + 2). 

The difference between simple pattern matching and unification is that unification 
allows two variables to be matched against each other. The two variables remain 
unbound, but they become equivalent. If either variable is subsequently bound to 
a value, then both variables adopt that value. The following example equates the 
variables ?x and ?y by binding ?x to ?y: 

> (unify '(f ?x) '(f ?y)) => ((?X . ?Y)) 

Unification can be used to do some sophisticated reasoning. For example, if we have 
two equations, .-h. = 0 and . y = y, and if we know that these two equations 
unify, then we can conclude that a, x, and y are all 0. The version of uni fy we will 
define shows this result by binding ?y to 0, ?x to ?y, and ?a to ?x. We will also 
define the function unifier, which shows the structure that results from unifying 
two structures. 

> (unify '(?a + ?a = 0) '(?x + ?y = ?y)) => 
((?Y . 0) (?X . ?Y) (?A . ?X)) 

> (unifier '(?a + ?a = 0) '(?x + ?y = ?y)) => (0 + 0 = 0) 

To avoid getting carried away by the power of unification, it is a good idea to take stock 
of exactly what unification provides. It does provide a way of stating that variables 
are equal to other variables or expressions. It does not provide a way of automatically 
solving equations or applying constraints other than equality. The following example 


<a id='page-353'></a>

makes it clear that unification treats the symbol + only as an uninterpreted atom, not 
as the addition operator: 

> (unifier '(?a + ?a = 2) '(?x + ?y= ?y)) ^(2+2=2) 

Before developing the code for unif y, we repeat here the code taken from the pattern-
matching utility (chapter 6): 

(defconstant fail nil "Indicates pat-match failure") 

(defconstant no-bindings *((t . t)) 
"Indicates pat-match success, with no variables.") 

(defun variable-p (x) 
"Is X a variable (a symbol beginning with *?*)?" 
(and (symbolp x) (equal (char (symbol-name x) 0) #\?))) 

(defun get-binding (var bindings) 
"Find a (variable . value) pair in a binding list. " 
(assoc var bindings)) 

(defun binding-val (binding) 
"Get the value part of a single binding." 
(cdr binding)) 

(defun lookup (var bindings) 
"Get the value part (for var) from a binding list. " 
(binding-val (get-binding var bindings))) 

(defun extend-bindings (var val bindings) 
"Add a (var . value) pair to a binding list." 
(cons (cons var val) 

Once we add a "real" binding, 
we can get rid of the dummy no-bindings 

(if (and (eq bindings no-bindings)) 
nil 
bindings))) 

(defun match-variable (var input bindings) 
"Does VAR match input? Uses (or updates) and returns bindings." 
(let ((binding (get-binding var bindings))) 

(cond ((not binding) (extend-bindings var input bindings)) 
((equal input (binding-val binding)) bindings) 
(t fail)))) 

The unify function follows; it is identical to pat-match (as defined on [page 180](chapter6.md#page-180)) 
except for the addition of the line marked The function uni fy-vari abl e also 
follows match -variable closely: 


<a id='page-354'></a>

(defun unify (. y &optional (bindings no-bindings)) 
"See if X and y match with given bindings." 
(cond ((eq bindings fail) fail) 

((variable-p x) (unify-variable . y bindings)) 

((variable-p y) (unify-variable y . bindings)) 

((eql X y) bindings) 

((and (consp x) (consp y)) 

(unify (rest x) (rest y) 
(unify (first x) (first y) bindings))) 
(t fail))) 

(defun unify-variable (var . bindings) 
"Unify var with x. using (and maybe extending) bindings." 
Warning - buggy version 

(if (get-binding var bindings) 
(unify (lookup var bindings) . bindings) 
(extend-bindings var . bindings))) 

Unfortunately, this definition is not quite right. It handles simple examples: 

> (unify '(?x + 1) '(2 + ?y)) => ((?Y . 1) (?X . 2)) 

> (unify '?x '?y) ((?X . ?Y)) 

> (unify '(?x ?x) '(Ty ?y)) => ((?Y . ?Y) (?X . ?Y)) 

but there are several pathological cases that it can't contend with: 

> (unify '(?x ?x ?x) '(?y ?y ?y)) 

>Trap #043622 (PDL-OVERFLOW REGULAR) 

The regular push-down list has overflowed. 

While in the function GET-BINDING ^ UNIFY-VARIABLE .= UNIFY 

The problem here is that once ?y gets bound to itself, the call to unify inside 

uni fy-vari abl e leads to an infinite loop. But matching ?y against itself must al


ways succeed, so we can move the equality test in uni fy before the variable test. This 

assumes that equal variables are eql, a valid assumption for variables implemented 

as symbols (but be careful if you ever decide to implement variables some other way). 

(defun unify (x y &optional (bindings no-bindings)) 
"See if X and y match with given bindings." 
(cond ((eq bindings fail) fail) 

((eql X y) bindings) moved this line 
((variable-p x) (unify-variable . y bindings)) 
((variable-p y) (unify-variable y . bindings)) 
((and (consp x) (consp y)) 

(unify (rest x) (rest y) 


<a id='page-355'></a>

(unify (first x) (first y) bindings))) 
(t fail))) 

Here are some test cases: 

> (unify '(?x ?x) '(?y ?y)) ((?X . ?Y)) 

> (unify '(?x ?x ?x) '(?y ?y ?y)) ((?X . ?Y)) 

> (unify '(?x ?y) '(?y ?x)) ^ ((?Y . ?X) (?X . ?Y)) 

> (unify '(?x ?y a) '(?y ?x ?x)) 
>Trap #043622 (PDL-OVERFLOW REGULAR) 
The regular push-down list has overflowed. 
While in the function GET-BINDING ^ UNIFY-VARIABLE <= UNIFY 

We have pushed off the problem but not solved it. Allowing both (?Y . ?X) and 
(?X . ?Y) in the same binding list is as bad as allowing (?Y . ?Y). To avoid the 
problem, the policy should be never to deal with bound variables, but rather with 
their values, as specified in the binding list. The function uni fy-vari abl e fails to 
implement this policy. It does have a check that gets the binding for va r when it is a 
bound variable, but it should also have a check that gets the value of x, when . is a 
bound variable: 

(defun unify-variable (var . bindings) 
"Unify var with x, using (and maybe extending) bindings." 
(cond ((get-binding var bindings) 

(unify (lookup var bindings) . bindings)) 
((and (variable-p x) (get-binding . bindings)) 
(unify var (lookup . bindings) bindings)) 
(t (extend-bindings var . bindings)))) 

Here are some more test cases: 

> (unify '(?x ?y) *(?y ?x)) ((?X . ?Y)) 

> (unify '(?x ?y a) '(?y ?x ?x)) ^ ((?Y . A) (?X . ?Y)) 

It seems the problem is solved. Now let's try a new problem: 

> (unify '?x '(f ?x)) => ((?X F ?X)) 

Here((?X F ?X)) really means ((?X . ((F ?X)))), so ?X is bound to (F ?X).This 
represents a circular, infinite unification. Some versions of Prolog, notably Prolog II 
(Giannesini et al. 1986), provide an interpretation for such structures, but it is tricky 
to define the semantics of infinite structures. 


<a id='page-356'></a>

The easiest way to deal with such infinite structures is just to ban them. This 
ban can be realized by modifying the unifier so that it fails whenever there is an 
attempt to unify a variable with a structure containing that variable. This is known in 
unification circles as the occurs check. In practice the problem rarely shows up, and 
since it can add a lot of computational complexity, most Prolog systems have ignored 
the occurs check. This means that these systems can potentially produce unsound 
answers. In the final version of uni fy following, a variable is provided to allow the 
user to turn occurs checking on or off. 

(defparameter *occurs-check* t "Should we do the occurs check?") 

(defun unify (x y &optional (bindings no-bindings)) 
"See if X and y match with given bindings." 
(cond ((eq bindings fail) fail) 

((eql X y) bindings) 
((variable-p x) (unify-variable . y bindings)) 
((variable-p y) (unify-variable y . bindings)) 
((and (consp x) (consp y)) 

(unify (rest x) (rest y) 
(unify (first x) (first y) bindings))) 
(t fail))) 

(defun unify-variable (var . bindings) 
"Unify var with x. using (and maybe extending) bindings." 
(cond ((get-binding var bindings) 

(unify (lookup var bindings) . bindings)) 
((and (variable-p x) (get-binding . bindings)) 
(unify var (lookup . bindings) bindings)) 
((and *occurs-check* (occurs-check var . bindings)) 
fail) 
(t (extend-bindings var . bindings)))) 

(defun occurs-check (var . bindings) 
"Does var occur anywhere inside x?" 
(cond ((eq var x) t) 

((and (variable-p x) (get-binding . bindings)) 
(occurs-check var (lookup . bindings) bindings)) 
((consp x) (or (occurs-check var (first x) bindings) 
(occurs-check var (rest x) bindings))) 
(t nil))) 

Now we consider how unify will be used. In particular, one thing we want is a 
function for substituting a binding list into an expression. We originally chose 
association lists as the implementation of bindings because of the availability of the 
function subl is. Ironically, subl is won't work any more, because variables can 
be bound to other variables, which are in turn bound to expressions. The function 
subst-bi ndi ngs acts like subl i s, except that it substitutes recursive bindings. 


<a id='page-357'></a>

(defun subst-bindings (bindings x) 
"Substitute the value of variables in bindings into x, 
taking recursively bound variables into account." 
(cond ((eq bindings fail) fail) 

((eq bindings no-bindings) x) 
((and (variable-p x) (get-binding . bindings)) 

(subst-bindings bindings (lookup . bindings))) 

((atom x) x) 

(t (reuse-cons (subst-bindings bindings (car x)) 

(subst-bindings bindings (cdr x)) 
x)))) 

Now let's try uni fy on some examples: 

> (unify '(?x ?y a) '(?y ?x ?x)) => ((?Y . A) (?X. ?Y)) 

> (unify .?. '(f ?x)) NIL 

> (unify '(?x ?y) '((f ?y) (f ?x))) ^ NIL 

> (unify '(?x ?y ?z) '((Ty ?z) (?x ?z) (?x ?y))) => NIL 

> (unify 'a 'a) ((T . T)) 

Finally, the function unifier calls unify and substitutes the resulting binding Ust 
into one of the arguments. The choice of . is arbitrary; an equal result would come 
from substituting the binding list into y. 

(defun unifier (x y) 
"Return something that unifies with both . and y (or fail)." 
(subst-bindings (unify . y) .)) 

Here are some examples of uni f i er: 

> (unifier '(?. ?y a) '(?y ?x ?x)) (A A A) 

> (unifier '((?a * ?x ^ 2) + (?b* ?x) + ?c) 
'(?z + (4 * 5) + 3)) => 
((?A * 5 ^ 2) + (4 * 5) + 3) 


<a id='page-358'></a>

When *occurs - check* is false, we get the following answers: 

> (unify '?x *(f ?x)) ^ ((?X F ?X)) 

> (unify '(?x ?y) '((f ?y) (f ?x))) => 
((?Y F ?X) (?X F ?Y)) 

> (unify '(?x ?y ?z) '((?y ?z) (?x ?z) (?x ?y))) 
((?Z ?X ?Y) (?Y ?X 11) (?X ?Y ?Z)) 

Programming with Prolog 

The amazing thing about Prolog clauses is that they can be used to express relations 
that we would normally think of as "programs," not "data." For example, we can 
define the member relation, which holds between an item and a list that contains that 
item. More precisely, an item is a member of a list if it is either the first element of the 
list or a member of the rest of the list. This definition can be translated into Prolog 
almost verbatim: 

(<- (member ?item (?item . ?rest))) 
(<- (member ?item (?x . ?rest)) (member ?item ?rest)) 

Of course, we can write a similar definition in Lisp. The most visible difference is that 
Prolog allows us to put patterns in the head of a clause, so we don't need recognizers 
like consp or accessors like first and rest. Otherwise, the Lisp definition is similar:^ 

(defun lisp-member (item list) 
(and (consp list) 
(or (eql item (first list)) 
(lisp-member item (rest list))))) 

If we wrote the Prolog code without taking advantage of the pattern feature, it would 
look more like the Lisp version: 

(<- (member ?item ?list) 
(= ?list (?item . ?rest))) 

^Actually, this is more like the Lisp f i nd than the Lisp member. In this chapter we have 
adopted the traditional Prolog definition of member. 


<a id='page-359'></a>

(<- (member ?item ?list) 
(= ?list (?x . ?rest)) 
(member ?item ?rest)) 

If we define or in Prolog, we would write a version that is clearly just a syntactic 
variant of the Lisp version. 

(<- (member ?item ?list) 
(= ?list (?fir$t . ?rest)) 
(or (= ?item ?first) 

(member ?i tern ?rest))) 

Let's see how the Prolog version of member works. Imagine that we have a Prolog 
interpreter that can be given a query using the macro ?-, and that the definition of 
member has been entered. Then we would see: 

> (?- (member 2 (1 2 3))) 
Yes; 

> (?- (member 2 (1 2 3 2 1))) 

Yes; 

Yes; 

The answer to the first query is "yes" because 2 is a member of the rest of the list. In 
the second query the answer is "yes" twice, because 2 appears in the list twice. This 
is a little surprising to Lisp programmers, but there still seems to be a fairly close 
correspondence between Prolog's and Lisp's member. However, there are things that 
the Prolog member can do that Lisp cannot: 

> (?- (member ?x (1 2 3))) 

?X = 1; 
?X = 2 
?X = 3 

Here member is used not as a predicate but as a generator of elements in a Hst. 
While Lisp functions always map from a specified input (or inputs) to a specified 
output, Prolog relations can be used in several ways. For member, we see that the 
first argument, ?x, can be either an input or an output, depending on the goal that 
is specified. This power to use a single specification as a function going in several 
different directions is a very flexible feature of Prolog. (Unfortunately, while it works 
very well for simple relations like member, in practice it does not work well for large 
programs. It is very difficult to, say, design a compiler and automatically have it work 
as a disassembler as well.) 


<a id='page-360'></a>

Now we turn to the implementation of the Prolog interpreter, as summarized in 
figure 11.1. The first implementation choice is the representation of rules and facts. 
We will build a single uniform data base of clauses, without distinguishing rules from 
facts. The simplest representation of clauses is as a cons cell holding the head and 
the body. For facts, the body will be empty. 

;; Clauses are represented as (head . body) cons cells 
(defun clause-head (clause) (first clause)) 
(defun clause-body (clause) (rest clause)) 

The next question is how to index the clauses. Recall the procedural interpretation 
of a clause: when we want to prove the head, we can do it by proving the body. This 
suggests that clauses should be indexed in terms of their heads. Each clause will be 
stored on the property list of the predicate of the head of the clause. Since the data 
base is now distributed across the property list of various symbols, we represent the 
entire data base as a Hst of symbols stored as the value of *db-predi cates*. 

Clauses are stored on the predicate's plist 
(defun get-clauses (pred) (get pred 'clauses)) 
(defun predicate (relation) (first relation)) 

(defvar *db-predicates* nil 
"A list of all predicates stored in the database.") 

Now we need a way of adding a new clause. The work is split up into the macro <-, 
which provides the user interface, and a function, add-cl a use, that does the work. 
It is worth defining a macro to add clauses because in effect we are defining a new 
language: Prolog-In-Lisp. This language has only two syntactic constructs: the <macro 
to add clauses, and the ? - macro to make queries. 

(defmacro <- (&rest clause) 
"Add a clause to the data base." 

'(add-clause '.clause)) 
(defun add-clause (clause) 
"Add a clause to the data base, indexed by head's predicate." 
The predicate must be a non-variable symbol, 

(let ((pred (predicate (clause-head clause)))) 
(assert (and (symbolp pred) (not (variable-p pred)))) 
(pushnew pred *db-predicates*) 
(setf (get pred 'clauses) 

(nconc (get-clauses pred) (list clause))) 
pred)) 

Now all we need is a way to remove clauses, and the data base will be complete. 


<a id='page-361'></a>
<


?


*db-precli cates* 
*occurs -check* 

clause 

variable 

add-clause 
prove 
prove-all 
top-level-prove 

get-clauses 
predicate 
clear-db 
clear-predicate 
rename-variables 
unique-find-anywhere-if 
show-prolog-soluti ons 
show-prolog-vars 
variables-in 

fail 
no-bindings 

unify 
unify-variable 
occurs-check 
subst-bindings 
get-binding 
lookup 
extend-bindings 
variable-p 
reuse-cons 

Top-Level Macros 

Add a clause to the data base. 
Prove a query and print answer(s). 

Special Variables 

A list of all predicates. 
Should we check for circular unifications? 

Data Types 

Consists of a head and a body. 
A symbol starting with a ?. 

Major Functions 

Add a clause to the data base. 
Return a list of possible solutions to goal. 
Return a list of solutions to the conjunction of goals. 
Prove the goals, and print variables readably. 

Auxiliary F^mctions 

Find all the clauses for a predicate. 
Pick out the predicate from a relation. 
Remove all clauses (for all predicates) from the data base. 
Remove the clauses for a single predicate. 
Replace all variables in . with new ones. 
Find all unique leaves satisfying predicate. 
Print the variables in each of the solutions. 
Print each variable with its binding. 
Return a list of all the variables in an expression. 

Previously Defined Constants 

An indication that unification has failed. 
A succesful unification with no variables. 

Previously Defined Functions 

